{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"CPG Flow","text":""},{"location":"#cpg-flow","title":"\ud83d\udc19 CPG Flow","text":""},{"location":"#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>\ud83d\udc19 What is this API ?</li> <li>\u2728 Documentation</li> <li>\ud83d\udd28 Installation</li> <li>\ud83d\ude80 Build</li> <li>\ud83e\udd16 Usage</li> <li>\ud83d\ude35\u200d\ud83d\udcab Key Considerations and Limitations</li> <li>\ud83d\udc33 Docker</li> <li>\ud83d\udcaf Tests</li> <li>\u2611\ufe0f Code analysis and consistency</li> <li>\ud83d\udcc8 Releases &amp; Changelog</li> <li>\ud83c\udfac GitHub Actions</li> </ol>"},{"location":"#what-is-this-api","title":"\ud83d\udc19 What is this API ?","text":"<p>Welcome to CPG Flow!</p> <p>This API provides a set of tools and workflows for managing population genomics data pipelines, designed to streamline the processing, analysis, and storage of large-scale genomic datasets. It facilitates automated pipeline execution, enabling reproducible research while integrating with cloud-based resources for scalable computation.</p> <p>CPG Flow supports various stages of genomic data processing, from raw data ingestion to final analysis outputs, making it easier for researchers to manage and scale their population genomics workflows.</p> <p>The API constructs a DAG (Directed Acyclic Graph) structure from a set of chained stages. This DAG structure then forms the pipeline.</p>"},{"location":"#documentation","title":"\u2728 Documentation","text":""},{"location":"#production","title":"\ud83c\udf10 Production","text":"<p>The production version of this API is documented at populationgenomics.github.io/cpg-flow/.</p> <p>The documentation is updated automatically when a commit is pushed on the <code>alpha</code> (prerelease) or <code>main</code> (release) branch.</p>"},{"location":"changelog/","title":"\ud83d\udcc8 Releases &amp; Changelog","text":"<p>Releases on main branch are generated and published automatically, pre-releases on the alpha branch are also generated and published by:</p> <p></p> <p>It uses the conventional commit strategy.</p> <p>This is enforced using the commitlint pre-commit hook that checks commit messages conform to the conventional commit standard.</p> <p>We recommend installing and using commitizen in order to create commit messages. Once installed, you can use either <code>cz commit</code> or <code>git cz</code> to create a commitizen generated commit message.</p> <p>Each change when a new release comes up is listed in the CHANGELOG.md file.</p>"},{"location":"changelog/#all-releases-for-this-project-are-available-here","title":"\ud83c\udff7\ufe0f All releases for this project are available here.","text":""},{"location":"code-analysis-consistency/","title":"\ud83d\udd0d Code Linting and Formatting","text":"<p>In order to keep the code clean, consistent and free of bad python practices, more than Over 10 pre-commit hooks are enabled !</p> <p>Complete list of all enabled rules is available in the .pre-commit-config.yaml file.</p>"},{"location":"code-analysis-consistency/#setup","title":"\u2699\ufe0f Setup","text":"<p>Info</p> <p>Before linting, you must follow the installation steps.</p> <p>Then, run the following command</p> <pre><code># Lint\npre-commit run --all-files\n</code></pre> <p>When setting up local linting for development you can also run the following once:</p> <pre><code># Install the pre-commit hook\npre-commit install\n\n# Or equivalently\nmake init || make init-dev\n</code></pre>"},{"location":"code-analysis-consistency/#project-quality-scanner","title":"\ud83e\udd47 Project quality scanner","text":"<p>Multiple tools are set up to maintain the best code quality and to prevent vulnerabilities:</p> <p></p> <p>SonarQube summary is available here.</p> <p> </p> <p> </p> <p> </p>"},{"location":"considerations-limitations/","title":"\ud83d\ude35\u200d\ud83d\udcab Key Considerations and Limitations","text":""},{"location":"considerations-limitations/#no-forward-discovery","title":"\ud83d\udeab No Forward Discovery","text":"<p>The framework exclusively relies on backward traversal. If a stage is not explicitly or indirectly linked to one of the final stages through the <code>required_stages</code> parameter of the <code>@stage</code> decorator, it will not be included in the workflow. In other words, stages that are not reachable from a final stage are effectively ignored. This backward discovery approach ensures that only the stages directly required for the specified final stages are included, optimizing the workflow by excluding irrelevant or unused stages.</p>"},{"location":"considerations-limitations/#workflow-definition","title":"\ud83d\udcdd Workflow Definition","text":"<p>The workflow definition serves as a lookup table for the final stages. If a final stage is not listed in this definition, it will not be part of the workflow, as there is no mechanism for forward discovery to identify it.</p> <pre><code>workflow = [GeneratePrimes, CumulativeCalc, FilterEvens, BuildAPrimePyramid]\n</code></pre>"},{"location":"considerations-limitations/#config-settings-for-expected_outputs","title":"\ud83d\udcdc Config Settings for <code>expected_outputs</code>","text":"<p>The <code>expected_outputs</code> method is called for every stage in the workflow, even if the <code>config.toml</code> configures the stage to be skipped. This ensures that the workflow can validate or reference the expected outputs of all stages.</p> <p>Since this method may depend on workflow-specific configuration settings, these settings must be present in the workflow configuration, regardless of whether the stage will run. To avoid issues, it is common practice to include dummy values for such settings in the default configuration. This is not the intended behaviour and is marked as an area of improvement in a future release.</p>"},{"location":"considerations-limitations/#verifying-results-of-expected_outputs","title":"\u2753 Verifying results of <code>expected_outputs</code>","text":"<p>The API uses the results of the <code>expected_outputs</code> method to determine whether a stage needs to run. A stage is scheduled for execution only if one or more Path objects returned by <code>expected_outputs</code> do not exist in Google Cloud Platform (GCP). If a returned Path object exists, the stage is considered to have already run successfully, and is therefore skipped.</p> <p>For outputs such as Matrix Tables (.mt), Hail Tables (.ht), or Variant Datasets (.vds), which are complex structures of thousands of files, the check is performed on the <code>object/_SUCCESS</code> file to verify that the output was written completely. However, it has been observed that the <code>object/_SUCCESS</code> file may be written multiple times during processing, contrary to the expectation that it should only be written once after all associated files have been fully processed.</p>"},{"location":"considerations-limitations/#string-outputs-from-expected_outputs","title":"<code>String</code> outputs from <code>expected_outputs</code>","text":"<p>String outputs from the <code>expected_outputs</code> method are not checked by the API. This is because string outputs cannot reliably be assumed to represent valid file paths and may instead correspond to other forms of outputs.</p>"},{"location":"considerations-limitations/#behavior-of-queue_jobs-in-relation-to-expected_outputs","title":"\ud83e\udd14 Behavior of <code>queue_jobs</code> in relation to <code>expected_outputs</code>","text":"<p>When the <code>expected_outputs</code> check determines that one or more required files do not exist, and the stage is not configured to be skipped, the <code>queue_jobs</code> method is invoked to define the specific work that needs to be scheduled in the workflow.</p> <p>The <code>queue_jobs</code> method runs within the driver image, before any jobs in the workflow are executed. Because of this, it cannot access or read files generated by earlier stages, as those outputs have not yet been created. The actual outputs from earlier jobs only become available as the jobs are executed during runtime.</p>"},{"location":"considerations-limitations/#explicit-dependency-between-all-jobs-from-queue_jobs","title":"\u26d3 Explicit dependency between all jobs from <code>queue_jobs</code>","text":"<p>When the <code>queue_jobs</code> method schedules a collection of jobs to Hail Batch, one or more jobs are returned from the method, and the framework sets an explicit dependency between these jobs, and all jobs from the <code>Stages</code> set in the <code>required_stages</code> parameter. Therefore, all jobs that run in a Stage must be returned within <code>queue_jobs</code> to ensure no jobs start out of sequence. As an example:</p> test_workflows_shared/cpg_flow_test/jobs/filter_evens.py<pre><code>def filter_evens(\n    b: Batch,\n    inputs: StageInput,\n    previous_stage: Stage,\n    sequencing_groups: list[SequencingGroup],\n    input_files: dict[str, dict[str, Any]],\n    sg_outputs: dict[str, dict[str, Any]],\n    output_file_path: str,\n) -&gt; list[Job]:\n    title = 'Filter Evens'\n\n    # Compute the no evens list for each sequencing group\n    sg_jobs = []\n    sg_output_files = []\n    for sg in sequencing_groups:  # type: ignore\n        job = b.new_job(name=title + ': ' + sg.id)\n        ...\n\n        cmd = f\"\"\"\n        ...\n        \"\"\"\n\n        job.command(cmd)\n        b.write_output(job.sg_no_evens_file, no_evens_output_file_path)\n        sg_jobs.append(job)\n\n    # Merge the no evens lists for all sequencing groups into a single file\n    job = b.new_job(name=title)\n    job.depends_on(*sg_jobs)\n    inputs = ' '.join([b.read_input(f) for f in sg_output_files])\n    job.command(f'cat {inputs} &gt;&gt; {job.no_evens_file}')\n    b.write_output(job.no_evens_file, output_file_path)\n\n    # ALL jobs are returned back to `queue_jobs`\n    # including new jobs created within this job.\n    all_jobs = [job, *sg_jobs]\n    return all_jobs\n</code></pre>"},{"location":"docker/","title":"\ud83d\udc33 Docker","text":""},{"location":"docker/#pulling-and-using-the-cpg-flow-docker-image","title":"\u2b07\ufe0f Pulling and Using the <code>cpg-flow</code> Docker Image","text":"<p>These steps are restricted to CPG members only. Anyone will have access to the code in this public repositry and can build a version of cpg-flow themselves. The following requires authentication with the CPG's GCP.</p> <p>To pull and use the Docker image for the <code>cpg-flow</code> Python package, follow these steps:</p> <ol> <li> <p>Authenticate with Google Cloud Registry:</p> <pre><code>gcloud auth configure-docker australia-southeast1-docker.pkg.dev\n</code></pre> </li> <li> <p>Pull the Docker Image:</p> <ul> <li>For alpha releases:</li> </ul> <pre><code>docker pull australia-southeast1-docker.pkg.dev/cpg-common/images/cpg_flow:0.1.0-alpha.11\n</code></pre> <ul> <li>For main releases:</li> </ul> <pre><code>docker pull australia-southeast1-docker.pkg.dev/cpg-common/images/cpg_flow:1.0.0\n</code></pre> </li> <li> <p>Run the Docker Container:</p> <pre><code>docker run -it australia-southeast1-docker.pkg.dev/cpg-common/images/cpg_flow:&lt;tag&gt;\n</code></pre> </li> </ol>"},{"location":"docker/#temporary-images-for-development","title":"\u23f3 Temporary Images for Development","text":"<p>Temporary images are created for each commit and expire in 30 days. These images are useful for development and testing purposes.</p> <ul> <li>Example of pulling a temporary image:</li> </ul> <pre><code>docker pull australia-southeast1-docker.pkg.dev/cpg-common/images-tmp/cpg_flow:991cf5783d7d35dee56a7ab0452d54e69c695c4e\n</code></pre>"},{"location":"docker/#accessing-build-images-for-cpg-members","title":"\ud83d\udd28 Accessing Build Images for CPG Members","text":"<p>Members of the CPG can find the build images in the Google Cloud Registry under the following paths:</p> <ul> <li>Alpha and main releases: <code>australia-southeast1-docker.pkg.dev/cpg-common/images/cpg_flow</code></li> <li>Temporary images: <code>australia-southeast1-docker.pkg.dev/cpg-common/images-tmp/cpg_flow</code></li> </ul> <p>Ensure you have the necessary permissions and are authenticated with Google Cloud to access these images.</p>"},{"location":"installation/","title":"\ud83d\udd28 Installation","text":""},{"location":"installation/#user-installation-instructions","title":"\ud83d\ude4b\u200d\u2640\ufe0f User Installation Instructions","text":"<p>The packages are hosted on:</p> <p></p> <p>To include <code>cpg-flow</code> in your python project simply install either the latest stable version as layed out in the PyPi package page. =</p> <p>This is as simple as running the following in your project python environment <pre><code>pip install cpg-flow\n</code></pre></p> <p>For a specific version <pre><code>pip install cpg-flow==0.1.2\n</code></pre></p> <p>We recommend making the appropriate choice for your individual project. Simply including <code>cpg-flow</code> in your dependency management system of choice will install the latest stable relase. But if neccessary you can pin the version. For example in your <code>pyproject.toml</code> file simply include the following: <pre><code>dependencies = [\n    \"cpg-flow\",         # latest OR\n    \"cpg-flow==0.1.2\",  # pinned version\n]\n</code></pre></p>"},{"location":"installation/#development-installation-instructions","title":"\ud83d\udee0\ufe0f Development Installation Instructions","text":"<p>These instructions are for contributors and developers on the <code>cpg-flow</code> project repository. Follow the following steps to setup your environment for development.</p> <p>To install this project, you will need to have Python and <code>uv</code> installed on your machine:</p> <p> </p> <p>We use uv for dependency management which can sync your environment locally with the following command:</p> <pre><code># Install the package using uv\nuv sync\n</code></pre> <p>However, to setup for development we recommend using the makefile setup which will do that for you.</p> <pre><code>make init-dev # installs pre-commit as a hook\n</code></pre>"},{"location":"installation/#upgrading-packages","title":"\ud83d\udce6 Upgrading Packages","text":"<p>To upgrade dependencies in your development environment:</p> <p>Security Cutoff Date Required</p> <p>Always use the <code>--exclude-newer</code> flag with a security cutoff date to exclude packages released in the last 7 days, reducing risk of vulnerable releases.</p> <pre><code># Calculate cutoff date and upgrade packages\nCUTOFF_DATE=$(python3 -c \"from datetime import datetime, timedelta; print((datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d'))\")\n\n# Upgrade all packages to their latest compatible versions\nuv sync --upgrade --exclude-newer $CUTOFF_DATE\n\n# OR upgrade a specific package\nuv sync --upgrade-package &lt;package-name&gt; --exclude-newer $CUTOFF_DATE\n</code></pre> <p>The <code>--exclude-newer</code> flag helps avoid pulling very recent package versions that may contain vulnerabilities or haven't been thoroughly tested. Use a recent date (e.g., a few days or weeks ago) as a safety buffer.</p> <p>After upgrading packages, ensure all tests pass and pre-commit hooks still work correctly.</p>"},{"location":"installation/#installing-pre-commit-hooks","title":"\ud83e\ude9d Installing Pre-commit Hooks","text":"<p>Pre-commit hooks help maintain code quality by running checks before each commit. The <code>make init-dev</code> command installs these hooks automatically, but you can also install them manually:</p> <pre><code># Install pre-commit hooks for standard checks\nuv run pre-commit install\n\n# Install commit message linting hook\nuv run pre-commit install --hook-type commit-msg\n</code></pre> <p>To run pre-commit hooks manually on all files:</p> <pre><code>uv run pre-commit run --all-files\n</code></pre> <p>To install <code>cpg-flow</code> locally for testing the code as an editable dependency</p> <pre><code>make install-local\n</code></pre> <p>This will install cpg-flow a an editable dependency in your environment. However, sometimes it can be useful to test the package post-build. <pre><code>make install-build\n</code></pre></p> <p>This will build and install the package as it would be distributed.</p> <p>You can confirm which version of cpg-flow is installed by running <pre><code>uv pip show cpg-flow\n</code></pre></p> <p>For an Editable package it should show the repo location on your machine under the <code>Editable:</code> key. <pre><code>Name: cpg-flow\nVersion: 0.1.2\nLocation: /Users/whoami/cpg-flow/.venv/lib/python3.10/site-packages\nEditable project location: /Users/whoami/cpg-flow\nRequires: coloredlogs, cpg-utils, grpcio, grpcio-status, hail, ipywidgets, metamist, networkx, plotly, pre-commit, pyyaml\nRequired-by:\n</code></pre></p> <p>The build version (static until you rebuild) will look like the following. <pre><code>Name: cpg-flow\nVersion: 0.1.2\nLocation: /Users/whoami/cpg-flow/.venv/lib/python3.10/site-packages\nRequires: coloredlogs, cpg-utils, grpcio, grpcio-status, hail, ipywidgets, metamist, networkx, plotly, pre-commit, pyyaml\nRequired-by:\n</code></pre></p> <p>Tip</p> <p>To try out the pre-installed <code>cpg-flow</code> in a Docker image, find more information in the Docker documentation.</p>"},{"location":"installation/#build","title":"\ud83d\ude80 Build","text":"<p>To build the project, run the following command:</p> <pre><code>make build\n</code></pre> <p>To make sure that you're actually using the installed build we suggest calling the following to install the build wheel.</p> <pre><code>make install-build\n</code></pre>"},{"location":"tests/","title":"\ud83e\uddea Unit and E2E tests","text":""},{"location":"tests/#unit-tests","title":"Unit Tests","text":""},{"location":"tests/#e2e-test","title":"E2E Test","text":"<p>We recommend frequently running the manual test workflow found in test_workflows_shared  specifically the <code>cpg_flow_test</code> workflow during development to ensure updates work with the CPG production environment.</p> <p>Docummentation for running the test can be found in the repository README file.</p> <p>Info</p> <p>Before testing, you must follow the installation steps.</p>"},{"location":"usage/","title":"\ud83e\udd16 Usage","text":"<p>This project provides the framework to construct pipelines but does not offer hosting the logic of any pipelines themselves. This approach offers the benefit of making all components more modular, manageable and decoupled. Pipelines themselves are hosted in a separate repository.</p> <p>The test_workflows_shared repository acts as a template and demonstrates how to structure a pipeline using CPG Flow.</p> <p>The components required to build pipelines with CPG Flow:</p>"},{"location":"usage/#some-toml-configuration-file","title":"Some <code>.toml</code> configuration file","text":"<p>This file contains the configuration settings to your pipeline. This file allows the pipeline developer to define settings such as:</p> <ol> <li>what stages will be run or skipped</li> <li>what dataset to use</li> <li>what access level to use</li> <li>any input cohorts</li> <li>sequencing type</li> </ol> config.toml<pre><code>[workflow]\ndataset = 'fewgenomes'\n\n# Note: for fewgenomes and sandbox mentioning datasets by name is not a security risk\n# DO NOT DO THIS FOR OTHER DATASETS\n\n# set this to specify the output version used by\n# workflow.output_version, target.tmp|web_prefix, etc.\noutput_version = 'ABC123'\n\ninput_cohorts = ['COH2142']\naccess_level = 'test'\n\n# Force stage rerun\nforce_stages = [\n    'GeneratePrimes', # the first stage\n    'CumulativeCalc', # the second stage\n    'FilterEvens', # the third stage\n    'BuildAPrimePyramid', # the last stage\n]\n\n# Show a workflow graph locally or save to web bucket.\n# Default is false, set to true to show the workflow graph.\nshow_workflow = true\n# ...\n</code></pre> <p>For a full list of supported config options with documentation, see defaults.toml</p> <p>Info</p> <p>This <code>.toml</code> file may be named anything, as long as it is correctly passed to the <code>analysis-runner</code> invocation. The <code>analysis-runner</code> supplies its own default settings, and combines it with the settings from this file, before submitting a job.</p>"},{"location":"usage/#an-entrypoint-for-the-pipeline","title":"An entrypoint for the pipeline","text":"<p>Ideally a file like <code>main.py</code> that would be the entrypoint for the pipeline, storing the workflow definition as a list of stages, and then running said workflow:</p> main.py<pre><code> import os\n from pathlib import Path\n from cpg_flow.workflow import run_workflow\n from cpg_utils.config import set_config_paths\n from stages import BuildAPrimePyramid, CumulativeCalc, FilterEvens, GeneratePrimes\n\n CONFIG_FILE = str(Path(__file__).parent / '&lt;YOUR_CONFIG&gt;.toml')\n\n def run_cpg_flow(dry_run=False):\n\n    #See the 'Key Considerations and Limitations' section for notes on the definition of the `workflow` variable.\n\n    # This represents the flow of the DAG\n     workflow = [GeneratePrimes, CumulativeCalc, FilterEvens, BuildAPrimePyramid]\n\n     config_paths = os.environ['CPG_CONFIG_PATH'].split(',')\n\n     # Inserting after the \"defaults\" config, but before user configs:\n     set_config_paths(config_paths[:1] + [CONFIG_FILE] + config_paths[1:])\n     run_workflow(stages=workflow, dry_run=dry_run)\n\n if __name__ == '__main__':\n   run_cpg_flow()\n</code></pre> <p>The workflow definition here forms a DAG (Directed Acyclic Graph) structure.</p> <p></p> <p>Tip</p> <p>To generate a plot of the DAG, <code>show_workflow = True</code> should be included in the config. The DAG plot generated from the pipeline definition is available in the logs via the job URL. To find the link to the plot, search the Logs section for the string: \"INFO - Link to the graph:\".</p> <p>Warning</p> <p>There are some key considerations and limitations to take into account when designing the DAG:</p> <ul> <li>No Forward Discovery</li> <li>Workflow Definition</li> </ul>"},{"location":"usage/#the-stage-definitions","title":"The <code>Stage</code> definitions","text":"<p>A <code>Stage</code> represents a node in the DAG. The stages can be abstracted from either a <code>DatasetStage</code>, <code>CohortStage</code>, <code>MultiCohortStage</code>, or a <code>SequencingGroupStage</code>.</p> <p>The stage definition should use the <code>@stage</code> decorator to optionally set:</p> <ul> <li>dependent stages (this is used to build the DAG)</li> <li>analysis keys (this determines what outputs should be written to metamist)</li> <li>the analysis type (this determines the analysis-type to be written to metamist)</li> </ul> <p>All stages require an <code>expected_outputs</code> class method definition, that sets the expected output path location for a given <code>Target</code> such as a <code>SequencingGroup</code>, <code>Dataset</code>, <code>Cohort</code>, or <code>MultiCohort</code>.</p> <p>Also required, is a <code>queue_jobs</code> class method definition that calls pipeline jobs, and stores the results of these jobs to the paths defined in <code>expected_outputs</code>.</p> <p>Tip</p> <p>It is good practice to separate the <code>Stage</code> definitions into their own files, to keep the code compact, and manageable.</p> stages.py<pre><code>from cpg_flow.stage import SequencingGroupStage, StageInput, StageOutput, stage\nfrom cpg_flow.targets.sequencing_group import SequencingGroup\nfrom jobs import cumulative_calc\n\nWORKFLOW_FOLDER = 'prime_pyramid'\n\n# ...\n# This stage depends on the `GeneratePrimes` stage, and requires outputs from that stage.\n@stage(required_stages=[GeneratePrimes], analysis_keys=['cumulative'], analysis_type='custom')\nclass CumulativeCalc(SequencingGroupStage):\n def expected_outputs(self, sequencing_group: SequencingGroup):\n     return {\n         'cumulative': sequencing_group.dataset.prefix() / WORKFLOW_FOLDER / f'{sequencing_group.id}_cumulative.txt',\n     }\n\n def queue_jobs(self, sequencing_group: SequencingGroup, inputs: StageInput) -&gt; StageOutput | None:\n     input_txt = inputs.as_path(sequencing_group, GeneratePrimes, 'primes')\n     b = get_batch()\n\n     cumulative_calc_output_path = str(self.expected_outputs(sequencing_group).get('cumulative', ''))\n\n     # We define a job instance from the `cumulative_calc` job definition.\n     job_cumulative_calc = cumulative_calc(b, sequencing_group, input_txt, cumulative_calc_output_path)\n\n     jobs = [job_cumulative_calc]\n\n     return self.make_outputs(\n         sequencing_group,\n         data=self.expected_outputs(sequencing_group),\n         jobs=jobs,\n     )\n# ...\n</code></pre> <p>Warning</p> <p>There is a key consideration to take into account when writing the stages:</p> <ul> <li>No Forward Discovery</li> </ul>"},{"location":"usage/#jobspy-or-equivalent-file-for-job-definitions","title":"<code>jobs.py</code> or equivalent file for <code>Job</code> definitions","text":"<p>Every <code>Stage</code> requires a collection of jobs that will be executed within.</p> <p>Tip</p> <p>It is good practice to store these job definitions in their own files, as the definitions can often get long.</p> jobs/cumulative_calc.py<pre><code>from cpg_flow.targets.sequencing_group import SequencingGroup\nfrom hailtop.batch import Batch\nfrom hailtop.batch.job import Job\n\n\ndef cumulative_calc(\n    b: Batch,\n    sequencing_group: SequencingGroup,\n    input_file_path: str,\n    output_file_path: str,\n) -&gt; list[Job]:\n    title = f'Cumulative Calc: {sequencing_group.id}'\n    job = b.new_job(name=title)\n    primes_path = b.read_input(input_file_path)\n\n    cmd = f\"\"\"\n    primes=($(cat {primes_path}))\n    csum=0\n    cumulative=()\n    for prime in \"${{primes[@]}}\"; do\n        ((csum += prime))\n        cumulative+=(\"$csum\")\n    done\n    echo \"${{cumulative[@]}}\" &gt; {job.cumulative}\n    \"\"\"\n\n    job.command(cmd)\n\n    print('-----PRINT CUMULATIVE-----')\n    print(output_file_path)\n    b.write_output(job.cumulative, output_file_path)\n\n    return job\n</code></pre> <p>Once these required components are written, the pipeline is ready to be executed against this framework.</p>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>All pipelines can only be exclusively run using the <code>analysis-runner</code> package which grants the user appropriate permissions based on the dataset and access level defined above. <code>analysis-runner</code> requires a repo, commit and the entrypoint file, and then runs the code inside a \"driver\" image on Hail Batch, logging the invocation to <code>metamist</code> for future audit and reproducibility.</p> <p>Therefore, the pipeline code needs to be pushed to a remote version control system, for <code>analysis-runner</code> to be able to pull it for execution. A job can then be submitted:</p> <pre><code>analysis-runner \\\n  --image \"australia-southeast1-docker.pkg.dev/cpg-common/images/cpg_flow:1.0.0\" \\\n  --dataset \"fewgenomes\" \\\n  --description \"cpg-flow_test\" \\\n  --access-level \"test\" \\\n  --output-dir \"cpg-flow_test\" \\\n  --config \"&lt;YOUR_CONFIG&gt;.toml\" \\\n  workflow.py\n</code></pre> <p>Success</p> <p>If the job is successfully created, the analysis-runner output will include a job URL. This driver job will trigger additional jobs, which can be monitored via the <code>/batches</code> page on Hail. Monitoring these jobs helps verify that the workflow ran successfully. When all expected jobs complete without errors, this confirms the successful execution of the workflow and indicates that the <code>cpg_flow</code> package is functioning as intended.</p> <p>Info</p> <p>See the Docker section for instruction on pulling valid images releases.</p>"},{"location":"workflows/","title":"\ud83c\udfac GitHub Actions <p>This project uses GitHub Actions to automate some boring tasks.</p> <p>You can find all the workflows in the .github/workflows directory.</p>","text":""},{"location":"workflows/#workflows","title":"\ud83c\udfa2 Workflows","text":"Name Description &amp; Status Triggered on Deploy API Documentation Deploys API documentation to GitHub Pages. <code>push</code> on <code>main, alpha</code> and <code>workflow_dispatch</code> Docker Builds and pushes Docker images for the project. <code>pull_request</code> on <code>main, alpha</code> and <code>push</code> on <code>main, alpha</code> and <code>workflow_dispatch</code> Lint Runs linting checks using pre-commit hooks. <code>push</code> Package Packages the project and publishes it to PyPI and GitHub Releases. <code>push</code> on <code>main, alpha</code> Renovate Runs Renovate to update dependencies. <code>schedule</code> and <code>workflow_dispatch</code> Security Checks Performs security checks using pip-audit. <code>workflow_dispatch</code> and <code>push</code> Test Runs unit tests and generates coverage reports. <code>push</code> Update Badges Updates badges.yaml with test results and coverage. <code>workflow_run</code> (completed)"},{"location":"reference/","title":"Reference","text":"<p>Here's the reference or code API, the classes, functions, parameters, attributes, and all the CPG Flow parts you can use in your pipelines.</p>"},{"location":"reference/filetypes/","title":"Filetypes","text":"<p>The following filetypes are available to use:</p> <ul> <li><code>AlignmentInput</code></li> <li><code>CramOrBamPath</code></li> <li><code>BamPath</code></li> <li><code>CramPath</code></li> <li><code>GvcfPath</code></li> <li><code>FastqPair</code></li> <li><code>FastqPairs</code></li> </ul> <p>You can import them from the <code>cpg_flow</code> package:</p> <pre><code>from cpg_flow import AlignmentInput, CramOrBamPath, BamPath, CramPath, GvcfPath, FastqPair, FastqPairs\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.AlignmentInput","title":"cpg_flow.filetypes.AlignmentInput","text":"<p>               Bases: <code>ABC</code></p> <p>Data that works as input for alignment or realignment.</p>"},{"location":"reference/filetypes/#cpg_flow.filetypes.AlignmentInput.exists","title":"exists  <code>abstractmethod</code>","text":"<pre><code>exists()\n</code></pre> <p>Check if all files exist.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>@abstractmethod\ndef exists(self) -&gt; bool:\n    \"\"\"\n    Check if all files exist.\n    \"\"\"\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath","title":"cpg_flow.filetypes.CramOrBamPath","text":"<pre><code>CramOrBamPath(\n    path, index_path=None, reference_assembly=None\n)\n</code></pre> <p>               Bases: <code>AlignmentInput</code>, <code>ABC</code></p> <p>Represents a path to a CRAM or a BAM file, optionally with corresponding index.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path,\n    index_path: str | Path | None = None,\n    reference_assembly: str | Path | None = None,\n):\n    self.path = to_path(path)\n    self.index_path: Path | None = None\n    self.full_index_suffix: str | None = None\n    if index_path:\n        self.index_path = to_path(index_path)\n        assert self.index_path.suffix == f'.{self.index_ext}'\n        self.full_index_suffix = str(self.index_path).replace(\n            str(self.path.with_suffix('')),\n            '',\n        )\n    self.reference_assembly = None\n    if reference_assembly:\n        self.reference_assembly = to_path(reference_assembly)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = to_path(path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.index_path","title":"index_path  <code>instance-attribute</code>","text":"<pre><code>index_path = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.full_index_suffix","title":"full_index_suffix  <code>instance-attribute</code>","text":"<pre><code>full_index_suffix = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.reference_assembly","title":"reference_assembly  <code>instance-attribute</code>","text":"<pre><code>reference_assembly = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.ext","title":"ext  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>ext\n</code></pre> <p>The canonical extension for the file type, without a '.' at the start.</p>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.index_ext","title":"index_ext  <code>abstractmethod</code> <code>property</code>","text":"<pre><code>index_ext\n</code></pre> <p>The canonical index file extension, without a '.' at the start.</p>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>CRAM file exists.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    CRAM file exists.\n    \"\"\"\n    return exists(self.path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramOrBamPath.resource_group","title":"resource_group","text":"<pre><code>resource_group(b)\n</code></pre> <p>Create a Hail Batch resource group</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def resource_group(self, b: Batch) -&gt; ResourceGroup:\n    \"\"\"\n    Create a Hail Batch resource group\n    \"\"\"\n    d = {\n        self.ext: str(self.path),\n    }\n    if self.full_index_suffix:\n        d[self.full_index_suffix] = str(self.index_path)\n\n    return b.read_input_group(**d)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath","title":"cpg_flow.filetypes.BamPath","text":"<pre><code>BamPath(path, index_path=None)\n</code></pre> <p>               Bases: <code>CramOrBamPath</code></p> <p>Represents a path to a BAM file, optionally with corresponding index.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path,\n    index_path: str | Path | None = None,\n):\n    super().__init__(path, index_path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.FILE_EXT","title":"FILE_EXT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILE_EXT = 'bam'\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.FILE_INDEX_EXT","title":"FILE_INDEX_EXT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILE_INDEX_EXT = 'bai'\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.ext","title":"ext  <code>property</code>","text":"<pre><code>ext\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.index_ext","title":"index_ext  <code>property</code>","text":"<pre><code>index_ext\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = to_path(path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.index_path","title":"index_path  <code>instance-attribute</code>","text":"<pre><code>index_path = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.full_index_suffix","title":"full_index_suffix  <code>instance-attribute</code>","text":"<pre><code>full_index_suffix = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.reference_assembly","title":"reference_assembly  <code>instance-attribute</code>","text":"<pre><code>reference_assembly = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>CRAM file exists.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    CRAM file exists.\n    \"\"\"\n    return exists(self.path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.BamPath.resource_group","title":"resource_group","text":"<pre><code>resource_group(b)\n</code></pre> <p>Create a Hail Batch resource group</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def resource_group(self, b: Batch) -&gt; ResourceGroup:\n    \"\"\"\n    Create a Hail Batch resource group\n    \"\"\"\n    d = {\n        self.ext: str(self.path),\n    }\n    if self.full_index_suffix:\n        d[self.full_index_suffix] = str(self.index_path)\n\n    return b.read_input_group(**d)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath","title":"cpg_flow.filetypes.CramPath","text":"<pre><code>CramPath(path, index_path=None, reference_assembly=None)\n</code></pre> <p>               Bases: <code>CramOrBamPath</code></p> <p>Represents a path to a CRAM file, optionally with corresponding index.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def __init__(\n    self,\n    path: str | Path,\n    index_path: str | Path | None = None,\n    reference_assembly: str | Path | None = None,\n):\n    super().__init__(path, index_path, reference_assembly)\n    self.somalier_path = to_path(f'{self.path}.somalier')\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.FILE_EXT","title":"FILE_EXT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILE_EXT = 'cram'\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.FILE_INDEX_EXT","title":"FILE_INDEX_EXT  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILE_INDEX_EXT = 'crai'\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.somalier_path","title":"somalier_path  <code>instance-attribute</code>","text":"<pre><code>somalier_path = to_path(f'{path}.somalier')\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.ext","title":"ext  <code>property</code>","text":"<pre><code>ext\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.index_ext","title":"index_ext  <code>property</code>","text":"<pre><code>index_ext\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = to_path(path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.index_path","title":"index_path  <code>instance-attribute</code>","text":"<pre><code>index_path = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.full_index_suffix","title":"full_index_suffix  <code>instance-attribute</code>","text":"<pre><code>full_index_suffix = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.reference_assembly","title":"reference_assembly  <code>instance-attribute</code>","text":"<pre><code>reference_assembly = None\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>CRAM file exists.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    CRAM file exists.\n    \"\"\"\n    return exists(self.path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.CramPath.resource_group","title":"resource_group","text":"<pre><code>resource_group(b)\n</code></pre> <p>Create a Hail Batch resource group</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def resource_group(self, b: Batch) -&gt; ResourceGroup:\n    \"\"\"\n    Create a Hail Batch resource group\n    \"\"\"\n    d = {\n        self.ext: str(self.path),\n    }\n    if self.full_index_suffix:\n        d[self.full_index_suffix] = str(self.index_path)\n\n    return b.read_input_group(**d)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.GvcfPath","title":"cpg_flow.filetypes.GvcfPath","text":"<pre><code>GvcfPath(path)\n</code></pre> <p>Represents GVCF data on a bucket within the workflow. Includes a path to a GVCF file along with a corresponding TBI index, and a corresponding fingerprint path.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def __init__(self, path: Path | str):\n    self.path = to_path(path)\n    self.somalier_path = to_path(f'{self.path}.somalier')\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.GvcfPath.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path = to_path(path)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.GvcfPath.somalier_path","title":"somalier_path  <code>instance-attribute</code>","text":"<pre><code>somalier_path = to_path(f'{path}.somalier')\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.GvcfPath.tbi_path","title":"tbi_path  <code>property</code>","text":"<pre><code>tbi_path\n</code></pre> <p>Path to the corresponding index</p>"},{"location":"reference/filetypes/#cpg_flow.filetypes.GvcfPath.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>GVCF file exists.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    GVCF file exists.\n    \"\"\"\n    return self.path.exists()\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.GvcfPath.resource_group","title":"resource_group","text":"<pre><code>resource_group(b)\n</code></pre> <p>Create a Hail Batch resource group</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def resource_group(self, b: Batch) -&gt; ResourceGroup:\n    \"\"\"\n    Create a Hail Batch resource group\n    \"\"\"\n    return b.read_input_group(\n        **{\n            'g.vcf.gz': str(self.path),\n            'g.vcf.gz.tbi': str(self.tbi_path),\n        },\n    )\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPair","title":"cpg_flow.filetypes.FastqPair  <code>dataclass</code>","text":"<pre><code>FastqPair(r1, r2)\n</code></pre> <p>               Bases: <code>AlignmentInput</code></p> <p>Pair of FASTQ files</p>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPair.r1","title":"r1  <code>instance-attribute</code>","text":"<pre><code>r1\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPair.r2","title":"r2  <code>instance-attribute</code>","text":"<pre><code>r2\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPair.as_resources","title":"as_resources","text":"<pre><code>as_resources(b)\n</code></pre> <p>Makes a pair of ResourceFile objects for r1 and r2.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def as_resources(self, b) -&gt; 'FastqPair':\n    \"\"\"\n    Makes a pair of ResourceFile objects for r1 and r2.\n    \"\"\"\n    return FastqPair(\n        *[(self[i] if isinstance(self[i], ResourceFile) else b.read_input(str(self[i]))) for i in [0, 1]],\n    )\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPair.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>Check if each FASTQ file in the pair exists.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    Check if each FASTQ file in the pair exists.\n    \"\"\"\n    return exists(self.r1) and exists(self.r2)\n</code></pre>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPairs","title":"cpg_flow.filetypes.FastqPairs","text":"<p>               Bases: <code>list[FastqPair | FastqOraPair]</code>, <code>AlignmentInput</code></p> <p>Multiple FASTQ file pairs belonging to the same sequencing_group (e.g. multiple lanes or top-ups).</p>"},{"location":"reference/filetypes/#cpg_flow.filetypes.FastqPairs.exists","title":"exists","text":"<pre><code>exists()\n</code></pre> <p>Check if each FASTQ file in each pair exists.</p> Source code in <code>src/cpg_flow/filetypes.py</code> <pre><code>def exists(self) -&gt; bool:\n    \"\"\"\n    Check if each FASTQ file in each pair exists.\n    \"\"\"\n    return all(pair.exists() for pair in self)\n</code></pre>"},{"location":"reference/inputs/","title":"Inputs","text":"<p>There are Metamist wrappers built to get input sequencing groups.</p> <p>You can import these from the <code>cpg_flow</code> package:</p> <pre><code>from cpg_flow.inputs import add_sg_to_dataset, get_multicohort, create_multicohort\n</code></pre>"},{"location":"reference/inputs/#cpg_flow.inputs.add_sg_to_dataset","title":"cpg_flow.inputs.add_sg_to_dataset","text":"<pre><code>add_sg_to_dataset(dataset, sg_data)\n</code></pre> <p>Adds a sequencing group to a dataset.</p> PARAMETER DESCRIPTION <code>dataset</code> <p>Dataset to insert the SequencingGroup into</p> <p> TYPE: <code>Dataset</code> </p> <code>sg_data</code> <p>data from the metamist API</p> <p> TYPE: <code>dict</code> </p> RETURNS DESCRIPTION <code>SequencingGroup</code> <p>The SequencingGroup object</p> Source code in <code>src/cpg_flow/inputs.py</code> <pre><code>def add_sg_to_dataset(dataset: Dataset, sg_data: dict) -&gt; SequencingGroup:\n    \"\"\"\n    Adds a sequencing group to a dataset.\n\n    Args:\n        dataset (Dataset): Dataset to insert the SequencingGroup into\n        sg_data (dict): data from the metamist API\n\n    Returns:\n        The SequencingGroup object\n    \"\"\"\n    # TODO: The update_dict calls are a bit of a hack, we should be able to do this in a cleaner way\n    # scavenge all the metadata from the SG dict (SG/Sample/Participant)\n    metadata = sg_data.get('meta', {})\n    update_dict(metadata, sg_data['sample']['participant'].get('meta', {}))\n\n    # phenotypes are managed badly here, need a cleaner way to get them into the SG\n    update_dict(\n        metadata,\n        {'phenotypes': sg_data['sample']['participant'].get('phenotypes', {})},\n    )\n\n    # create a SequencingGroup object from its component parts\n    sequencing_group = dataset.add_sequencing_group(\n        id=str(sg_data['id']),\n        external_id=str(sg_data['sample']['externalId']),\n        participant_id=sg_data['sample']['participant'].get('externalId'),\n        meta=metadata,\n        sequencing_type=sg_data['type'],\n        sequencing_technology=sg_data['technology'],\n        sequencing_platform=sg_data['platform'],\n    )\n\n    if reported_sex := sg_data['sample']['participant'].get('reportedSex'):\n        sequencing_group.pedigree.sex = Sex.parse(reported_sex)\n\n    # parse the assays and related dict content\n    if config_retrieve(['workflow', 'populate_assays'], None):\n        _populate_assays(sequencing_group, sg_data)\n\n    return sequencing_group\n</code></pre>"},{"location":"reference/inputs/#cpg_flow.inputs.get_multicohort","title":"cpg_flow.inputs.get_multicohort","text":"<pre><code>get_multicohort()\n</code></pre> <p>Return the cohort or multicohort object based on the workflow configuration.</p> Source code in <code>src/cpg_flow/inputs.py</code> <pre><code>def get_multicohort() -&gt; MultiCohort:\n    \"\"\"\n    Return the cohort or multicohort object based on the workflow configuration.\n    \"\"\"\n    input_datasets = config_retrieve(['workflow', 'input_datasets'], None)\n\n    # pull the list of cohort IDs from the config\n    custom_cohort_ids = config_retrieve(['workflow', 'input_cohorts'], None)\n\n    if input_datasets:\n        raise ValueError('Argument input_datasets is deprecated, use input_cohorts instead')\n\n    if isinstance(custom_cohort_ids, list) and len(custom_cohort_ids) &lt;= 0:\n        raise ValueError('No custom_cohort_ids found in the config')\n\n    # NOTE: When configuring sgs in the config is deprecated, this will be removed.\n    if custom_cohort_ids and not isinstance(custom_cohort_ids, list):\n        raise ValueError('Argument input_cohorts must be a list')\n\n    # After the check for no cusotom_cohort_ids in the config convert\n    # to a tuple for the cache decorator\n    custom_cohort_ids = tuple() if not custom_cohort_ids else tuple(custom_cohort_ids)\n\n    return create_multicohort(custom_cohort_ids)\n</code></pre>"},{"location":"reference/inputs/#cpg_flow.inputs.create_multicohort","title":"cpg_flow.inputs.create_multicohort  <code>cached</code>","text":"<pre><code>create_multicohort(custom_cohort_ids)\n</code></pre> <p>Add cohorts in the multicohort.</p> Source code in <code>src/cpg_flow/inputs.py</code> <pre><code>@cache\ndef create_multicohort(custom_cohort_ids: tuple[str]) -&gt; MultiCohort:\n    \"\"\"\n    Add cohorts in the multicohort.\n    \"\"\"\n    # get a unique set of cohort IDs\n    custom_cohort_ids_unique = sorted(set(custom_cohort_ids))\n    custom_cohort_ids_removed = sorted(set(custom_cohort_ids) - set(custom_cohort_ids_unique))\n\n    # if any cohort id duplicates were removed we log them\n    if len(custom_cohort_ids_unique) != len(custom_cohort_ids):\n        logger.warning(\n            f'Removed {len(custom_cohort_ids_removed)} non-unique cohort IDs',\n        )\n        duplicated_cohort_ids = ', '.join(custom_cohort_ids_removed)\n        logger.warning(f'Non-unique cohort IDs: {duplicated_cohort_ids}')\n\n    multicohort = MultiCohort()\n\n    # for each Cohort ID\n    for cohort_id in custom_cohort_ids_unique:\n        # get the dictionary representation of all SGs in this cohort\n        # dataset_id is sequencing_group_dict['sample']['project']['name']\n        cohort_sg_dict = get_cohort_sgs(cohort_id)\n        cohort_name = cohort_sg_dict.get('name', cohort_id)\n        cohort_dataset = cohort_sg_dict.get('dataset', None)\n        cohort_sgs = cohort_sg_dict.get('sequencing_groups', [])\n\n        if len(cohort_sgs) == 0:\n            raise MetamistError(f'Cohort {cohort_id} has no sequencing groups')\n\n        # create a new Cohort object\n        cohort = multicohort.create_cohort(\n            id=cohort_id,\n            name=cohort_name,\n            dataset=cohort_dataset,\n        )\n\n        # first populate these SGs into their Datasets\n        # required so that the SG objects can be referenced in the collective Datasets\n        # SG.dataset.prefix is meaningful, to correctly store outputs in the project location\n        for entry in cohort_sgs:\n            sg_dataset = entry['sample']['project']['name']\n            dataset = multicohort.create_dataset(sg_dataset.removesuffix('-test'))\n\n            sequencing_group = add_sg_to_dataset(dataset, entry)\n\n            # also add the same sequencing group to the cohort\n            cohort.add_sequencing_group_object(sequencing_group)\n\n    # we've populated all the sequencing groups in the cohorts and datasets\n    # all SequencingGroup objects should be populated uniquely (pointers to instances, so updating Analysis entries\n    # for each SG should update both the Dataset's version and the Cohort's version)\n\n    # only go to metamist once per dataset to get analysis entries\n    for dataset in multicohort.get_datasets():\n        _populate_analysis(dataset)\n        if config_retrieve(['workflow', 'read_pedigree'], True):\n            _populate_pedigree(dataset)\n\n    return multicohort\n</code></pre>"},{"location":"reference/metamist/","title":"Metamist","text":"<p>Helpers to communicate with the <code>metamist</code> database.</p>"},{"location":"reference/metamist/#cpg_flow.metamist.get_metamist","title":"cpg_flow.metamist.get_metamist","text":"<pre><code>get_metamist()\n</code></pre> <p>Return the cohort object</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def get_metamist() -&gt; 'Metamist':\n    \"\"\"Return the cohort object\"\"\"\n    global _metamist\n    if not _metamist:\n        _metamist = Metamist()\n    return _metamist\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist","title":"cpg_flow.metamist.Metamist","text":"<pre><code>Metamist()\n</code></pre> <p>Communication with metamist.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.default_dataset: str = get_config()['workflow']['dataset']\n    self.aapi = AnalysisApi()\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.default_dataset","title":"default_dataset  <code>instance-attribute</code>","text":"<pre><code>default_dataset = get_config()['workflow']['dataset']\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.aapi","title":"aapi  <code>instance-attribute</code>","text":"<pre><code>aapi = AnalysisApi()\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.make_retry_aapi_call","title":"make_retry_aapi_call","text":"<pre><code>make_retry_aapi_call(api_func, **kwargv)\n</code></pre> <p>Make a generic API call to self.aapi with retries. Retry only if ServiceException is thrown</p> <p>TODO: How many retries? e.g. try 3 times, wait 2^3: 8, 16, 24 seconds</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=3, min=8, max=30),\n    retry=retry_if_exception_type(ServiceException),\n    reraise=True,\n)\ndef make_retry_aapi_call(self, api_func: Callable, **kwargv: Any):  # noqa: PLR6301\n    \"\"\"\n    Make a generic API call to self.aapi with retries.\n    Retry only if ServiceException is thrown\n\n    TODO: How many retries?\n    e.g. try 3 times, wait 2^3: 8, 16, 24 seconds\n    \"\"\"\n    try:\n        return api_func(**kwargv)\n    except ServiceException:\n        # raise here so the retry occurs\n        logger.warning(\n            f'Retrying {api_func} ...',\n        )\n        raise\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.make_aapi_call","title":"make_aapi_call","text":"<pre><code>make_aapi_call(api_func, **kwargv)\n</code></pre> <p>Make a generic API call to self.aapi. This is a wrapper around retry of API call to handle exceptions and logger.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def make_aapi_call(self, api_func: Callable, **kwargv: Any):\n    \"\"\"\n    Make a generic API call to self.aapi.\n    This is a wrapper around retry of API call to handle exceptions and logger.\n    \"\"\"\n    try:\n        return self.make_retry_aapi_call(api_func, **kwargv)\n    except (ServiceException, ApiException) as e:\n        # Metamist API failed even after retries\n        # log the error and continue\n        traceback.print_exc()\n        logger.error(\n            f'Error: {e} Call {api_func} failed with payload:\\n{kwargv!s}',\n        )\n    # TODO: discuss should we catch all here as well?\n    # except Exception as e:\n    #     # Other exceptions?\n\n    return None\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.get_sg_entries","title":"get_sg_entries","text":"<pre><code>get_sg_entries(dataset_name)\n</code></pre> <p>Retrieve sequencing group entries for a dataset, in the context of access level and filtering options.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=3, min=8, max=30),\n    retry=retry_if_exception_type(TransportServerError),\n    reraise=True,\n)\ndef get_sg_entries(self, dataset_name: str) -&gt; list[dict]:\n    \"\"\"\n    Retrieve sequencing group entries for a dataset, in the context of access level\n    and filtering options.\n    \"\"\"\n    metamist_proj = self.get_metamist_proj(dataset_name)\n    logger.info(f'Getting sequencing groups for dataset {metamist_proj}')\n\n    skip_sgs = get_config()['workflow'].get('skip_sgs', [])\n    only_sgs = get_config()['workflow'].get('only_sgs', [])\n    sequencing_type = get_config()['workflow'].get('sequencing_type')\n\n    if only_sgs and skip_sgs:\n        raise MetamistError('Cannot specify both only_sgs and skip_sgs in config')\n\n    sequencing_group_entries = query(\n        GET_SEQUENCING_GROUPS_QUERY,\n        variables={\n            'metamist_proj': metamist_proj,\n            'only_sgs': only_sgs,\n            'skip_sgs': skip_sgs,\n            'sequencing_type': sequencing_type,\n        },\n    )\n\n    return sequencing_group_entries['project']['sequencingGroups']\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.update_analysis","title":"update_analysis","text":"<pre><code>update_analysis(analysis, status)\n</code></pre> <p>Update \"status\" of an Analysis entry.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def update_analysis(self, analysis: Analysis, status: AnalysisStatus):\n    \"\"\"\n    Update \"status\" of an Analysis entry.\n    \"\"\"\n    self.make_aapi_call(\n        self.aapi.update_analysis,\n        analysis_id=analysis.id,\n        analysis_update_model=models.AnalysisUpdateModel(\n            status=models.AnalysisStatus(status.value),\n        ),\n    )\n    # Keeping this as is for compatibility with the existing code\n    # However this should only be set after the API call is successful\n    analysis.status = status\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.get_analyses_by_sgid","title":"get_analyses_by_sgid","text":"<pre><code>get_analyses_by_sgid(\n    analysis_type, analysis_status=COMPLETED, dataset=None\n)\n</code></pre> <p>Query the DB to find the last completed analysis for the type, sequencing group ids, and sequencing type, one Analysis object per sequencing group. Assumes the analysis is defined for a single sequencing group (that is, analysis_type=cram|gvcf|qc).</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def get_analyses_by_sgid(\n    self,\n    analysis_type: AnalysisType,\n    analysis_status: AnalysisStatus = AnalysisStatus.COMPLETED,\n    dataset: str | None = None,\n) -&gt; dict[str, Analysis]:\n    \"\"\"\n    Query the DB to find the last completed analysis for the type, sequencing group ids,\n    and sequencing type, one Analysis object per sequencing group. Assumes the analysis\n    is defined for a single sequencing group (that is, analysis_type=cram|gvcf|qc).\n    \"\"\"\n    metamist_proj = self.get_metamist_proj(dataset)\n\n    analyses = query(\n        GET_ANALYSES_QUERY,\n        variables={\n            'metamist_proj': metamist_proj,\n            'analysis_type': analysis_type.value,\n            'analysis_status': analysis_status.name,\n        },\n    )\n\n    analysis_per_sid: dict[str, Analysis] = dict()\n\n    for analysis in analyses['project']['analyses']:\n        a = Analysis.parse(analysis)\n        if not a:\n            continue\n\n        assert a.status == analysis_status, analysis\n        assert a.type == analysis_type, analysis\n        if len(a.sequencing_group_ids) &lt; 1:\n            logger.warning(f'Analysis has no sequencing group ids. {analysis}')\n            continue\n\n        assert len(a.sequencing_group_ids) == 1, analysis\n        analysis_per_sid[list(a.sequencing_group_ids)[0]] = a\n\n    logger.info(\n        f'Querying {analysis_type} analysis entries for {metamist_proj}: found {len(analysis_per_sid)}',\n    )\n    return analysis_per_sid\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.create_analysis","title":"create_analysis","text":"<pre><code>create_analysis(\n    output,\n    type_,\n    status,\n    cohort_ids=None,\n    sequencing_group_ids=None,\n    dataset=None,\n    meta=None,\n)\n</code></pre> <p>Tries to create an Analysis entry, returns its id if successful.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def create_analysis(  # noqa: PLR0917\n    self,\n    output: Path | str,\n    type_: str | AnalysisType,\n    status: str | AnalysisStatus,\n    cohort_ids: list[str] | None = None,\n    sequencing_group_ids: list[str] | None = None,\n    dataset: str | None = None,\n    meta: dict | None = None,\n) -&gt; int | None:\n    \"\"\"\n    Tries to create an Analysis entry, returns its id if successful.\n    \"\"\"\n    metamist_proj = self.get_metamist_proj(dataset)\n\n    if isinstance(type_, AnalysisType):\n        type_ = type_.value\n    if isinstance(status, AnalysisStatus):\n        status = status.value\n\n    if not cohort_ids:\n        cohort_ids = []\n\n    if not sequencing_group_ids:\n        sequencing_group_ids = []\n\n    am = models.Analysis(\n        type=type_,\n        status=models.AnalysisStatus(status),\n        output=str(output),\n        cohort_ids=list(cohort_ids),\n        sequencing_group_ids=list(sequencing_group_ids),\n        meta=meta or {},\n    )\n    aid = self.make_aapi_call(\n        self.aapi.create_analysis,\n        project=metamist_proj,\n        analysis=am,\n    )\n    if aid is None:\n        logger.error(\n            f'Failed to create Analysis(type={type_}, status={status}, output={output!s}) in {metamist_proj}',\n        )\n        return None\n    logger.info(\n        f'Created Analysis(id={aid}, type={type_}, status={status}, output={output!s}) in {metamist_proj}',\n    )\n    return aid\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.get_ped_entries","title":"get_ped_entries","text":"<pre><code>get_ped_entries(dataset=None)\n</code></pre> <p>Retrieve PED lines for a specified SM project, with external participant IDs.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def get_ped_entries(self, dataset: str | None = None) -&gt; list[dict[str, str]]:\n    \"\"\"\n    Retrieve PED lines for a specified SM project, with external participant IDs.\n    \"\"\"\n    metamist_proj = self.get_metamist_proj(dataset)\n    entries = query(GET_PEDIGREE_QUERY, variables={'metamist_proj': metamist_proj})\n\n    pedigree_entries = entries['project']['pedigree']\n\n    return pedigree_entries\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Metamist.get_metamist_proj","title":"get_metamist_proj","text":"<pre><code>get_metamist_proj(dataset=None)\n</code></pre> <p>Return the Metamist project name, appending '-test' if the access level is 'test'.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def get_metamist_proj(self, dataset: str | None = None) -&gt; str:\n    \"\"\"\n    Return the Metamist project name, appending '-test' if the access level is 'test'.\n    \"\"\"\n    metamist_proj = dataset or self.default_dataset\n    if config_retrieve(['workflow', 'access_level']) == 'test' and not metamist_proj.endswith('-test'):\n        metamist_proj += '-test'\n\n    return metamist_proj\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus","title":"cpg_flow.metamist.AnalysisStatus","text":"<p>               Bases: <code>Enum</code></p> <p>Corresponds to metamist Analysis statuses: https://github.com/populationgenomics/sample-metadata/blob/dev/models/enums/analysis.py#L14-L21</p>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus.QUEUED","title":"QUEUED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QUEUED = 'queued'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus.IN_PROGRESS","title":"IN_PROGRESS  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IN_PROGRESS = 'in-progress'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus.FAILED","title":"FAILED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FAILED = 'failed'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus.COMPLETED","title":"COMPLETED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMPLETED = 'completed'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus.UNKNOWN","title":"UNKNOWN  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNKNOWN = 'unknown'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisStatus.parse","title":"parse  <code>staticmethod</code>","text":"<pre><code>parse(name)\n</code></pre> <p>Parse str and create a AnalysisStatus object</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>@staticmethod\ndef parse(name: str) -&gt; 'AnalysisStatus':\n    \"\"\"\n    Parse str and create a AnalysisStatus object\n    \"\"\"\n    return {v.value: v for v in AnalysisStatus}[name.lower()]\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType","title":"cpg_flow.metamist.AnalysisType","text":"<p>               Bases: <code>Enum</code></p> <p>Corresponds to metamist Analysis types: https://github.com/populationgenomics/sample-metadata/blob/dev/models/enums /analysis.py#L4-L11</p> <p>Re-defined in a separate module to decouple from the main metamist module, so decorators can use <code>@stage(analysis_type=AnalysisType.QC)</code> without importing the metamist package.</p>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.QC","title":"QC  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QC = 'qc'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.JOINT_CALLING","title":"JOINT_CALLING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>JOINT_CALLING = 'joint-calling'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.GVCF","title":"GVCF  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>GVCF = 'gvcf'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.CRAM","title":"CRAM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CRAM = 'cram'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.MITO_CRAM","title":"MITO_CRAM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>MITO_CRAM = 'mito-cram'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.CUSTOM","title":"CUSTOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CUSTOM = 'custom'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.ES_INDEX","title":"ES_INDEX  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ES_INDEX = 'es-index'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.COMBINER","title":"COMBINER  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>COMBINER = 'combiner'\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.AnalysisType.parse","title":"parse  <code>staticmethod</code>","text":"<pre><code>parse(val)\n</code></pre> <p>Parse str and create a AnalysisStatus object</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>@staticmethod\ndef parse(val: str) -&gt; 'AnalysisType':\n    \"\"\"\n    Parse str and create a AnalysisStatus object\n    \"\"\"\n    d = {v.value: v for v in AnalysisType}\n    if val not in d:\n        raise MetamistError(\n            f'Unrecognised analysis type {val}. Available: {list(d.keys())}',\n        )\n    return d[val.lower()]\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis","title":"cpg_flow.metamist.Analysis  <code>dataclass</code>","text":"<pre><code>Analysis(\n    id, type, status, sequencing_group_ids, output, meta\n)\n</code></pre> <p>Metamist DB Analysis entry.</p> <p>See the metamist package for more details: https://github.com/populationgenomics/sample-metadata</p>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.type","title":"type  <code>instance-attribute</code>","text":"<pre><code>type\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.status","title":"status  <code>instance-attribute</code>","text":"<pre><code>status\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.sequencing_group_ids","title":"sequencing_group_ids  <code>instance-attribute</code>","text":"<pre><code>sequencing_group_ids\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.meta","title":"meta  <code>instance-attribute</code>","text":"<pre><code>meta\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Analysis.parse","title":"parse  <code>staticmethod</code>","text":"<pre><code>parse(data)\n</code></pre> <p>Parse data to create an Analysis object.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>@staticmethod\ndef parse(data: dict) -&gt; 'Analysis':\n    \"\"\"\n    Parse data to create an Analysis object.\n    \"\"\"\n    req_keys = ['id', 'type', 'status']\n    if any(k not in data for k in req_keys):\n        for key in req_keys:\n            if key not in data:\n                logger.error(f'\"Analysis\" data does not have {key}: {data}')\n        raise ValueError(f'Cannot parse metamist Sequence {data}')\n\n    output = data.get('output')\n    if output:\n        output = to_path(output)\n\n    a = Analysis(\n        id=int(data['id']),\n        type=AnalysisType.parse(data['type']),\n        status=AnalysisStatus.parse(data['status']),\n        sequencing_group_ids=set([s['id'] for s in data['sequencingGroups']]),\n        output=output,\n        meta=data.get('meta') or {},\n    )\n    return a\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay","title":"cpg_flow.metamist.Assay  <code>dataclass</code>","text":"<pre><code>Assay(\n    id,\n    sequencing_group_id,\n    meta,\n    assay_type,\n    alignment_input=None,\n)\n</code></pre> <p>Metamist \"Assay\" entry.</p> <p>See metamist for more details: https://github.com/populationgenomics/sample-metadata</p>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay.sequencing_group_id","title":"sequencing_group_id  <code>instance-attribute</code>","text":"<pre><code>sequencing_group_id\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay.meta","title":"meta  <code>instance-attribute</code>","text":"<pre><code>meta\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay.assay_type","title":"assay_type  <code>instance-attribute</code>","text":"<pre><code>assay_type\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay.alignment_input","title":"alignment_input  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>alignment_input = None\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.Assay.parse","title":"parse  <code>staticmethod</code>","text":"<pre><code>parse(\n    data, sg_id, check_existence=False, run_parse_reads=True\n)\n</code></pre> <p>Create from a dictionary.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>@staticmethod\ndef parse(\n    data: dict,\n    sg_id: str,\n    check_existence: bool = False,\n    run_parse_reads: bool = True,\n) -&gt; 'Assay':\n    \"\"\"\n    Create from a dictionary.\n    \"\"\"\n\n    assay_keys = ['id', 'type', 'meta']\n    missing_keys = [key for key in assay_keys if data.get(key) is None]\n\n    if missing_keys:\n        raise ValueError(\n            f'Cannot parse metamist Sequence {data}. Missing keys: {missing_keys}',\n        )\n\n    assay_type = str(data['type'])\n    assert assay_type, data\n    mm_seq = Assay(\n        id=str(data['id']),\n        sequencing_group_id=sg_id,\n        meta=data['meta'],\n        assay_type=assay_type,\n    )\n    if run_parse_reads:\n        mm_seq.alignment_input = parse_reads(\n            sequencing_group_id=sg_id,\n            assay_meta=data['meta'],\n            check_existence=check_existence,\n        )\n    return mm_seq\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.get_cohort_sgs","title":"cpg_flow.metamist.get_cohort_sgs","text":"<pre><code>get_cohort_sgs(cohort_id)\n</code></pre> <p>Retrieve sequencing group entries for a single cohort.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def get_cohort_sgs(cohort_id: str) -&gt; dict:\n    \"\"\"\n    Retrieve sequencing group entries for a single cohort.\n    \"\"\"\n    logger.info(f'Getting sequencing groups for cohort {cohort_id}')\n    entries = query(GET_SEQUENCING_GROUPS_BY_COHORT_QUERY, {'cohort_id': cohort_id})\n\n    # Create dictionary keying sequencing groups by project and including cohort name\n    # {\n    #     \"sequencing_groups\": {\n    #         project_id: [sequencing_group_1, sequencing_group_2, ...],\n    #         ...\n    #     },\n    #     \"name\": \"CohortName\"\n    #     \"dataset\": \"DatasetName\"\n    # }\n\n    if len(entries['cohorts']) != 1:\n        raise MetamistError('We only support one cohort at a time currently')\n\n    if entries.get('data') is None and 'errors' in entries:\n        message = entries['errors'][0]['message']\n        raise MetamistError(f'Error fetching cohort: {message}')\n\n    cohort_status = entries['cohorts'][0]['status']\n    if cohort_status.lower() != 'active':  # support upper and lower formats during migration\n        raise MetamistError(\n            f'Cohort {cohort_id} is {cohort_status}. Only active cohorts are allowed. Please check the input cohort list.'\n        )\n\n    cohort_name = entries['cohorts'][0]['name']\n    cohort_dataset = entries['cohorts'][0]['project']['dataset']\n    sequencing_groups = entries['cohorts'][0]['sequencingGroups']\n\n    return {\n        'name': cohort_name,\n        'dataset': cohort_dataset,\n        'sequencing_groups': sequencing_groups,\n    }\n</code></pre>"},{"location":"reference/metamist/#cpg_flow.metamist.parse_reads","title":"cpg_flow.metamist.parse_reads","text":"<pre><code>parse_reads(\n    sequencing_group_id, assay_meta, check_existence\n)\n</code></pre> <p>Parse a AlignmentInput object from the meta dictionary. <code>check_existence</code>: check if fastq/crams exist on buckets. Default value is pulled from self.metamist and can be overridden.</p> Source code in <code>src/cpg_flow/metamist.py</code> <pre><code>def parse_reads(\n    sequencing_group_id: str,\n    assay_meta: dict,\n    check_existence: bool,\n) -&gt; AlignmentInput:\n    \"\"\"\n    Parse a AlignmentInput object from the meta dictionary.\n    `check_existence`: check if fastq/crams exist on buckets.\n    Default value is pulled from self.metamist and can be overridden.\n    \"\"\"\n    reads_data = assay_meta.get('reads')\n    reads_type = assay_meta.get('reads_type')\n    reference_assembly = assay_meta.get('reference_assembly', {}).get('location')\n\n    if not reads_data:\n        raise MetamistError(f'{sequencing_group_id}: no \"meta/reads\" field in meta')\n\n    if not reads_type:\n        raise MetamistError(\n            f'{sequencing_group_id}: no \"meta/reads_type\" field in meta',\n        )\n\n    if reads_type in {'bam', 'cram'} and len(reads_data) &gt; 1:\n        raise MetamistError(\n            f'{sequencing_group_id}: supporting only single bam/cram input',\n        )\n\n    if reads_type not in SUPPORTED_READ_TYPES:\n        raise MetamistError(\n            f'{sequencing_group_id}: ERROR: \"reads_type\" is expected to be one of {sorted(SUPPORTED_READ_TYPES)}',\n        )\n\n    if reads_type in {'bam', 'cram'}:\n        return find_cram_or_bam(\n            reads_data,\n            sequencing_group_id,\n            check_existence,\n            reference_assembly,\n            access_level=config_retrieve(['workflow', 'access_level']),\n        )\n\n    # special handling case for FQ.ora files, these require a reference for decompression\n    elif reads_type == 'fastq_ora':\n        if (ora_reference := assay_meta.get('ora_reference')) is None or (\n            reference_location := ora_reference.get('location')\n        ) is None:\n            raise MetamistError(f'\"meta.ora_reference.location\" is mandatory for {reads_type} assays: \\n{assay_meta}')\n        return find_fastqs(reads_data, sequencing_group_id, check_existence, read_reference=reference_location)\n\n    else:\n        return find_fastqs(reads_data, sequencing_group_id, check_existence)\n</code></pre>"},{"location":"reference/pedigree_info/","title":"<code>PedigreeInfo</code> class","text":"<p>You can import this class directly from the <code>cpg_flow</code> package:</p> <pre><code>from cpg_flow.targets import PedigreeInfo\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo","title":"cpg_flow.targets.PedigreeInfo  <code>dataclass</code>","text":"<pre><code>PedigreeInfo(\n    sequencing_group,\n    sex=UNKNOWN,\n    fam_id=None,\n    phenotype=0,\n    dad=None,\n    mom=None,\n)\n</code></pre> <p>Pedigree relationships with other sequencing groups in the cohort, and other PED data</p>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.sequencing_group","title":"sequencing_group  <code>instance-attribute</code>","text":"<pre><code>sequencing_group\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.sex","title":"sex  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sex = UNKNOWN\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.fam_id","title":"fam_id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fam_id = None\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.phenotype","title":"phenotype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>phenotype = 0\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.dad","title":"dad  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dad = None\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.mom","title":"mom  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mom = None\n</code></pre>"},{"location":"reference/pedigree_info/#cpg_flow.targets.PedigreeInfo.get_ped_dict","title":"get_ped_dict","text":"<pre><code>get_ped_dict(use_participant_id=False)\n</code></pre> <p>Returns a dictionary of pedigree fields for this sequencing group, corresponding a PED file entry.</p> Source code in <code>src/cpg_flow/targets/pedigree_info.py</code> <pre><code>def get_ped_dict(self, use_participant_id: bool = False) -&gt; dict[str, str]:\n    \"\"\"\n    Returns a dictionary of pedigree fields for this sequencing group, corresponding\n    a PED file entry.\n    \"\"\"\n\n    def _get_id(_s: Union['SequencingGroup', None]) -&gt; str:\n        if _s is None:\n            return '0'\n        if use_participant_id:\n            return _s.participant_id\n        return _s.id\n\n    return {\n        'Family.ID': self.fam_id or self.sequencing_group.participant_id,\n        'Individual.ID': _get_id(self.sequencing_group),\n        'Father.ID': _get_id(self.dad),\n        'Mother.ID': _get_id(self.mom),\n        'Sex': str(self.sex.value),\n        'Phenotype': str(self.phenotype),\n    }\n</code></pre>"},{"location":"reference/resources/","title":"Resources","text":"<p>The following resources are available for use:</p>"},{"location":"reference/resources/#cpg_flow.resources.gcp_machine_name","title":"cpg_flow.resources.gcp_machine_name","text":"<pre><code>gcp_machine_name(name, ncpu)\n</code></pre> <p>Machine type name in the GCP world</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def gcp_machine_name(name: str, ncpu: int) -&gt; str:\n    \"\"\"\n    Machine type name in the GCP world\n    \"\"\"\n    assert name in {'standard', 'highmem', 'highcpu'}, name\n    assert is_power_of_two(ncpu), ncpu\n    return f'n1-{name}-{ncpu}'\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType","title":"cpg_flow.resources.MachineType  <code>dataclass</code>","text":"<pre><code>MachineType(\n    name,\n    ncpu,\n    mem_gb_per_core,\n    price_per_hour,\n    disk_size_gb,\n)\n</code></pre> <p>Hail Batch machine type on GCP</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    ncpu: int,\n    mem_gb_per_core: float,\n    price_per_hour: float,\n    disk_size_gb: int,\n):\n    self.name = name\n    self.max_ncpu = ncpu\n    self.mem_gb_per_core = mem_gb_per_core\n    self.price_per_hour = price_per_hour\n    self.disk_size_gb = disk_size_gb\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.min_cpu","title":"min_cpu  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_cpu = 2\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.threads_on_cpu","title":"threads_on_cpu  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>threads_on_cpu = 1\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.max_ncpu","title":"max_ncpu  <code>instance-attribute</code>","text":"<pre><code>max_ncpu = ncpu\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.mem_gb_per_core","title":"mem_gb_per_core  <code>instance-attribute</code>","text":"<pre><code>mem_gb_per_core = mem_gb_per_core\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.price_per_hour","title":"price_per_hour  <code>instance-attribute</code>","text":"<pre><code>price_per_hour = price_per_hour\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.disk_size_gb","title":"disk_size_gb  <code>instance-attribute</code>","text":"<pre><code>disk_size_gb = disk_size_gb\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.max_threads","title":"max_threads","text":"<pre><code>max_threads()\n</code></pre> <p>Number of available threads</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def max_threads(self) -&gt; int:\n    \"\"\"\n    Number of available threads\n    \"\"\"\n    return self.max_ncpu * self.threads_on_cpu\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.calc_instance_disk_gb","title":"calc_instance_disk_gb","text":"<pre><code>calc_instance_disk_gb()\n</code></pre> <p>The maximum available storage on an instance is calculated in <code>batch/batch/utils.py/unreserved_worker_data_disk_size_gib()</code> as the disk size (375G) minus reserved image size (30G) minus reserved storage per core (5G*ncpu = 120G for a 32-core instance),</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def calc_instance_disk_gb(self) -&gt; int:\n    \"\"\"\n    The maximum available storage on an instance is calculated\n    in `batch/batch/utils.py/unreserved_worker_data_disk_size_gib()`\n    as the disk size (375G) minus reserved image size (30G) minus\n    reserved storage per core (5G*ncpu = 120G for a 32-core instance),\n    \"\"\"\n    reserved_gb = 30\n    reserved_gb_per_core = 5\n    return self.disk_size_gb - reserved_gb - reserved_gb_per_core * self.max_ncpu\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.set_resources","title":"set_resources","text":"<pre><code>set_resources(\n    *,\n    j,\n    fraction=None,\n    ncpu=None,\n    nthreads=None,\n    mem_gb=None,\n    storage_gb=None\n)\n</code></pre> <p>Set resources to a Job object. If any optional parameters are set, they will be used as a bound to request a fraction of an instance.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def set_resources(\n    self,\n    *,\n    j: Job,\n    fraction: float | None = None,\n    ncpu: int | None = None,\n    nthreads: int | None = None,\n    mem_gb: float | None = None,\n    storage_gb: float | None = None,\n) -&gt; 'JobResource':\n    \"\"\"\n    Set resources to a Job object. If any optional parameters are set,\n    they will be used as a bound to request a fraction of an instance.\n    \"\"\"\n    return self.request_resources(\n        fraction=fraction,\n        ncpu=ncpu,\n        nthreads=nthreads,\n        mem_gb=mem_gb,\n        storage_gb=storage_gb,\n    ).set_to_job(j)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.request_resources","title":"request_resources","text":"<pre><code>request_resources(\n    fraction=None,\n    ncpu=None,\n    nthreads=None,\n    mem_gb=None,\n    storage_gb=None,\n)\n</code></pre> <p>Request resources from the machine, satisfying all provided requirements. If not requirements are provided, the minimal amount of cores (self.MIN_NCPU) will be used.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def request_resources(\n    self,\n    fraction: float | None = None,\n    ncpu: int | None = None,\n    nthreads: int | None = None,\n    mem_gb: float | None = None,\n    storage_gb: float | None = None,\n) -&gt; 'JobResource':\n    \"\"\"\n    Request resources from the machine, satisfying all provided requirements.\n    If not requirements are provided, the minimal amount of cores\n    (self.MIN_NCPU) will be used.\n    \"\"\"\n    # determining the biggest limit to satisfy, measured in the number of CPUs:\n    min_ncpu = max(\n        filter(\n            None,\n            [\n                self.adjust_ncpu(ncpu or self.min_cpu),\n                self.fraction_to_ncpu(fraction) if fraction else None,\n                self.nthreads_to_ncpu(nthreads) if nthreads else None,\n                self.mem_gb_to_ncpu(mem_gb) if mem_gb else None,\n                self.storage_gb_to_ncpu(storage_gb) if storage_gb else None,\n            ],\n        ),\n    )\n    return JobResource(\n        machine_type=self,\n        ncpu=min_ncpu,\n        attach_disk_storage_gb=(storage_gb if storage_gb and storage_gb &gt; self.calc_instance_disk_gb() else None),\n    )\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.fraction_to_ncpu","title":"fraction_to_ncpu","text":"<pre><code>fraction_to_ncpu(fraction)\n</code></pre> <p>Converts fraction to the number of CPU (e.g. fraction=1.0 to take the entire machine, fraction=0.5 to take half of it, etc.).</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def fraction_to_ncpu(self, fraction: float) -&gt; int:\n    \"\"\"\n    Converts fraction to the number of CPU (e.g. fraction=1.0 to take the entire\n    machine, fraction=0.5 to take half of it, etc.).\n    \"\"\"\n    ncpu = math.ceil(self.max_ncpu * fraction)\n    return self.adjust_ncpu(ncpu)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.mem_gb_to_ncpu","title":"mem_gb_to_ncpu","text":"<pre><code>mem_gb_to_ncpu(mem_gb)\n</code></pre> <p>Converts memory requirement to the number of CPU requirement.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def mem_gb_to_ncpu(self, mem_gb: float) -&gt; int:\n    \"\"\"\n    Converts memory requirement to the number of CPU requirement.\n    \"\"\"\n    ncpu = math.ceil(mem_gb / self.mem_gb_per_core)\n    return self.adjust_ncpu(ncpu)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.storage_gb_to_ncpu","title":"storage_gb_to_ncpu","text":"<pre><code>storage_gb_to_ncpu(storage_gb)\n</code></pre> <p>Converts storage requirement to the number of CPU requirement.</p> <p>We want to avoid attaching disks: attaching a disk to an existing instance might fail with <code>mkfs.ext4 ...</code> error, see: https://batch.hail.populationgenomics.org.au/batches/7488/jobs/12 So this function will calculate the number of CPU to request so your jobs can be packed to fit the default instance's available storage (calculated with self.calc_instance_disk_gb()).</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def storage_gb_to_ncpu(self, storage_gb: float) -&gt; int:\n    \"\"\"\n    Converts storage requirement to the number of CPU requirement.\n\n    We want to avoid attaching disks: attaching a disk to an existing instance\n    might fail with `mkfs.ext4 ...` error, see:\n    https://batch.hail.populationgenomics.org.au/batches/7488/jobs/12\n    So this function will calculate the number of CPU to request so your jobs\n    can be packed to fit the default instance's available storage\n    (calculated with self.calc_instance_disk_gb()).\n    \"\"\"\n    fraction = storage_gb / self.calc_instance_disk_gb()\n    fraction = min(fraction, 1.0)\n    return self.fraction_to_ncpu(fraction)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.nthreads_to_ncpu","title":"nthreads_to_ncpu","text":"<pre><code>nthreads_to_ncpu(nthreads)\n</code></pre> <p>Convert number of threads into number of cores/CPU</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def nthreads_to_ncpu(self, nthreads: int) -&gt; int:\n    \"\"\"\n    Convert number of threads into number of cores/CPU\n    \"\"\"\n    return self.adjust_ncpu(math.ceil(nthreads / 2))\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.MachineType.adjust_ncpu","title":"adjust_ncpu","text":"<pre><code>adjust_ncpu(ncpu)\n</code></pre> <p>Adjust request number of CPU to a number allowed by Hail, i.e. the nearest power of 2, not less than the minimal number of cores allowed.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def adjust_ncpu(self, ncpu: int) -&gt; int:\n    \"\"\"\n    Adjust request number of CPU to a number allowed by Hail, i.e.\n    the nearest power of 2, not less than the minimal number of cores allowed.\n    \"\"\"\n    if ncpu &gt; self.max_ncpu:\n        raise ValueError(\n            f'Requesting more cores than available on {self.name} machine: {ncpu}&gt;{self.max_ncpu}',\n        )\n\n    ncpu = max(ncpu, MachineType.min_cpu)\n\n    # round to the nearest power of 2 (15 -&gt; 16, 16 -&gt; 16, 17 -&gt; 32)\n    return int(pow(2, math.ceil(math.log2(ncpu))))\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.STANDARD","title":"cpg_flow.resources.STANDARD  <code>module-attribute</code>","text":"<pre><code>STANDARD = MachineType(\n    \"standard\",\n    ncpu=16,\n    mem_gb_per_core=3.75,\n    price_per_hour=1.0787,\n    disk_size_gb=375,\n)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.HIGHMEM","title":"cpg_flow.resources.HIGHMEM  <code>module-attribute</code>","text":"<pre><code>HIGHMEM = MachineType(\n    \"highmem\",\n    ncpu=16,\n    mem_gb_per_core=6.5,\n    price_per_hour=1.3431,\n    disk_size_gb=375,\n)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource","title":"cpg_flow.resources.JobResource  <code>dataclass</code>","text":"<pre><code>JobResource(\n    machine_type, ncpu=None, attach_disk_storage_gb=None\n)\n</code></pre> <p>Represents a fraction of a Hail Batch instance.</p> <p>@param machine_type: Hail Batch machine pool type @param ncpu: number of CPU request. Will be used to calculate the fraction of     the machine to take. If not set, all machine's CPUs will be used. @param attach_disk_storage_gb: if set to &gt; MachineType.max_default_storage_gb,     a larger disc will be attached by Hail Batch.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def __init__(\n    self,\n    machine_type: MachineType,\n    ncpu: int | None = None,\n    attach_disk_storage_gb: float | None = None,\n):\n    \"\"\"\n    @param machine_type: Hail Batch machine pool type\n    @param ncpu: number of CPU request. Will be used to calculate the fraction of\n        the machine to take. If not set, all machine's CPUs will be used.\n    @param attach_disk_storage_gb: if set to &gt; MachineType.max_default_storage_gb,\n        a larger disc will be attached by Hail Batch.\n    \"\"\"\n    self.machine_type = machine_type\n\n    self.fraction_of_full: float = 1.0\n    if ncpu is not None:\n        if ncpu &gt; self.machine_type.max_ncpu:\n            raise ValueError(\n                f'Max number of CPU on machine {self.machine_type.name} '\n                f'is {self.machine_type.max_ncpu}, requested {ncpu}',\n            )\n        self.fraction_of_full = ncpu / self.machine_type.max_ncpu\n\n    self.attach_disk_storage_gb = None\n    if attach_disk_storage_gb is not None:\n        if self.fraction_of_full &lt; 1:\n            raise ValueError(\n                f'Storage can be overridden only when the entire machine is used, '\n                f'not a fraction ({self.fraction_of_full}). '\n                f'override_storage_gb={attach_disk_storage_gb}',\n            )\n        self.attach_disk_storage_gb = attach_disk_storage_gb\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.machine_type","title":"machine_type  <code>instance-attribute</code>","text":"<pre><code>machine_type = machine_type\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.fraction_of_full","title":"fraction_of_full  <code>instance-attribute</code>","text":"<pre><code>fraction_of_full = 1.0\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.attach_disk_storage_gb","title":"attach_disk_storage_gb  <code>instance-attribute</code>","text":"<pre><code>attach_disk_storage_gb = None\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.get_mem_gb","title":"get_mem_gb","text":"<pre><code>get_mem_gb()\n</code></pre> <p>Memory resources in GB</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def get_mem_gb(self) -&gt; float:\n    \"\"\"\n    Memory resources in GB\n    \"\"\"\n    return self.get_ncpu() * self.machine_type.mem_gb_per_core\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.java_mem_options","title":"java_mem_options","text":"<pre><code>java_mem_options(overhead_gb=1)\n</code></pre> <p>Returns -Xms -Xmx options to set Java JVM memory usage to use all the memory resources represented. @param overhead_gb: Amount of memory (in decimal GB) to leave available for other purposes.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def java_mem_options(self, overhead_gb: float = 1) -&gt; str:\n    \"\"\"\n    Returns -Xms -Xmx options to set Java JVM memory usage to use all the memory\n    resources represented.\n    @param overhead_gb: Amount of memory (in decimal GB) to leave available for\n    other purposes.\n    \"\"\"\n    mem_bytes = (self.get_mem_gb() - overhead_gb) * 1_000_000_000\n    # Approximate as binary MiB (but not GiB as these options don't support\n    # fractional values) so that logs are easier to read\n    mem_mib = math.floor(mem_bytes / 1_048_576)\n    return f'-Xms{mem_mib}M -Xmx{mem_mib}M'\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.java_gc_thread_options","title":"java_gc_thread_options","text":"<pre><code>java_gc_thread_options(surplus=2)\n</code></pre> <p>Returns -XX options to set Java JVM garbage collection threading. @param surplus: Number of threads to leave available for other purposes.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def java_gc_thread_options(self, surplus: int = 2) -&gt; str:\n    \"\"\"\n    Returns -XX options to set Java JVM garbage collection threading.\n    @param surplus: Number of threads to leave available for other purposes.\n    \"\"\"\n    gc_threads = self.get_nthreads() - surplus\n    return f'-XX:+UseParallelGC -XX:ParallelGCThreads={gc_threads}'\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.get_ncpu","title":"get_ncpu","text":"<pre><code>get_ncpu()\n</code></pre> <p>Number of cores/CPU</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def get_ncpu(self) -&gt; int:\n    \"\"\"\n    Number of cores/CPU\n    \"\"\"\n    return int(self.machine_type.max_ncpu * self.fraction_of_full)\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.get_nthreads","title":"get_nthreads","text":"<pre><code>get_nthreads()\n</code></pre> <p>Number of threads</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def get_nthreads(self) -&gt; int:\n    \"\"\"\n    Number of threads\n    \"\"\"\n    return self.get_ncpu() * MachineType.threads_on_cpu\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.get_storage_gb","title":"get_storage_gb","text":"<pre><code>get_storage_gb()\n</code></pre> <p>Calculate storage in GB</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def get_storage_gb(self) -&gt; float:\n    \"\"\"\n    Calculate storage in GB\n    \"\"\"\n    if self.attach_disk_storage_gb:\n        storage_gb = self.attach_disk_storage_gb\n    else:\n        storage_gb = self.machine_type.calc_instance_disk_gb() * self.fraction_of_full\n\n    # Hail Batch actually requests 5% lower number than the\n    # requested one (e.g. \"req_storage: 46.25G, actual_storage: 44.0 GiB\"),\n    # so we will ask for a bigger number.\n    return storage_gb * 1.05\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.JobResource.set_to_job","title":"set_to_job","text":"<pre><code>set_to_job(j)\n</code></pre> <p>Set the resources to a Job object. Return self to allow chaining, e.g.:</p> <p>nthreads = STANDARD.request_resources(nthreads=4).set_to_job(j).get_nthreads()</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def set_to_job(self, j: Job) -&gt; 'JobResource':\n    \"\"\"\n    Set the resources to a Job object. Return self to allow chaining, e.g.:\n    &gt;&gt;&gt; nthreads = STANDARD.request_resources(nthreads=4).set_to_job(j).get_nthreads()\n    \"\"\"\n\n    j.storage(f'{self.get_storage_gb()}G')\n    j.cpu(self.get_ncpu())\n    j.memory(f'{self.get_mem_gb()}G')\n\n    # Returning self to allow command chaining.\n    return self\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.storage_for_cram_qc_job","title":"cpg_flow.resources.storage_for_cram_qc_job","text":"<pre><code>storage_for_cram_qc_job()\n</code></pre> <p>Get storage request for a CRAM QC processing job, gb</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def storage_for_cram_qc_job() -&gt; int | None:\n    \"\"\"\n    Get storage request for a CRAM QC processing job, gb\n    \"\"\"\n    sequencing_type = get_config()['workflow']['sequencing_type']\n    storage_gb = None  # avoid extra disk by default\n    if sequencing_type == 'genome':\n        storage_gb = 100\n    if sequencing_type == 'exome':\n        storage_gb = 20\n    return storage_gb\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.joint_calling_scatter_count","title":"cpg_flow.resources.joint_calling_scatter_count","text":"<pre><code>joint_calling_scatter_count(sequencing_group_count)\n</code></pre> <p>Number of partitions for joint-calling jobs (GenotypeGVCFs, VQSR, VEP), as a function of the sequencing group number.</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def joint_calling_scatter_count(sequencing_group_count: int) -&gt; int:\n    \"\"\"\n    Number of partitions for joint-calling jobs (GenotypeGVCFs, VQSR, VEP),\n    as a function of the sequencing group number.\n    \"\"\"\n    if scatter_count := get_config()['workflow'].get('scatter_count'):\n        return scatter_count\n\n    # Estimating this is challenging because GenotypeGVCFs does not scale\n    # linearly with the number of genomes.\n    # Values are adjusted based on experience with the actual number of genomes.\n    # e.g. 1000 scatter count was too low for 3800 genomes.\n    for threshold, scatter_count in {\n        4000: 1400,\n        3500: 1200,\n        3000: 1000,\n        2000: 600,\n        1000: 400,\n        500: 200,\n        250: 100,\n    }.items():\n        if sequencing_group_count &gt;= threshold:\n            return scatter_count\n    return 50\n</code></pre>"},{"location":"reference/resources/#cpg_flow.resources.storage_for_joint_vcf","title":"cpg_flow.resources.storage_for_joint_vcf","text":"<pre><code>storage_for_joint_vcf(\n    sequencing_group_count, site_only=True\n)\n</code></pre> <p>Storage enough to fit and process a joint-called VCF</p> Source code in <code>src/cpg_flow/resources.py</code> <pre><code>def storage_for_joint_vcf(\n    sequencing_group_count: int | None,\n    site_only: bool = True,\n) -&gt; float | None:\n    \"\"\"\n    Storage enough to fit and process a joint-called VCF\n    \"\"\"\n    if not sequencing_group_count:\n        return None\n    if get_config()['workflow']['sequencing_type'] == 'exome':\n        gb_per_sequencing_group = 0.1\n    else:\n        gb_per_sequencing_group = 1.0\n        if not site_only:\n            gb_per_sequencing_group = 1.5\n\n    return gb_per_sequencing_group * sequencing_group_count\n</code></pre>"},{"location":"reference/stage/","title":"<code>Stage</code> class","text":"<p>The following <code>Stage</code> classes are available to use:</p> <ul> <li><code>Stage</code></li> <li><code>DatasetStage</code></li> <li><code>CohortStage</code></li> <li><code>MultiCohortStage</code></li> <li><code>SequencingGroupStage</code></li> </ul> <p>You can import them from the <code>cpg_flow</code> package:</p> <pre><code>from cpg_flow.stage import Stage, DatasetStage, CohortStage, MultiCohortStage, SequencingGroupStage\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage","title":"cpg_flow.stage.Stage","text":"<pre><code>Stage(\n    *,\n    name,\n    required_stages=None,\n    analysis_type=None,\n    analysis_keys=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    skipped=False,\n    assume_outputs_exist=False,\n    forced=False\n)\n</code></pre> <p>               Bases: <code>ABC</code>, <code>Generic[TargetT]</code></p> <p>Abstract class for a workflow stage. Parametrised by specific Target subclass, i.e. SequencingGroupStage(Stage[SequencingGroup]) should only be able to work on SequencingGroup(Target).</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    required_stages: list[StageDecorator] | StageDecorator | None = None,\n    analysis_type: str | None = None,\n    analysis_keys: list[str] | None = None,\n    update_analysis_meta: Callable[[str], dict] | None = None,\n    tolerate_missing_output: bool = False,\n    skipped: bool = False,\n    assume_outputs_exist: bool = False,\n    forced: bool = False,\n):\n    self._name = name\n    self.required_stages_classes: list[StageDecorator] = []\n    if required_stages:\n        if isinstance(required_stages, list):\n            self.required_stages_classes.extend(required_stages)\n        else:\n            self.required_stages_classes.append(required_stages)\n\n    # Dependencies. Populated in workflow.run(), after we know all stages.\n    self.required_stages: list[Stage] = []\n\n    self.status_reporter = get_workflow().status_reporter\n    # If `analysis_type` is defined, it will be used to create/update Analysis\n    # entries in Metamist.\n    self.analysis_type = analysis_type\n    # If `analysis_keys` are defined, it will be used to extract the value for\n    # `Analysis.output` if the Stage.expected_outputs() returns a dict.\n    self.analysis_keys = analysis_keys\n    # if `update_analysis_meta` is defined, it is called on the `Analysis.output`\n    # field, and result is merged into the `Analysis.meta` dictionary.\n    self.update_analysis_meta = update_analysis_meta\n\n    self.tolerate_missing_output = tolerate_missing_output\n\n    # Populated with the return value of `add_to_the_workflow()`\n    self.output_by_target: dict[str, StageOutput | None] = dict()\n\n    self.skipped = skipped\n    self.forced = forced or self.name in get_config()['workflow'].get(\n        'force_stages',\n        [],\n    )\n    self.assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.required_stages_classes","title":"required_stages_classes  <code>instance-attribute</code>","text":"<pre><code>required_stages_classes = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.required_stages","title":"required_stages  <code>instance-attribute</code>","text":"<pre><code>required_stages = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.status_reporter","title":"status_reporter  <code>instance-attribute</code>","text":"<pre><code>status_reporter = status_reporter\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.analysis_type","title":"analysis_type  <code>instance-attribute</code>","text":"<pre><code>analysis_type = analysis_type\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.analysis_keys","title":"analysis_keys  <code>instance-attribute</code>","text":"<pre><code>analysis_keys = analysis_keys\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.update_analysis_meta","title":"update_analysis_meta  <code>instance-attribute</code>","text":"<pre><code>update_analysis_meta = update_analysis_meta\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.tolerate_missing_output","title":"tolerate_missing_output  <code>instance-attribute</code>","text":"<pre><code>tolerate_missing_output = tolerate_missing_output\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.output_by_target","title":"output_by_target  <code>instance-attribute</code>","text":"<pre><code>output_by_target = dict()\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.skipped","title":"skipped  <code>instance-attribute</code>","text":"<pre><code>skipped = skipped\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = forced or name in get('force_stages', [])\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.assume_outputs_exist","title":"assume_outputs_exist  <code>instance-attribute</code>","text":"<pre><code>assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.tmp_prefix","title":"tmp_prefix  <code>property</code>","text":"<pre><code>tmp_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.web_prefix","title":"web_prefix  <code>property</code>","text":"<pre><code>web_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.analysis_prefix","title":"analysis_prefix  <code>property</code>","text":"<pre><code>analysis_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Stage name (unique and descriptive stage)</p>"},{"location":"reference/stage/#cpg_flow.stage.Stage.get_stage_cohort_prefix","title":"get_stage_cohort_prefix","text":"<pre><code>get_stage_cohort_prefix(cohort, category=None)\n</code></pre> <p>Takes a cohort as an argument, calls through to the Workflow cohort_prefix method Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"</p> PARAMETER DESCRIPTION <code>cohort</code> <p>we pull the analysis dataset and name from this Cohort</p> <p> TYPE: <code>Cohort</code> </p> <code>category</code> <p>main, tmp, test, analysis, web</p> <p> TYPE: <code>str | none</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_stage_cohort_prefix(\n    self,\n    cohort: Cohort,\n    category: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Takes a cohort as an argument, calls through to the Workflow cohort_prefix method\n    Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME\n    e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"\n\n    Args:\n        cohort (Cohort): we pull the analysis dataset and name from this Cohort\n        category (str | none): main, tmp, test, analysis, web\n\n    Returns:\n        Path\n    \"\"\"\n    return get_workflow().cohort_prefix(cohort, category=category) / self.name\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.queue_jobs","title":"queue_jobs  <code>abstractmethod</code>","text":"<pre><code>queue_jobs(target, inputs)\n</code></pre> <p>Adds Hail Batch jobs that process <code>target</code>. Assumes that all the household work is done: checking missing inputs from required stages, checking for possible reuse of existing outputs.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef queue_jobs(self, target: TargetT, inputs: StageInput) -&gt; StageOutput | None:\n    \"\"\"\n    Adds Hail Batch jobs that process `target`.\n    Assumes that all the household work is done: checking missing inputs\n    from required stages, checking for possible reuse of existing outputs.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.expected_outputs","title":"expected_outputs  <code>abstractmethod</code>","text":"<pre><code>expected_outputs(target)\n</code></pre> <p>Get path(s) to files that the stage is expected to generate for a <code>target</code>. Used within in <code>queue_jobs()</code> to pass paths to outputs to job commands, as well as by the workflow to check if the stage's expected outputs already exist and can be reused.</p> <p>Can be a str, a Path object, or a dictionary of str/Path objects.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef expected_outputs(self, target: TargetT) -&gt; ExpectedResultT:\n    \"\"\"\n    Get path(s) to files that the stage is expected to generate for a `target`.\n    Used within in `queue_jobs()` to pass paths to outputs to job commands,\n    as well as by the workflow to check if the stage's expected outputs already\n    exist and can be reused.\n\n    Can be a str, a Path object, or a dictionary of str/Path objects.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.queue_for_multicohort","title":"queue_for_multicohort  <code>abstractmethod</code>","text":"<pre><code>queue_for_multicohort(multicohort)\n</code></pre> <p>Queues jobs for each corresponding target, defined by Stage subclass.</p> <p>Returns a dictionary of <code>StageOutput</code> objects indexed by target unique_id.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef queue_for_multicohort(\n    self,\n    multicohort: MultiCohort,\n) -&gt; dict[str, StageOutput | None]:\n    \"\"\"\n    Queues jobs for each corresponding target, defined by Stage subclass.\n\n    Returns a dictionary of `StageOutput` objects indexed by target unique_id.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.make_outputs","title":"make_outputs","text":"<pre><code>make_outputs(\n    target,\n    data=None,\n    *,\n    jobs=None,\n    meta=None,\n    reusable=False,\n    skipped=False,\n    error_msg=None\n)\n</code></pre> <p>Create StageOutput for this stage.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def make_outputs(\n    self,\n    target: Target,\n    data: ExpectedResultT = None,  # TODO: ExpectedResultT is probably too broad, our code only really support dict\n    *,\n    jobs: Sequence[Job | None] | Job | None = None,\n    meta: dict | None = None,\n    reusable: bool = False,\n    skipped: bool = False,\n    error_msg: str | None = None,\n) -&gt; StageOutput:\n    \"\"\"\n    Create StageOutput for this stage.\n    \"\"\"\n    return StageOutput(\n        target=target,\n        data=data,\n        jobs=jobs,\n        meta=meta,\n        reusable=reusable,\n        skipped=skipped,\n        error_msg=error_msg,\n        stage=self,\n    )\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.Stage.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs(target=None)\n</code></pre> <p>Create Hail Batch Job attributes dictionary</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_job_attrs(self, target: TargetT | None = None) -&gt; dict[str, str]:\n    \"\"\"\n    Create Hail Batch Job attributes dictionary\n    \"\"\"\n    job_attrs = dict(stage=self.name)\n    if sequencing_type := get_config()['workflow'].get('sequencing_type'):\n        job_attrs['sequencing_type'] = sequencing_type\n    if target:\n        job_attrs |= target.get_job_attrs()\n    return job_attrs\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage","title":"cpg_flow.stage.DatasetStage","text":"<pre><code>DatasetStage(\n    *,\n    name,\n    required_stages=None,\n    analysis_type=None,\n    analysis_keys=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    skipped=False,\n    assume_outputs_exist=False,\n    forced=False\n)\n</code></pre> <p>               Bases: <code>Stage</code>, <code>ABC</code></p> <p>Dataset-level stage</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    required_stages: list[StageDecorator] | StageDecorator | None = None,\n    analysis_type: str | None = None,\n    analysis_keys: list[str] | None = None,\n    update_analysis_meta: Callable[[str], dict] | None = None,\n    tolerate_missing_output: bool = False,\n    skipped: bool = False,\n    assume_outputs_exist: bool = False,\n    forced: bool = False,\n):\n    self._name = name\n    self.required_stages_classes: list[StageDecorator] = []\n    if required_stages:\n        if isinstance(required_stages, list):\n            self.required_stages_classes.extend(required_stages)\n        else:\n            self.required_stages_classes.append(required_stages)\n\n    # Dependencies. Populated in workflow.run(), after we know all stages.\n    self.required_stages: list[Stage] = []\n\n    self.status_reporter = get_workflow().status_reporter\n    # If `analysis_type` is defined, it will be used to create/update Analysis\n    # entries in Metamist.\n    self.analysis_type = analysis_type\n    # If `analysis_keys` are defined, it will be used to extract the value for\n    # `Analysis.output` if the Stage.expected_outputs() returns a dict.\n    self.analysis_keys = analysis_keys\n    # if `update_analysis_meta` is defined, it is called on the `Analysis.output`\n    # field, and result is merged into the `Analysis.meta` dictionary.\n    self.update_analysis_meta = update_analysis_meta\n\n    self.tolerate_missing_output = tolerate_missing_output\n\n    # Populated with the return value of `add_to_the_workflow()`\n    self.output_by_target: dict[str, StageOutput | None] = dict()\n\n    self.skipped = skipped\n    self.forced = forced or self.name in get_config()['workflow'].get(\n        'force_stages',\n        [],\n    )\n    self.assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.required_stages_classes","title":"required_stages_classes  <code>instance-attribute</code>","text":"<pre><code>required_stages_classes = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.required_stages","title":"required_stages  <code>instance-attribute</code>","text":"<pre><code>required_stages = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.status_reporter","title":"status_reporter  <code>instance-attribute</code>","text":"<pre><code>status_reporter = status_reporter\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.analysis_type","title":"analysis_type  <code>instance-attribute</code>","text":"<pre><code>analysis_type = analysis_type\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.analysis_keys","title":"analysis_keys  <code>instance-attribute</code>","text":"<pre><code>analysis_keys = analysis_keys\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.update_analysis_meta","title":"update_analysis_meta  <code>instance-attribute</code>","text":"<pre><code>update_analysis_meta = update_analysis_meta\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.tolerate_missing_output","title":"tolerate_missing_output  <code>instance-attribute</code>","text":"<pre><code>tolerate_missing_output = tolerate_missing_output\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.output_by_target","title":"output_by_target  <code>instance-attribute</code>","text":"<pre><code>output_by_target = dict()\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.skipped","title":"skipped  <code>instance-attribute</code>","text":"<pre><code>skipped = skipped\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = forced or name in get('force_stages', [])\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.assume_outputs_exist","title":"assume_outputs_exist  <code>instance-attribute</code>","text":"<pre><code>assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.tmp_prefix","title":"tmp_prefix  <code>property</code>","text":"<pre><code>tmp_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.web_prefix","title":"web_prefix  <code>property</code>","text":"<pre><code>web_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.analysis_prefix","title":"analysis_prefix  <code>property</code>","text":"<pre><code>analysis_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Stage name (unique and descriptive stage)</p>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.expected_outputs","title":"expected_outputs  <code>abstractmethod</code>","text":"<pre><code>expected_outputs(dataset)\n</code></pre> <p>Override to declare expected output paths.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef expected_outputs(self, dataset: Dataset) -&gt; ExpectedResultT:\n    \"\"\"\n    Override to declare expected output paths.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.queue_jobs","title":"queue_jobs  <code>abstractmethod</code>","text":"<pre><code>queue_jobs(dataset, inputs)\n</code></pre> <p>Override to add Hail Batch jobs.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef queue_jobs(self, dataset: Dataset, inputs: StageInput) -&gt; StageOutput | None:\n    \"\"\"\n    Override to add Hail Batch jobs.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.queue_for_multicohort","title":"queue_for_multicohort","text":"<pre><code>queue_for_multicohort(multicohort)\n</code></pre> <p>Plug the stage into the workflow.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def queue_for_multicohort(\n    self,\n    multicohort: MultiCohort,\n) -&gt; dict[str, StageOutput | None]:\n    \"\"\"\n    Plug the stage into the workflow.\n    \"\"\"\n    output_by_target: dict[str, StageOutput | None] = dict()\n    # iterate directly over the datasets in this multicohort\n    for dataset in multicohort.get_datasets():\n        action = self._get_action(dataset)\n        output_by_target[dataset.target_id] = self._queue_jobs_with_checks(\n            dataset,\n            action,\n        )\n    return output_by_target\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.get_stage_cohort_prefix","title":"get_stage_cohort_prefix","text":"<pre><code>get_stage_cohort_prefix(cohort, category=None)\n</code></pre> <p>Takes a cohort as an argument, calls through to the Workflow cohort_prefix method Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"</p> PARAMETER DESCRIPTION <code>cohort</code> <p>we pull the analysis dataset and name from this Cohort</p> <p> TYPE: <code>Cohort</code> </p> <code>category</code> <p>main, tmp, test, analysis, web</p> <p> TYPE: <code>str | none</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_stage_cohort_prefix(\n    self,\n    cohort: Cohort,\n    category: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Takes a cohort as an argument, calls through to the Workflow cohort_prefix method\n    Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME\n    e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"\n\n    Args:\n        cohort (Cohort): we pull the analysis dataset and name from this Cohort\n        category (str | none): main, tmp, test, analysis, web\n\n    Returns:\n        Path\n    \"\"\"\n    return get_workflow().cohort_prefix(cohort, category=category) / self.name\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.make_outputs","title":"make_outputs","text":"<pre><code>make_outputs(\n    target,\n    data=None,\n    *,\n    jobs=None,\n    meta=None,\n    reusable=False,\n    skipped=False,\n    error_msg=None\n)\n</code></pre> <p>Create StageOutput for this stage.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def make_outputs(\n    self,\n    target: Target,\n    data: ExpectedResultT = None,  # TODO: ExpectedResultT is probably too broad, our code only really support dict\n    *,\n    jobs: Sequence[Job | None] | Job | None = None,\n    meta: dict | None = None,\n    reusable: bool = False,\n    skipped: bool = False,\n    error_msg: str | None = None,\n) -&gt; StageOutput:\n    \"\"\"\n    Create StageOutput for this stage.\n    \"\"\"\n    return StageOutput(\n        target=target,\n        data=data,\n        jobs=jobs,\n        meta=meta,\n        reusable=reusable,\n        skipped=skipped,\n        error_msg=error_msg,\n        stage=self,\n    )\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.DatasetStage.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs(target=None)\n</code></pre> <p>Create Hail Batch Job attributes dictionary</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_job_attrs(self, target: TargetT | None = None) -&gt; dict[str, str]:\n    \"\"\"\n    Create Hail Batch Job attributes dictionary\n    \"\"\"\n    job_attrs = dict(stage=self.name)\n    if sequencing_type := get_config()['workflow'].get('sequencing_type'):\n        job_attrs['sequencing_type'] = sequencing_type\n    if target:\n        job_attrs |= target.get_job_attrs()\n    return job_attrs\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage","title":"cpg_flow.stage.CohortStage","text":"<pre><code>CohortStage(\n    *,\n    name,\n    required_stages=None,\n    analysis_type=None,\n    analysis_keys=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    skipped=False,\n    assume_outputs_exist=False,\n    forced=False\n)\n</code></pre> <p>               Bases: <code>Stage</code>, <code>ABC</code></p> <p>Cohort-level stage (all datasets of a workflow run).</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    required_stages: list[StageDecorator] | StageDecorator | None = None,\n    analysis_type: str | None = None,\n    analysis_keys: list[str] | None = None,\n    update_analysis_meta: Callable[[str], dict] | None = None,\n    tolerate_missing_output: bool = False,\n    skipped: bool = False,\n    assume_outputs_exist: bool = False,\n    forced: bool = False,\n):\n    self._name = name\n    self.required_stages_classes: list[StageDecorator] = []\n    if required_stages:\n        if isinstance(required_stages, list):\n            self.required_stages_classes.extend(required_stages)\n        else:\n            self.required_stages_classes.append(required_stages)\n\n    # Dependencies. Populated in workflow.run(), after we know all stages.\n    self.required_stages: list[Stage] = []\n\n    self.status_reporter = get_workflow().status_reporter\n    # If `analysis_type` is defined, it will be used to create/update Analysis\n    # entries in Metamist.\n    self.analysis_type = analysis_type\n    # If `analysis_keys` are defined, it will be used to extract the value for\n    # `Analysis.output` if the Stage.expected_outputs() returns a dict.\n    self.analysis_keys = analysis_keys\n    # if `update_analysis_meta` is defined, it is called on the `Analysis.output`\n    # field, and result is merged into the `Analysis.meta` dictionary.\n    self.update_analysis_meta = update_analysis_meta\n\n    self.tolerate_missing_output = tolerate_missing_output\n\n    # Populated with the return value of `add_to_the_workflow()`\n    self.output_by_target: dict[str, StageOutput | None] = dict()\n\n    self.skipped = skipped\n    self.forced = forced or self.name in get_config()['workflow'].get(\n        'force_stages',\n        [],\n    )\n    self.assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.required_stages_classes","title":"required_stages_classes  <code>instance-attribute</code>","text":"<pre><code>required_stages_classes = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.required_stages","title":"required_stages  <code>instance-attribute</code>","text":"<pre><code>required_stages = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.status_reporter","title":"status_reporter  <code>instance-attribute</code>","text":"<pre><code>status_reporter = status_reporter\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.analysis_type","title":"analysis_type  <code>instance-attribute</code>","text":"<pre><code>analysis_type = analysis_type\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.analysis_keys","title":"analysis_keys  <code>instance-attribute</code>","text":"<pre><code>analysis_keys = analysis_keys\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.update_analysis_meta","title":"update_analysis_meta  <code>instance-attribute</code>","text":"<pre><code>update_analysis_meta = update_analysis_meta\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.tolerate_missing_output","title":"tolerate_missing_output  <code>instance-attribute</code>","text":"<pre><code>tolerate_missing_output = tolerate_missing_output\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.output_by_target","title":"output_by_target  <code>instance-attribute</code>","text":"<pre><code>output_by_target = dict()\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.skipped","title":"skipped  <code>instance-attribute</code>","text":"<pre><code>skipped = skipped\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = forced or name in get('force_stages', [])\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.assume_outputs_exist","title":"assume_outputs_exist  <code>instance-attribute</code>","text":"<pre><code>assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.tmp_prefix","title":"tmp_prefix  <code>property</code>","text":"<pre><code>tmp_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.web_prefix","title":"web_prefix  <code>property</code>","text":"<pre><code>web_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.analysis_prefix","title":"analysis_prefix  <code>property</code>","text":"<pre><code>analysis_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Stage name (unique and descriptive stage)</p>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.expected_outputs","title":"expected_outputs  <code>abstractmethod</code>","text":"<pre><code>expected_outputs(cohort)\n</code></pre> <p>Override to declare expected output paths.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef expected_outputs(self, cohort: Cohort) -&gt; ExpectedResultT:\n    \"\"\"\n    Override to declare expected output paths.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.queue_jobs","title":"queue_jobs  <code>abstractmethod</code>","text":"<pre><code>queue_jobs(cohort, inputs)\n</code></pre> <p>Override to add Hail Batch jobs.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef queue_jobs(self, cohort: Cohort, inputs: StageInput) -&gt; StageOutput | None:\n    \"\"\"\n    Override to add Hail Batch jobs.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.queue_for_multicohort","title":"queue_for_multicohort","text":"<pre><code>queue_for_multicohort(multicohort)\n</code></pre> <p>Plug the stage into the workflow.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def queue_for_multicohort(\n    self,\n    multicohort: MultiCohort,\n) -&gt; dict[str, StageOutput | None]:\n    \"\"\"\n    Plug the stage into the workflow.\n    \"\"\"\n    output_by_target: dict[str, StageOutput | None] = dict()\n    for cohort in multicohort.get_cohorts():\n        action = self._get_action(cohort)\n        output_by_target[cohort.target_id] = self._queue_jobs_with_checks(\n            cohort,\n            action,\n        )\n    return output_by_target\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.get_stage_cohort_prefix","title":"get_stage_cohort_prefix","text":"<pre><code>get_stage_cohort_prefix(cohort, category=None)\n</code></pre> <p>Takes a cohort as an argument, calls through to the Workflow cohort_prefix method Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"</p> PARAMETER DESCRIPTION <code>cohort</code> <p>we pull the analysis dataset and name from this Cohort</p> <p> TYPE: <code>Cohort</code> </p> <code>category</code> <p>main, tmp, test, analysis, web</p> <p> TYPE: <code>str | none</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_stage_cohort_prefix(\n    self,\n    cohort: Cohort,\n    category: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Takes a cohort as an argument, calls through to the Workflow cohort_prefix method\n    Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME\n    e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"\n\n    Args:\n        cohort (Cohort): we pull the analysis dataset and name from this Cohort\n        category (str | none): main, tmp, test, analysis, web\n\n    Returns:\n        Path\n    \"\"\"\n    return get_workflow().cohort_prefix(cohort, category=category) / self.name\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.make_outputs","title":"make_outputs","text":"<pre><code>make_outputs(\n    target,\n    data=None,\n    *,\n    jobs=None,\n    meta=None,\n    reusable=False,\n    skipped=False,\n    error_msg=None\n)\n</code></pre> <p>Create StageOutput for this stage.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def make_outputs(\n    self,\n    target: Target,\n    data: ExpectedResultT = None,  # TODO: ExpectedResultT is probably too broad, our code only really support dict\n    *,\n    jobs: Sequence[Job | None] | Job | None = None,\n    meta: dict | None = None,\n    reusable: bool = False,\n    skipped: bool = False,\n    error_msg: str | None = None,\n) -&gt; StageOutput:\n    \"\"\"\n    Create StageOutput for this stage.\n    \"\"\"\n    return StageOutput(\n        target=target,\n        data=data,\n        jobs=jobs,\n        meta=meta,\n        reusable=reusable,\n        skipped=skipped,\n        error_msg=error_msg,\n        stage=self,\n    )\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.CohortStage.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs(target=None)\n</code></pre> <p>Create Hail Batch Job attributes dictionary</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_job_attrs(self, target: TargetT | None = None) -&gt; dict[str, str]:\n    \"\"\"\n    Create Hail Batch Job attributes dictionary\n    \"\"\"\n    job_attrs = dict(stage=self.name)\n    if sequencing_type := get_config()['workflow'].get('sequencing_type'):\n        job_attrs['sequencing_type'] = sequencing_type\n    if target:\n        job_attrs |= target.get_job_attrs()\n    return job_attrs\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage","title":"cpg_flow.stage.MultiCohortStage","text":"<pre><code>MultiCohortStage(\n    *,\n    name,\n    required_stages=None,\n    analysis_type=None,\n    analysis_keys=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    skipped=False,\n    assume_outputs_exist=False,\n    forced=False\n)\n</code></pre> <p>               Bases: <code>Stage</code>, <code>ABC</code></p> <p>MultiCohort-level stage (all datasets of a workflow run).</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    required_stages: list[StageDecorator] | StageDecorator | None = None,\n    analysis_type: str | None = None,\n    analysis_keys: list[str] | None = None,\n    update_analysis_meta: Callable[[str], dict] | None = None,\n    tolerate_missing_output: bool = False,\n    skipped: bool = False,\n    assume_outputs_exist: bool = False,\n    forced: bool = False,\n):\n    self._name = name\n    self.required_stages_classes: list[StageDecorator] = []\n    if required_stages:\n        if isinstance(required_stages, list):\n            self.required_stages_classes.extend(required_stages)\n        else:\n            self.required_stages_classes.append(required_stages)\n\n    # Dependencies. Populated in workflow.run(), after we know all stages.\n    self.required_stages: list[Stage] = []\n\n    self.status_reporter = get_workflow().status_reporter\n    # If `analysis_type` is defined, it will be used to create/update Analysis\n    # entries in Metamist.\n    self.analysis_type = analysis_type\n    # If `analysis_keys` are defined, it will be used to extract the value for\n    # `Analysis.output` if the Stage.expected_outputs() returns a dict.\n    self.analysis_keys = analysis_keys\n    # if `update_analysis_meta` is defined, it is called on the `Analysis.output`\n    # field, and result is merged into the `Analysis.meta` dictionary.\n    self.update_analysis_meta = update_analysis_meta\n\n    self.tolerate_missing_output = tolerate_missing_output\n\n    # Populated with the return value of `add_to_the_workflow()`\n    self.output_by_target: dict[str, StageOutput | None] = dict()\n\n    self.skipped = skipped\n    self.forced = forced or self.name in get_config()['workflow'].get(\n        'force_stages',\n        [],\n    )\n    self.assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.required_stages_classes","title":"required_stages_classes  <code>instance-attribute</code>","text":"<pre><code>required_stages_classes = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.required_stages","title":"required_stages  <code>instance-attribute</code>","text":"<pre><code>required_stages = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.status_reporter","title":"status_reporter  <code>instance-attribute</code>","text":"<pre><code>status_reporter = status_reporter\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.analysis_type","title":"analysis_type  <code>instance-attribute</code>","text":"<pre><code>analysis_type = analysis_type\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.analysis_keys","title":"analysis_keys  <code>instance-attribute</code>","text":"<pre><code>analysis_keys = analysis_keys\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.update_analysis_meta","title":"update_analysis_meta  <code>instance-attribute</code>","text":"<pre><code>update_analysis_meta = update_analysis_meta\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.tolerate_missing_output","title":"tolerate_missing_output  <code>instance-attribute</code>","text":"<pre><code>tolerate_missing_output = tolerate_missing_output\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.output_by_target","title":"output_by_target  <code>instance-attribute</code>","text":"<pre><code>output_by_target = dict()\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.skipped","title":"skipped  <code>instance-attribute</code>","text":"<pre><code>skipped = skipped\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = forced or name in get('force_stages', [])\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.assume_outputs_exist","title":"assume_outputs_exist  <code>instance-attribute</code>","text":"<pre><code>assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.tmp_prefix","title":"tmp_prefix  <code>property</code>","text":"<pre><code>tmp_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.web_prefix","title":"web_prefix  <code>property</code>","text":"<pre><code>web_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.analysis_prefix","title":"analysis_prefix  <code>property</code>","text":"<pre><code>analysis_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Stage name (unique and descriptive stage)</p>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.expected_outputs","title":"expected_outputs  <code>abstractmethod</code>","text":"<pre><code>expected_outputs(multicohort)\n</code></pre> <p>Override to declare expected output paths.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef expected_outputs(self, multicohort: MultiCohort) -&gt; ExpectedResultT:\n    \"\"\"\n    Override to declare expected output paths.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.queue_jobs","title":"queue_jobs  <code>abstractmethod</code>","text":"<pre><code>queue_jobs(multicohort, inputs)\n</code></pre> <p>Override to add Hail Batch jobs.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef queue_jobs(\n    self,\n    multicohort: MultiCohort,\n    inputs: StageInput,\n) -&gt; StageOutput | None:\n    \"\"\"\n    Override to add Hail Batch jobs.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.queue_for_multicohort","title":"queue_for_multicohort","text":"<pre><code>queue_for_multicohort(multicohort)\n</code></pre> <p>Plug the stage into the workflow.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def queue_for_multicohort(\n    self,\n    multicohort: MultiCohort,\n) -&gt; dict[str, StageOutput | None]:\n    \"\"\"\n    Plug the stage into the workflow.\n    \"\"\"\n    output_by_target: dict[str, StageOutput | None] = dict()\n    action = self._get_action(multicohort)\n    output_by_target[multicohort.target_id] = self._queue_jobs_with_checks(\n        multicohort,\n        action,\n    )\n    return output_by_target\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.get_stage_cohort_prefix","title":"get_stage_cohort_prefix","text":"<pre><code>get_stage_cohort_prefix(cohort, category=None)\n</code></pre> <p>Takes a cohort as an argument, calls through to the Workflow cohort_prefix method Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"</p> PARAMETER DESCRIPTION <code>cohort</code> <p>we pull the analysis dataset and name from this Cohort</p> <p> TYPE: <code>Cohort</code> </p> <code>category</code> <p>main, tmp, test, analysis, web</p> <p> TYPE: <code>str | none</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_stage_cohort_prefix(\n    self,\n    cohort: Cohort,\n    category: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Takes a cohort as an argument, calls through to the Workflow cohort_prefix method\n    Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME\n    e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"\n\n    Args:\n        cohort (Cohort): we pull the analysis dataset and name from this Cohort\n        category (str | none): main, tmp, test, analysis, web\n\n    Returns:\n        Path\n    \"\"\"\n    return get_workflow().cohort_prefix(cohort, category=category) / self.name\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.make_outputs","title":"make_outputs","text":"<pre><code>make_outputs(\n    target,\n    data=None,\n    *,\n    jobs=None,\n    meta=None,\n    reusable=False,\n    skipped=False,\n    error_msg=None\n)\n</code></pre> <p>Create StageOutput for this stage.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def make_outputs(\n    self,\n    target: Target,\n    data: ExpectedResultT = None,  # TODO: ExpectedResultT is probably too broad, our code only really support dict\n    *,\n    jobs: Sequence[Job | None] | Job | None = None,\n    meta: dict | None = None,\n    reusable: bool = False,\n    skipped: bool = False,\n    error_msg: str | None = None,\n) -&gt; StageOutput:\n    \"\"\"\n    Create StageOutput for this stage.\n    \"\"\"\n    return StageOutput(\n        target=target,\n        data=data,\n        jobs=jobs,\n        meta=meta,\n        reusable=reusable,\n        skipped=skipped,\n        error_msg=error_msg,\n        stage=self,\n    )\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.MultiCohortStage.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs(target=None)\n</code></pre> <p>Create Hail Batch Job attributes dictionary</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_job_attrs(self, target: TargetT | None = None) -&gt; dict[str, str]:\n    \"\"\"\n    Create Hail Batch Job attributes dictionary\n    \"\"\"\n    job_attrs = dict(stage=self.name)\n    if sequencing_type := get_config()['workflow'].get('sequencing_type'):\n        job_attrs['sequencing_type'] = sequencing_type\n    if target:\n        job_attrs |= target.get_job_attrs()\n    return job_attrs\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage","title":"cpg_flow.stage.SequencingGroupStage","text":"<pre><code>SequencingGroupStage(\n    *,\n    name,\n    required_stages=None,\n    analysis_type=None,\n    analysis_keys=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    skipped=False,\n    assume_outputs_exist=False,\n    forced=False\n)\n</code></pre> <p>               Bases: <code>Stage[SequencingGroup]</code>, <code>ABC</code></p> <p>Sequencing Group level stage.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def __init__(\n    self,\n    *,\n    name: str,\n    required_stages: list[StageDecorator] | StageDecorator | None = None,\n    analysis_type: str | None = None,\n    analysis_keys: list[str] | None = None,\n    update_analysis_meta: Callable[[str], dict] | None = None,\n    tolerate_missing_output: bool = False,\n    skipped: bool = False,\n    assume_outputs_exist: bool = False,\n    forced: bool = False,\n):\n    self._name = name\n    self.required_stages_classes: list[StageDecorator] = []\n    if required_stages:\n        if isinstance(required_stages, list):\n            self.required_stages_classes.extend(required_stages)\n        else:\n            self.required_stages_classes.append(required_stages)\n\n    # Dependencies. Populated in workflow.run(), after we know all stages.\n    self.required_stages: list[Stage] = []\n\n    self.status_reporter = get_workflow().status_reporter\n    # If `analysis_type` is defined, it will be used to create/update Analysis\n    # entries in Metamist.\n    self.analysis_type = analysis_type\n    # If `analysis_keys` are defined, it will be used to extract the value for\n    # `Analysis.output` if the Stage.expected_outputs() returns a dict.\n    self.analysis_keys = analysis_keys\n    # if `update_analysis_meta` is defined, it is called on the `Analysis.output`\n    # field, and result is merged into the `Analysis.meta` dictionary.\n    self.update_analysis_meta = update_analysis_meta\n\n    self.tolerate_missing_output = tolerate_missing_output\n\n    # Populated with the return value of `add_to_the_workflow()`\n    self.output_by_target: dict[str, StageOutput | None] = dict()\n\n    self.skipped = skipped\n    self.forced = forced or self.name in get_config()['workflow'].get(\n        'force_stages',\n        [],\n    )\n    self.assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.required_stages_classes","title":"required_stages_classes  <code>instance-attribute</code>","text":"<pre><code>required_stages_classes = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.required_stages","title":"required_stages  <code>instance-attribute</code>","text":"<pre><code>required_stages = []\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.status_reporter","title":"status_reporter  <code>instance-attribute</code>","text":"<pre><code>status_reporter = status_reporter\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.analysis_type","title":"analysis_type  <code>instance-attribute</code>","text":"<pre><code>analysis_type = analysis_type\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.analysis_keys","title":"analysis_keys  <code>instance-attribute</code>","text":"<pre><code>analysis_keys = analysis_keys\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.update_analysis_meta","title":"update_analysis_meta  <code>instance-attribute</code>","text":"<pre><code>update_analysis_meta = update_analysis_meta\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.tolerate_missing_output","title":"tolerate_missing_output  <code>instance-attribute</code>","text":"<pre><code>tolerate_missing_output = tolerate_missing_output\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.output_by_target","title":"output_by_target  <code>instance-attribute</code>","text":"<pre><code>output_by_target = dict()\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.skipped","title":"skipped  <code>instance-attribute</code>","text":"<pre><code>skipped = skipped\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = forced or name in get('force_stages', [])\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.assume_outputs_exist","title":"assume_outputs_exist  <code>instance-attribute</code>","text":"<pre><code>assume_outputs_exist = assume_outputs_exist\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.tmp_prefix","title":"tmp_prefix  <code>property</code>","text":"<pre><code>tmp_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.web_prefix","title":"web_prefix  <code>property</code>","text":"<pre><code>web_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.analysis_prefix","title":"analysis_prefix  <code>property</code>","text":"<pre><code>analysis_prefix\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.name","title":"name  <code>property</code>","text":"<pre><code>name\n</code></pre> <p>Stage name (unique and descriptive stage)</p>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.expected_outputs","title":"expected_outputs  <code>abstractmethod</code>","text":"<pre><code>expected_outputs(sequencing_group)\n</code></pre> <p>Override to declare expected output paths.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef expected_outputs(self, sequencing_group: SequencingGroup) -&gt; ExpectedResultT:\n    \"\"\"\n    Override to declare expected output paths.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.queue_jobs","title":"queue_jobs  <code>abstractmethod</code>","text":"<pre><code>queue_jobs(sequencing_group, inputs)\n</code></pre> <p>Override to add Hail Batch jobs.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>@abstractmethod\ndef queue_jobs(\n    self,\n    sequencing_group: SequencingGroup,\n    inputs: StageInput,\n) -&gt; StageOutput | None:\n    \"\"\"\n    Override to add Hail Batch jobs.\n    \"\"\"\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.queue_for_multicohort","title":"queue_for_multicohort","text":"<pre><code>queue_for_multicohort(multicohort)\n</code></pre> <p>Plug the stage into the workflow.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def queue_for_multicohort(\n    self,\n    multicohort: MultiCohort,\n) -&gt; dict[str, StageOutput | None]:\n    \"\"\"\n    Plug the stage into the workflow.\n    \"\"\"\n    output_by_target: dict[str, StageOutput | None] = dict()\n    if not (active_sgs := multicohort.get_sequencing_groups()):\n        all_sgs = len(multicohort.get_sequencing_groups(only_active=False))\n        logger.warning(\n            f'{len(active_sgs)}/{all_sgs} usable (active=True) SGs found in the multicohort. '\n            'Check that input_cohorts` or `input_datasets` are provided and not skipped',\n        )\n        return output_by_target\n\n    # evaluate_stuff en masse\n    for sequencing_group in active_sgs:\n        action = self._get_action(sequencing_group)\n        output_by_target[sequencing_group.target_id] = self._queue_jobs_with_checks(\n            sequencing_group,\n            action,\n        )\n    return output_by_target\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.get_stage_cohort_prefix","title":"get_stage_cohort_prefix","text":"<pre><code>get_stage_cohort_prefix(cohort, category=None)\n</code></pre> <p>Takes a cohort as an argument, calls through to the Workflow cohort_prefix method Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"</p> PARAMETER DESCRIPTION <code>cohort</code> <p>we pull the analysis dataset and name from this Cohort</p> <p> TYPE: <code>Cohort</code> </p> <code>category</code> <p>main, tmp, test, analysis, web</p> <p> TYPE: <code>str | none</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_stage_cohort_prefix(\n    self,\n    cohort: Cohort,\n    category: str | None = None,\n) -&gt; Path:\n    \"\"\"\n    Takes a cohort as an argument, calls through to the Workflow cohort_prefix method\n    Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID / STAGE_NAME\n    e.g. \"gs://cpg-project-main/seqr_loader/COH123/MyStage\"\n\n    Args:\n        cohort (Cohort): we pull the analysis dataset and name from this Cohort\n        category (str | none): main, tmp, test, analysis, web\n\n    Returns:\n        Path\n    \"\"\"\n    return get_workflow().cohort_prefix(cohort, category=category) / self.name\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.make_outputs","title":"make_outputs","text":"<pre><code>make_outputs(\n    target,\n    data=None,\n    *,\n    jobs=None,\n    meta=None,\n    reusable=False,\n    skipped=False,\n    error_msg=None\n)\n</code></pre> <p>Create StageOutput for this stage.</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def make_outputs(\n    self,\n    target: Target,\n    data: ExpectedResultT = None,  # TODO: ExpectedResultT is probably too broad, our code only really support dict\n    *,\n    jobs: Sequence[Job | None] | Job | None = None,\n    meta: dict | None = None,\n    reusable: bool = False,\n    skipped: bool = False,\n    error_msg: str | None = None,\n) -&gt; StageOutput:\n    \"\"\"\n    Create StageOutput for this stage.\n    \"\"\"\n    return StageOutput(\n        target=target,\n        data=data,\n        jobs=jobs,\n        meta=meta,\n        reusable=reusable,\n        skipped=skipped,\n        error_msg=error_msg,\n        stage=self,\n    )\n</code></pre>"},{"location":"reference/stage/#cpg_flow.stage.SequencingGroupStage.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs(target=None)\n</code></pre> <p>Create Hail Batch Job attributes dictionary</p> Source code in <code>src/cpg_flow/stage.py</code> <pre><code>def get_job_attrs(self, target: TargetT | None = None) -&gt; dict[str, str]:\n    \"\"\"\n    Create Hail Batch Job attributes dictionary\n    \"\"\"\n    job_attrs = dict(stage=self.name)\n    if sequencing_type := get_config()['workflow'].get('sequencing_type'):\n        job_attrs['sequencing_type'] = sequencing_type\n    if target:\n        job_attrs |= target.get_job_attrs()\n    return job_attrs\n</code></pre>"},{"location":"reference/status/","title":"Status","text":"<p>Metamist wrappers to report analysis progress.</p>"},{"location":"reference/status/#cpg_flow.status.complete_analysis_job","title":"cpg_flow.status.complete_analysis_job","text":"<pre><code>complete_analysis_job(\n    output,\n    analysis_type,\n    cohort_ids,\n    sg_ids,\n    project_name,\n    meta,\n    update_analysis_meta=None,\n    tolerate_missing=False,\n)\n</code></pre> <p>a job to be called within the batch as a pythonJob this will register the analysis outputs from a Stage</p> PARAMETER DESCRIPTION <code>output</code> <p>path to the output file</p> <p> TYPE: <code>str</code> </p> <code>analysis_type</code> <p>metamist analysis type</p> <p> TYPE: <code>str</code> </p> <code>sg_ids</code> <p>all CPG IDs relevant to this target</p> <p> TYPE: <code>list[str]</code> </p> <code>project_name</code> <p>project/dataset name</p> <p> TYPE: <code>str</code> </p> <code>meta</code> <p>any metadata to add</p> <p> TYPE: <code>dict</code> </p> <code>update_analysis_meta</code> <p>function to update analysis meta</p> <p> TYPE: <code>Callable | None</code> DEFAULT: <code>None</code> </p> <code>tolerate_missing</code> <p>if True, allow missing output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/cpg_flow/status.py</code> <pre><code>def complete_analysis_job(  # noqa: PLR0917\n    output: str,\n    analysis_type: str,\n    cohort_ids: list[str],\n    sg_ids: list[str],\n    project_name: str,\n    meta: dict,\n    update_analysis_meta: Callable | None = None,\n    tolerate_missing: bool = False,\n):\n    \"\"\"\n    a job to be called within the batch as a pythonJob\n    this will register the analysis outputs from a Stage\n\n    Args:\n        output (str): path to the output file\n        analysis_type (str): metamist analysis type\n        sg_ids (list[str]): all CPG IDs relevant to this target\n        project_name (str): project/dataset name\n        meta (dict): any metadata to add\n        update_analysis_meta (Callable | None): function to update analysis meta\n        tolerate_missing (bool): if True, allow missing output\n    \"\"\"\n\n    assert isinstance(output, str)\n    output_cloudpath = to_path(output)\n\n    if update_analysis_meta is not None:\n        meta |= update_analysis_meta(output)\n\n    # if SG IDs are listed in the meta, remove them\n    # these are already captured in the sg_ids list\n    meta.pop('sequencing_groups', None)\n\n    # if the meta has a remove_sgids key, we need to remove those from the list\n    # this occurs when samples are soft-filtered from joint-calls in a way that\n    # doesn't percolate through to the dataset/cohorts\n    # removal here prevents results being registered for samples that were omitted\n    if 'remove_sgids' in meta and to_path(meta['remove_sgids']).exists():\n        with to_path(meta['remove_sgids']).open() as f:\n            exclusion_ids = set(f.read().splitlines())\n            print(f'removing {len(exclusion_ids)} samples from analysis')\n            print(f'samples for removal: {\", \".join(exclusion_ids)}')\n            sg_ids = [sg for sg in sg_ids if sg not in exclusion_ids]\n\n    # we know that es indexes are registered names, not files/dirs\n    # skip all relevant checks for this output type\n    if analysis_type != 'es-index':\n        if not output_cloudpath.exists():\n            if tolerate_missing:\n                print(f\"Output {output} doesn't exist, allowing silent return\")\n                return\n            raise ValueError(f\"Output {output} doesn't exist\")\n\n        # add file size to meta\n        if not output_cloudpath.is_dir():\n            meta |= {'size': output_cloudpath.stat().st_size}\n\n    a_id = get_metamist().create_analysis(\n        output=output,\n        type_=analysis_type,\n        status=AnalysisStatus('completed'),\n        cohort_ids=cohort_ids,\n        sequencing_group_ids=sg_ids,\n        dataset=project_name,\n        meta=meta,\n    )\n    if a_id is None:\n        msg = f'Creation of Analysis failed (type={analysis_type}, output={output}) in {project_name}'\n        print(msg)\n        raise ConnectionError(msg)\n    print(\n        f'Created Analysis(id={a_id}, type={analysis_type}, output={output}) in {project_name}',\n    )\n</code></pre>"},{"location":"reference/status/#cpg_flow.status.StatusReporterError","title":"cpg_flow.status.StatusReporterError","text":"<p>               Bases: <code>Exception</code></p> <p>Error thrown by StatusReporter.</p>"},{"location":"reference/status/#cpg_flow.status.StatusReporter","title":"cpg_flow.status.StatusReporter","text":"<p>               Bases: <code>ABC</code></p> <p>Status reporter</p>"},{"location":"reference/status/#cpg_flow.status.StatusReporter.create_analysis","title":"create_analysis  <code>abstractmethod</code>","text":"<pre><code>create_analysis(\n    b,\n    output,\n    analysis_type,\n    target,\n    jobs=None,\n    job_attr=None,\n    meta=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    project_name=None,\n)\n</code></pre> <p>Record analysis entry.</p> Source code in <code>src/cpg_flow/status.py</code> <pre><code>@abstractmethod\ndef create_analysis(  # noqa: PLR0917\n    self,\n    b: Batch,\n    output: str,\n    analysis_type: str,\n    target: Target,\n    jobs: list[Job] | None = None,\n    job_attr: dict | None = None,\n    meta: dict | None = None,\n    update_analysis_meta: Callable | None = None,\n    tolerate_missing_output: bool = False,\n    project_name: str | None = None,\n):\n    \"\"\"\n    Record analysis entry.\n    \"\"\"\n</code></pre>"},{"location":"reference/status/#cpg_flow.status.MetamistStatusReporter","title":"cpg_flow.status.MetamistStatusReporter","text":"<pre><code>MetamistStatusReporter()\n</code></pre> <p>               Bases: <code>StatusReporter</code></p> <p>Job status reporter. Works through creating metamist Analysis entries.</p> Source code in <code>src/cpg_flow/status.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n</code></pre>"},{"location":"reference/status/#cpg_flow.status.MetamistStatusReporter.create_analysis","title":"create_analysis  <code>staticmethod</code>","text":"<pre><code>create_analysis(\n    b,\n    output,\n    analysis_type,\n    target,\n    jobs=None,\n    job_attr=None,\n    meta=None,\n    update_analysis_meta=None,\n    tolerate_missing_output=False,\n    project_name=None,\n)\n</code></pre> <p>Create completed analysis job</p> Source code in <code>src/cpg_flow/status.py</code> <pre><code>@staticmethod\ndef create_analysis(  # noqa: PLR0917\n    b: Batch,\n    output: str,\n    analysis_type: str,\n    target: Target,\n    jobs: list[Job] | None = None,\n    job_attr: dict | None = None,\n    meta: dict | None = None,\n    update_analysis_meta: Callable | None = None,\n    tolerate_missing_output: bool = False,\n    project_name: str | None = None,\n):\n    \"\"\"\n    Create completed analysis job\n    \"\"\"\n\n    # no jobs means no output, so no need to create analysis\n    if not jobs:\n        return\n\n    if meta is None:\n        meta = {}\n\n    # find all relevant SG IDs\n    # Currently this implementation will only return sg ids or cohort ids\n    # The other list of ids will be empty\n    # It is unclear if metamist would accept both list of ids and succeed\n    cohort_ids = []\n    sequencing_group_ids = []\n    if target is None:\n        raise ValueError('Target is required to create analysis')\n    if isinstance(target, MultiCohort):\n        cohort_ids = target.get_cohort_ids()\n    elif isinstance(target, Cohort):\n        cohort_ids = [target.get_cohort_id()]\n    else:\n        sequencing_group_ids = target.get_sequencing_group_ids()\n\n    py_job = b.new_python_job(\n        f'Register analysis output {output}',\n        job_attr or {} | {'tool': 'metamist'},\n    )\n    py_job.image(get_config()['workflow']['driver_image'])\n    py_job.call(\n        complete_analysis_job,\n        str(output),\n        analysis_type,\n        cohort_ids,\n        sequencing_group_ids,\n        project_name,\n        meta,\n        update_analysis_meta,\n        tolerate_missing_output,\n    )\n\n    py_job.depends_on(*jobs)\n</code></pre>"},{"location":"reference/targets/","title":"<code>Target</code> class","text":"<p>The following <code>Target</code> classes are available to use:</p> <ul> <li><code>Target</code></li> <li><code>Cohort</code></li> <li><code>Dataset</code></li> <li><code>MultiCohort</code></li> <li><code>SequencingGroup</code></li> </ul> <p>You can import them from the <code>cpg_flow</code> package:</p> <pre><code>from cpg_flow.targets import Cohort, Dataset, MultiCohort, SequencingGroup, Target\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target","title":"cpg_flow.targets.Target","text":"<pre><code>Target()\n</code></pre> <p>Defines a target that a stage can act upon.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def __init__(self) -&gt; None:\n    # Whether to process even if outputs exist:\n    self.forced: bool = False\n\n    # If not set, exclude from the workflow:\n    self.active: bool = True\n\n    # create a self.alignment_inputs_hash variable to store the hash of the alignment inputs\n    # this begins as None, and is set upon first calling\n    self.alignment_inputs_hash: str | None = None\n\n    # similar to alignment_inputs_hash, but based on the SG IDs instead of the underlying assays\n    self.sg_hash: str | None = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = False\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.active","title":"active  <code>instance-attribute</code>","text":"<pre><code>active = True\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.alignment_inputs_hash","title":"alignment_inputs_hash  <code>instance-attribute</code>","text":"<pre><code>alignment_inputs_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.sg_hash","title":"sg_hash  <code>instance-attribute</code>","text":"<pre><code>sg_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.target_id","title":"target_id  <code>property</code>","text":"<pre><code>target_id\n</code></pre> <p>ID should be unique across target of all levels.</p> <p>We are raising NotImplementedError instead of making it an abstract class, because mypy is not happy about binding TypeVar to abstract classes, see: https://stackoverflow.com/questions/48349054/how-do-you-annotate-the-type-of -an-abstract-class-with-mypy</p> <p>Specifically, <pre><code>TypeVar('TargetT', bound=Target)\n</code></pre> Will raise: <pre><code>Only concrete class can be given where \"Type[Target]\" is expected\n</code></pre></p>"},{"location":"reference/targets/#cpg_flow.targets.Target.get_sequencing_groups","title":"get_sequencing_groups","text":"<pre><code>get_sequencing_groups(only_active=True)\n</code></pre> <p>Get flat list of all sequencing groups corresponding to this target.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sequencing_groups(\n    self,\n    only_active: bool = True,\n) -&gt; list['SequencingGroup']:\n    \"\"\"\n    Get flat list of all sequencing groups corresponding to this target.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.get_sequencing_group_ids","title":"get_sequencing_group_ids","text":"<pre><code>get_sequencing_group_ids(only_active=True)\n</code></pre> <p>Get flat list of all sequencing group IDs corresponding to this target.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sequencing_group_ids(self, only_active: bool = True) -&gt; list[str]:\n    \"\"\"\n    Get flat list of all sequencing group IDs corresponding to this target.\n    \"\"\"\n    return [s.id for s in self.get_sequencing_groups(only_active=only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.get_alignment_inputs_hash","title":"get_alignment_inputs_hash","text":"<pre><code>get_alignment_inputs_hash()\n</code></pre> <p>If this hash has been set, return it, otherwise set it, then return it This should be safe as it matches the current usage: - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)     - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets) - we then set up the Stages, where alignment input hashes are generated     - at this point, the alignment inputs are fixed     - all calls to get_alignment_inputs_hash() need to return the same value</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_alignment_inputs_hash(self) -&gt; str:\n    \"\"\"\n    If this hash has been set, return it, otherwise set it, then return it\n    This should be safe as it matches the current usage:\n    - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)\n        - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets)\n    - we then set up the Stages, where alignment input hashes are generated\n        - at this point, the alignment inputs are fixed\n        - all calls to get_alignment_inputs_hash() need to return the same value\n    \"\"\"\n    if self.alignment_inputs_hash is None:\n        self.set_alignment_inputs_hash()\n    if self.alignment_inputs_hash is None:\n        raise TypeError('Alignment_inputs_hash was not populated by the setter method')\n    return self.alignment_inputs_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.set_alignment_inputs_hash","title":"set_alignment_inputs_hash","text":"<pre><code>set_alignment_inputs_hash()\n</code></pre> <p>Unique hash string of sample alignment inputs. Useful to decide whether the analysis on the target needs to be rerun.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_alignment_inputs_hash(self):\n    \"\"\"\n    Unique hash string of sample alignment inputs. Useful to decide\n    whether the analysis on the target needs to be rerun.\n    \"\"\"\n    s = ' '.join(\n        sorted(' '.join(str(s.alignment_input)) for s in self.get_sequencing_groups() if s.alignment_input),\n    )\n    h = hashlib.sha256(s.encode()).hexdigest()[:38]\n    self.alignment_inputs_hash = f'{h}_{len(self.get_sequencing_group_ids())}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.get_sg_hash","title":"get_sg_hash","text":"<pre><code>get_sg_hash()\n</code></pre> <p>If the SG hash was generated, return it, otherwise generate and return it.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sg_hash(self) -&gt; str:\n    \"\"\"If the SG hash was generated, return it, otherwise generate and return it.\"\"\"\n    if self.sg_hash is None:\n        self.set_sg_hash()\n    if self.sg_hash is None:\n        raise TypeError('SG_hash was not populated by the setter method')\n    return self.sg_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.set_sg_hash","title":"set_sg_hash","text":"<pre><code>set_sg_hash()\n</code></pre> <p>Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_sg_hash(self):\n    \"\"\"Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.\"\"\"\n    sg_ids = sorted(self.get_sequencing_group_ids())\n    h = hashlib.sha256(''.join(sg_ids).encode()).hexdigest()[:38]\n    self.sg_hash = f'{h}_{len(sg_ids)}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs()\n</code></pre> <p>Attributes for Hail Batch job.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_job_attrs(self) -&gt; dict:\n    \"\"\"\n    Attributes for Hail Batch job.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.get_job_prefix","title":"get_job_prefix","text":"<pre><code>get_job_prefix()\n</code></pre> <p>Prefix job names.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_job_prefix(self) -&gt; str:\n    \"\"\"\n    Prefix job names.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Target.rich_id_map","title":"rich_id_map","text":"<pre><code>rich_id_map()\n</code></pre> <p>Map if internal IDs to participant or external IDs, if the latter is provided.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def rich_id_map(self) -&gt; dict[str, str]:\n    \"\"\"\n    Map if internal IDs to participant or external IDs, if the latter is provided.\n    \"\"\"\n    return {s.id: s.rich_id for s in self.get_sequencing_groups() if s.participant_id != s.id}\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort","title":"cpg_flow.targets.Cohort","text":"<pre><code>Cohort(id=None, name=None, dataset=None)\n</code></pre> <p>               Bases: <code>Target</code></p> <p>Represents a \"cohort\" target - all sequencing groups from a single CustomCohort (potentially spanning multiple datasets) in the workflow. Analysis dataset name is required and will be used as the default name for the cohort.</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def __init__(self, id: str | None = None, name: str | None = None, dataset: str | None = None) -&gt; None:\n    super().__init__()\n    self.id = id or get_config()['workflow']['dataset']\n    self.name = name or get_config()['workflow']['dataset']\n\n    # This is the analysis_dataset specified in the workflow config\n    analysis_dataset = Dataset(name=get_config()['workflow']['dataset'])\n\n    # This value should be populated by the cohort_dataset parameter\n    # which represents the dataset that the cohort is associated with\n    # If no cohort dataset is provided it will default to the analysis dataset\n    self.dataset = Dataset(name=dataset) if dataset else analysis_dataset\n\n    self._sequencing_group_by_id: dict[str, SequencingGroup] = {}\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = False\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.active","title":"active  <code>instance-attribute</code>","text":"<pre><code>active = True\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.alignment_inputs_hash","title":"alignment_inputs_hash  <code>instance-attribute</code>","text":"<pre><code>alignment_inputs_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.sg_hash","title":"sg_hash  <code>instance-attribute</code>","text":"<pre><code>sg_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = id or get_config()['workflow']['dataset']\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name or get_config()['workflow']['dataset']\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = (\n    Dataset(name=dataset) if dataset else analysis_dataset\n)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.target_id","title":"target_id  <code>property</code>","text":"<pre><code>target_id\n</code></pre> <p>Unique target ID</p>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_sequencing_group_ids","title":"get_sequencing_group_ids","text":"<pre><code>get_sequencing_group_ids(only_active=True)\n</code></pre> <p>Get flat list of all sequencing group IDs corresponding to this target.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sequencing_group_ids(self, only_active: bool = True) -&gt; list[str]:\n    \"\"\"\n    Get flat list of all sequencing group IDs corresponding to this target.\n    \"\"\"\n    return [s.id for s in self.get_sequencing_groups(only_active=only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_alignment_inputs_hash","title":"get_alignment_inputs_hash","text":"<pre><code>get_alignment_inputs_hash()\n</code></pre> <p>If this hash has been set, return it, otherwise set it, then return it This should be safe as it matches the current usage: - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)     - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets) - we then set up the Stages, where alignment input hashes are generated     - at this point, the alignment inputs are fixed     - all calls to get_alignment_inputs_hash() need to return the same value</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_alignment_inputs_hash(self) -&gt; str:\n    \"\"\"\n    If this hash has been set, return it, otherwise set it, then return it\n    This should be safe as it matches the current usage:\n    - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)\n        - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets)\n    - we then set up the Stages, where alignment input hashes are generated\n        - at this point, the alignment inputs are fixed\n        - all calls to get_alignment_inputs_hash() need to return the same value\n    \"\"\"\n    if self.alignment_inputs_hash is None:\n        self.set_alignment_inputs_hash()\n    if self.alignment_inputs_hash is None:\n        raise TypeError('Alignment_inputs_hash was not populated by the setter method')\n    return self.alignment_inputs_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.set_alignment_inputs_hash","title":"set_alignment_inputs_hash","text":"<pre><code>set_alignment_inputs_hash()\n</code></pre> <p>Unique hash string of sample alignment inputs. Useful to decide whether the analysis on the target needs to be rerun.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_alignment_inputs_hash(self):\n    \"\"\"\n    Unique hash string of sample alignment inputs. Useful to decide\n    whether the analysis on the target needs to be rerun.\n    \"\"\"\n    s = ' '.join(\n        sorted(' '.join(str(s.alignment_input)) for s in self.get_sequencing_groups() if s.alignment_input),\n    )\n    h = hashlib.sha256(s.encode()).hexdigest()[:38]\n    self.alignment_inputs_hash = f'{h}_{len(self.get_sequencing_group_ids())}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_sg_hash","title":"get_sg_hash","text":"<pre><code>get_sg_hash()\n</code></pre> <p>If the SG hash was generated, return it, otherwise generate and return it.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sg_hash(self) -&gt; str:\n    \"\"\"If the SG hash was generated, return it, otherwise generate and return it.\"\"\"\n    if self.sg_hash is None:\n        self.set_sg_hash()\n    if self.sg_hash is None:\n        raise TypeError('SG_hash was not populated by the setter method')\n    return self.sg_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.set_sg_hash","title":"set_sg_hash","text":"<pre><code>set_sg_hash()\n</code></pre> <p>Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_sg_hash(self):\n    \"\"\"Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.\"\"\"\n    sg_ids = sorted(self.get_sequencing_group_ids())\n    h = hashlib.sha256(''.join(sg_ids).encode()).hexdigest()[:38]\n    self.sg_hash = f'{h}_{len(sg_ids)}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.rich_id_map","title":"rich_id_map","text":"<pre><code>rich_id_map()\n</code></pre> <p>Map if internal IDs to participant or external IDs, if the latter is provided.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def rich_id_map(self) -&gt; dict[str, str]:\n    \"\"\"\n    Map if internal IDs to participant or external IDs, if the latter is provided.\n    \"\"\"\n    return {s.id: s.rich_id for s in self.get_sequencing_groups() if s.participant_id != s.id}\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.prefix","title":"prefix","text":"<pre><code>prefix(unique_for_multicohort=False, **kwargs)\n</code></pre> <p>The primary storage path for the cohort.</p> <p>Constructs the suffix based on whether this is a multi-cohort context. Resulting path structure:     - With workflow &amp; multicohort:  BUCKET/workflow_name/multicohort_hash/cohort_name/...     - With workflow only:           BUCKET/workflow_name/cohort_name/...     - With neither:                 BUCKET/cohort_name/...</p> <p>The inclusion of the multicohort hash is determined when this method is called. The inclusion of the workflow name is determined by the config.</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def prefix(self, unique_for_multicohort: bool = False, **kwargs) -&gt; Path:\n    \"\"\"\n    The primary storage path for the cohort.\n\n    Constructs the suffix based on whether this is a multi-cohort context.\n    Resulting path structure:\n        - With workflow &amp; multicohort:  BUCKET/workflow_name/multicohort_hash/cohort_name/...\n        - With workflow only:           BUCKET/workflow_name/cohort_name/...\n        - With neither:                 BUCKET/cohort_name/...\n\n    The inclusion of the multicohort hash is determined when this method is called.\n    The inclusion of the workflow name is determined by the config.\n    \"\"\"\n    from cpg_flow.inputs import get_multicohort\n\n    path_elements = []\n\n    if workflow_name := config_retrieve(['workflow', 'name'], default=None):\n        path_elements.append(workflow_name)\n\n    if unique_for_multicohort:\n        path_elements.append(get_multicohort().name)\n\n    path_elements.append(self.name)\n\n    return to_path(\n        dataset_path(\n            suffix=os.path.join(*path_elements),\n            dataset=self.dataset.name,\n            **kwargs,\n        )\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_cohort_id","title":"get_cohort_id","text":"<pre><code>get_cohort_id()\n</code></pre> <p>Get the cohort ID</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def get_cohort_id(self) -&gt; str:\n    \"\"\"Get the cohort ID\"\"\"\n    return self.id\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.write_ped_file","title":"write_ped_file","text":"<pre><code>write_ped_file(out_path=None, use_participant_id=False)\n</code></pre> <p>Create a PED file for all samples in the whole cohort PED is written with no header line to be strict specification compliant</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def write_ped_file(\n    self,\n    out_path: Path | None = None,\n    use_participant_id: bool = False,\n) -&gt; Path:\n    \"\"\"\n    Create a PED file for all samples in the whole cohort\n    PED is written with no header line to be strict specification compliant\n    \"\"\"\n    datas = []\n    for sequencing_group in self.get_sequencing_groups():\n        datas.append(\n            sequencing_group.pedigree.get_ped_dict(\n                use_participant_id=use_participant_id,\n            ),\n        )\n    if not datas:\n        raise ValueError(f'No pedigree data found for {self.id}')\n    df = pd.DataFrame(datas)\n\n    if out_path is None:\n        out_path = self.dataset.tmp_prefix() / 'ped' / f'{self.get_alignment_inputs_hash()}.ped'\n\n    if not get_config()['workflow'].get('dry_run', False):\n        with out_path.open('w') as fp:\n            df.to_csv(fp, sep='\\t', index=False, header=False)\n    return out_path\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.add_sequencing_group_object","title":"add_sequencing_group_object","text":"<pre><code>add_sequencing_group_object(s, allow_duplicates=True)\n</code></pre> <p>Add a sequencing group object to the Cohort. Args:     s: SequencingGroup object     allow_duplicates: if True, allow adding the same object twice</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def add_sequencing_group_object(\n    self,\n    s: 'SequencingGroup',\n    allow_duplicates: bool = True,\n):\n    \"\"\"\n    Add a sequencing group object to the Cohort.\n    Args:\n        s: SequencingGroup object\n        allow_duplicates: if True, allow adding the same object twice\n    \"\"\"\n    if s.id in self._sequencing_group_by_id:\n        if allow_duplicates:\n            logger.debug(\n                f'SequencingGroup {s.id} already exists in the Cohort {self.name}',\n            )\n            return self._sequencing_group_by_id[s.id]\n        raise ValueError(\n            f'SequencingGroup {s.id} already exists in the Cohort {self.name}',\n        )\n    self._sequencing_group_by_id[s.id] = s\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_sequencing_groups","title":"get_sequencing_groups","text":"<pre><code>get_sequencing_groups(only_active=True)\n</code></pre> <p>Gets a flat list of all sequencing groups from all datasets. Include only \"active\" sequencing groups (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def get_sequencing_groups(\n    self,\n    only_active: bool = True,\n) -&gt; list['SequencingGroup']:\n    \"\"\"\n    Gets a flat list of all sequencing groups from all datasets.\n    Include only \"active\" sequencing groups (unless only_active is False)\n    \"\"\"\n    return [s for s in self._sequencing_group_by_id.values() if (s.active or not only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs()\n</code></pre> <p>Attributes for Hail Batch job.</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def get_job_attrs(self) -&gt; dict:\n    \"\"\"\n    Attributes for Hail Batch job.\n    \"\"\"\n    return {\n        'sequencing_groups': self.get_sequencing_group_ids(),\n    }\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.get_job_prefix","title":"get_job_prefix","text":"<pre><code>get_job_prefix()\n</code></pre> <p>Prefix job names.</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def get_job_prefix(self) -&gt; str:\n    \"\"\"\n    Prefix job names.\n    \"\"\"\n    return ''\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Cohort.to_tsv","title":"to_tsv","text":"<pre><code>to_tsv()\n</code></pre> <p>Export to a parsable TSV file</p> Source code in <code>src/cpg_flow/targets/cohort.py</code> <pre><code>def to_tsv(self) -&gt; str:\n    \"\"\"\n    Export to a parsable TSV file\n    \"\"\"\n    assert self.get_sequencing_groups()\n    tsv_path = to_path(self.dataset.tmp_prefix() / 'samples.tsv')\n    df = pd.DataFrame(\n        {\n            's': s.id,\n            'gvcf': s.gvcf or '-',\n            'sex': s.meta.get('sex') or '-',\n            'continental_pop': s.meta.get('continental_pop') or '-',\n            'subcontinental_pop': s.meta.get('subcontinental_pop') or '-',\n        }\n        for s in self.get_sequencing_groups()\n    ).set_index('s', drop=False)\n\n    with tsv_path.open('w') as f:\n        df.to_csv(f, index=False, sep='\\t', na_rep='NA')\n\n    return str(tsv_path)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset","title":"cpg_flow.targets.Dataset","text":"<pre><code>Dataset(name)\n</code></pre> <p>               Bases: <code>Target</code></p> <p>Represents a CPG dataset.</p> <p>Each <code>dataset</code> at the CPG corresponds to * a GCP project: https://github.com/populationgenomics/team-docs/tree/main/storage_policies * a Pulumi stack: https://github.com/populationgenomics/analysis-runner/tree/main/stack * a metamist project</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n):\n    super().__init__()\n    self._sequencing_group_by_id: dict[str, SequencingGroup] = {}\n    self.name = name\n    self.active = True\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = False\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.alignment_inputs_hash","title":"alignment_inputs_hash  <code>instance-attribute</code>","text":"<pre><code>alignment_inputs_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.sg_hash","title":"sg_hash  <code>instance-attribute</code>","text":"<pre><code>sg_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = name\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.active","title":"active  <code>instance-attribute</code>","text":"<pre><code>active = True\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.target_id","title":"target_id  <code>property</code>","text":"<pre><code>target_id\n</code></pre> <p>Unique target ID</p>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_sequencing_group_ids","title":"get_sequencing_group_ids","text":"<pre><code>get_sequencing_group_ids(only_active=True)\n</code></pre> <p>Get flat list of all sequencing group IDs corresponding to this target.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sequencing_group_ids(self, only_active: bool = True) -&gt; list[str]:\n    \"\"\"\n    Get flat list of all sequencing group IDs corresponding to this target.\n    \"\"\"\n    return [s.id for s in self.get_sequencing_groups(only_active=only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_alignment_inputs_hash","title":"get_alignment_inputs_hash","text":"<pre><code>get_alignment_inputs_hash()\n</code></pre> <p>If this hash has been set, return it, otherwise set it, then return it This should be safe as it matches the current usage: - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)     - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets) - we then set up the Stages, where alignment input hashes are generated     - at this point, the alignment inputs are fixed     - all calls to get_alignment_inputs_hash() need to return the same value</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_alignment_inputs_hash(self) -&gt; str:\n    \"\"\"\n    If this hash has been set, return it, otherwise set it, then return it\n    This should be safe as it matches the current usage:\n    - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)\n        - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets)\n    - we then set up the Stages, where alignment input hashes are generated\n        - at this point, the alignment inputs are fixed\n        - all calls to get_alignment_inputs_hash() need to return the same value\n    \"\"\"\n    if self.alignment_inputs_hash is None:\n        self.set_alignment_inputs_hash()\n    if self.alignment_inputs_hash is None:\n        raise TypeError('Alignment_inputs_hash was not populated by the setter method')\n    return self.alignment_inputs_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.set_alignment_inputs_hash","title":"set_alignment_inputs_hash","text":"<pre><code>set_alignment_inputs_hash()\n</code></pre> <p>Unique hash string of sample alignment inputs. Useful to decide whether the analysis on the target needs to be rerun.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_alignment_inputs_hash(self):\n    \"\"\"\n    Unique hash string of sample alignment inputs. Useful to decide\n    whether the analysis on the target needs to be rerun.\n    \"\"\"\n    s = ' '.join(\n        sorted(' '.join(str(s.alignment_input)) for s in self.get_sequencing_groups() if s.alignment_input),\n    )\n    h = hashlib.sha256(s.encode()).hexdigest()[:38]\n    self.alignment_inputs_hash = f'{h}_{len(self.get_sequencing_group_ids())}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_sg_hash","title":"get_sg_hash","text":"<pre><code>get_sg_hash()\n</code></pre> <p>If the SG hash was generated, return it, otherwise generate and return it.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sg_hash(self) -&gt; str:\n    \"\"\"If the SG hash was generated, return it, otherwise generate and return it.\"\"\"\n    if self.sg_hash is None:\n        self.set_sg_hash()\n    if self.sg_hash is None:\n        raise TypeError('SG_hash was not populated by the setter method')\n    return self.sg_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.set_sg_hash","title":"set_sg_hash","text":"<pre><code>set_sg_hash()\n</code></pre> <p>Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_sg_hash(self):\n    \"\"\"Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.\"\"\"\n    sg_ids = sorted(self.get_sequencing_group_ids())\n    h = hashlib.sha256(''.join(sg_ids).encode()).hexdigest()[:38]\n    self.sg_hash = f'{h}_{len(sg_ids)}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.rich_id_map","title":"rich_id_map","text":"<pre><code>rich_id_map()\n</code></pre> <p>Map if internal IDs to participant or external IDs, if the latter is provided.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def rich_id_map(self) -&gt; dict[str, str]:\n    \"\"\"\n    Map if internal IDs to participant or external IDs, if the latter is provided.\n    \"\"\"\n    return {s.id: s.rich_id for s in self.get_sequencing_groups() if s.participant_id != s.id}\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(name)\n</code></pre> <p>Create a dataset.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>@staticmethod\ndef create(name: str) -&gt; 'Dataset':\n    \"\"\"\n    Create a dataset.\n    \"\"\"\n    return Dataset(name=name)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.prefix","title":"prefix","text":"<pre><code>prefix(**kwargs)\n</code></pre> <p>The primary storage path.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def prefix(self, **kwargs) -&gt; Path:\n    \"\"\"\n    The primary storage path.\n    \"\"\"\n    return to_path(\n        dataset_path(\n            sequencing_subdir(),\n            dataset=self.name,\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.tmp_prefix","title":"tmp_prefix","text":"<pre><code>tmp_prefix(**kwargs)\n</code></pre> <p>Storage path for temporary files.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def tmp_prefix(self, **kwargs) -&gt; Path:\n    \"\"\"\n    Storage path for temporary files.\n    \"\"\"\n    return to_path(\n        dataset_path(\n            sequencing_subdir(),\n            dataset=self.name,\n            category='tmp',\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.analysis_prefix","title":"analysis_prefix","text":"<pre><code>analysis_prefix(**kwargs)\n</code></pre> <p>Storage path for analysis files.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def analysis_prefix(self, **kwargs) -&gt; Path:\n    \"\"\"\n    Storage path for analysis files.\n    \"\"\"\n    return to_path(\n        dataset_path(\n            sequencing_subdir(),\n            dataset=self.name,\n            category='analysis',\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.web_prefix","title":"web_prefix","text":"<pre><code>web_prefix(**kwargs)\n</code></pre> <p>Path for files served by an HTTP server Matches corresponding URLs returns by self.web_url() URLs.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def web_prefix(self, **kwargs) -&gt; Path:\n    \"\"\"\n    Path for files served by an HTTP server Matches corresponding URLs returns by\n    self.web_url() URLs.\n    \"\"\"\n    return to_path(\n        dataset_path(\n            sequencing_subdir(),\n            dataset=self.name,\n            category='web',\n            **kwargs,\n        ),\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.web_url","title":"web_url","text":"<pre><code>web_url()\n</code></pre> <p>URLs matching self.storage_web_path() files serverd by an HTTP server.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def web_url(self) -&gt; str | None:\n    \"\"\"\n    URLs matching self.storage_web_path() files serverd by an HTTP server.\n    \"\"\"\n    return web_url(\n        sequencing_subdir(),\n        dataset=self.name,\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.add_sequencing_group","title":"add_sequencing_group","text":"<pre><code>add_sequencing_group(\n    id,\n    *,\n    sequencing_type,\n    sequencing_technology,\n    sequencing_platform,\n    external_id=None,\n    participant_id=None,\n    meta=None,\n    sex=None,\n    pedigree=None,\n    alignment_input=None\n)\n</code></pre> <p>Create a new sequencing group and add it to the dataset.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def add_sequencing_group(\n    self,\n    id: str,  # pylint: disable=redefined-builtin\n    *,\n    sequencing_type: str,\n    sequencing_technology: str,\n    sequencing_platform: str,\n    external_id: str | None = None,\n    participant_id: str | None = None,\n    meta: dict | None = None,\n    sex: Optional['Sex'] = None,\n    pedigree: Optional['PedigreeInfo'] = None,\n    alignment_input: AlignmentInput | None = None,\n) -&gt; 'SequencingGroup':\n    \"\"\"\n    Create a new sequencing group and add it to the dataset.\n    \"\"\"\n    if id in self._sequencing_group_by_id:\n        logger.debug(\n            f'SequencingGroup {id} already exists in the dataset {self.name}',\n        )\n        return self._sequencing_group_by_id[id]\n\n    force_sgs = get_config()['workflow'].get('force_sgs', set())\n    forced = id in force_sgs or external_id in force_sgs or participant_id in force_sgs\n\n    s = SequencingGroup(\n        id=id,\n        dataset=self,\n        external_id=external_id,\n        sequencing_type=sequencing_type,\n        sequencing_technology=sequencing_technology,\n        sequencing_platform=sequencing_platform,\n        participant_id=participant_id,\n        meta=meta,\n        sex=sex,\n        pedigree=pedigree,\n        alignment_input=alignment_input,\n        forced=forced,\n    )\n    self._sequencing_group_by_id[id] = s\n    return s\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.add_sequencing_group_object","title":"add_sequencing_group_object","text":"<pre><code>add_sequencing_group_object(s)\n</code></pre> <p>Add a sequencing group object to the dataset. Args:     s: SequencingGroup object</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def add_sequencing_group_object(self, s: 'SequencingGroup'):\n    \"\"\"\n    Add a sequencing group object to the dataset.\n    Args:\n        s: SequencingGroup object\n    \"\"\"\n    if s.id in self._sequencing_group_by_id:\n        logger.debug(\n            f'SequencingGroup {s.id} already exists in the dataset {self.name}',\n        )\n    else:\n        self._sequencing_group_by_id[s.id] = s\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_sequencing_group_by_id","title":"get_sequencing_group_by_id","text":"<pre><code>get_sequencing_group_by_id(id)\n</code></pre> <p>Get sequencing group by ID</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def get_sequencing_group_by_id(self, id: str) -&gt; Optional['SequencingGroup']:\n    \"\"\"\n    Get sequencing group by ID\n    \"\"\"\n    return self._sequencing_group_by_id.get(id)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_sequencing_groups","title":"get_sequencing_groups","text":"<pre><code>get_sequencing_groups(only_active=True)\n</code></pre> <p>Get dataset's sequencing groups. Include only \"active\" sequencing groups, unless only_active=False</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def get_sequencing_groups(\n    self,\n    only_active: bool = True,\n) -&gt; list['SequencingGroup']:\n    \"\"\"\n    Get dataset's sequencing groups. Include only \"active\" sequencing groups, unless only_active=False\n    \"\"\"\n    return [s for sid, s in self._sequencing_group_by_id.items() if (s.active or not only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs()\n</code></pre> <p>Attributes for Hail Batch job.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def get_job_attrs(self) -&gt; dict:\n    \"\"\"\n    Attributes for Hail Batch job.\n    \"\"\"\n    return {\n        'dataset': self.name,\n        'sequencing_groups': self.get_sequencing_group_ids(),\n    }\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.get_job_prefix","title":"get_job_prefix","text":"<pre><code>get_job_prefix()\n</code></pre> <p>Prefix job names.</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def get_job_prefix(self) -&gt; str:\n    \"\"\"\n    Prefix job names.\n    \"\"\"\n    return f'{self.name}: '\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.Dataset.write_ped_file","title":"write_ped_file","text":"<pre><code>write_ped_file(out_path=None, use_participant_id=False)\n</code></pre> <p>Create a PED file for all sequencing groups PED is written with no header line to be strict specification compliant</p> Source code in <code>src/cpg_flow/targets/dataset.py</code> <pre><code>def write_ped_file(\n    self,\n    out_path: Path | None = None,\n    use_participant_id: bool = False,\n) -&gt; Path:\n    \"\"\"\n    Create a PED file for all sequencing groups\n    PED is written with no header line to be strict specification compliant\n    \"\"\"\n    datas = []\n    for sequencing_group in self.get_sequencing_groups():\n        datas.append(\n            sequencing_group.pedigree.get_ped_dict(\n                use_participant_id=use_participant_id,\n            ),\n        )\n    if not datas:\n        raise ValueError(f'No pedigree data found for {self.name}')\n    df = pd.DataFrame(datas)\n\n    if out_path is None:\n        out_path = self.tmp_prefix() / 'ped' / f'{self.get_alignment_inputs_hash()}.ped'\n\n    if not get_config()['workflow'].get('dry_run', False):\n        with out_path.open('w') as fp:\n            df.to_csv(fp, sep='\\t', index=False, header=False)\n    return out_path\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort","title":"cpg_flow.targets.MultiCohort","text":"<pre><code>MultiCohort()\n</code></pre> <p>               Bases: <code>Target</code></p> <p>Represents a \"multi-cohort\" target - multiple cohorts in the workflow.</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def __init__(self) -&gt; None:\n    super().__init__()\n\n    # Previously MultiCohort.name was an underscore-delimited string of all the input cohorts\n    # this was expanding to the point where filenames including this String were too long for *nix\n    # instead we can create a hash of the input cohorts, and use that as the name\n    # the exact cohorts can be obtained from the config associated with the ar-guid\n    input_cohorts = get_config()['workflow'].get('input_cohorts', [])\n    if input_cohorts:\n        self.name = hash_from_list_of_strings(sorted(input_cohorts), suffix='cohorts')\n    else:\n        self.name = get_config()['workflow']['dataset']\n\n    assert self.name, 'Ensure cohorts or dataset is defined in the config file.'\n\n    self._cohorts_by_id: dict[str, Cohort] = {}\n    self._datasets_by_name: dict[str, Dataset] = {}\n    self.analysis_dataset = Dataset(name=get_config()['workflow']['dataset'])\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = False\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.active","title":"active  <code>instance-attribute</code>","text":"<pre><code>active = True\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.alignment_inputs_hash","title":"alignment_inputs_hash  <code>instance-attribute</code>","text":"<pre><code>alignment_inputs_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.sg_hash","title":"sg_hash  <code>instance-attribute</code>","text":"<pre><code>sg_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = hash_from_list_of_strings(\n    sorted(input_cohorts), suffix=\"cohorts\"\n)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.analysis_dataset","title":"analysis_dataset  <code>instance-attribute</code>","text":"<pre><code>analysis_dataset = Dataset(\n    name=get_config()[\"workflow\"][\"dataset\"]\n)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.target_id","title":"target_id  <code>property</code>","text":"<pre><code>target_id\n</code></pre> <p>Unique target ID</p>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_sequencing_group_ids","title":"get_sequencing_group_ids","text":"<pre><code>get_sequencing_group_ids(only_active=True)\n</code></pre> <p>Get flat list of all sequencing group IDs corresponding to this target.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sequencing_group_ids(self, only_active: bool = True) -&gt; list[str]:\n    \"\"\"\n    Get flat list of all sequencing group IDs corresponding to this target.\n    \"\"\"\n    return [s.id for s in self.get_sequencing_groups(only_active=only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_alignment_inputs_hash","title":"get_alignment_inputs_hash","text":"<pre><code>get_alignment_inputs_hash()\n</code></pre> <p>If this hash has been set, return it, otherwise set it, then return it This should be safe as it matches the current usage: - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)     - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets) - we then set up the Stages, where alignment input hashes are generated     - at this point, the alignment inputs are fixed     - all calls to get_alignment_inputs_hash() need to return the same value</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_alignment_inputs_hash(self) -&gt; str:\n    \"\"\"\n    If this hash has been set, return it, otherwise set it, then return it\n    This should be safe as it matches the current usage:\n    - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)\n        - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets)\n    - we then set up the Stages, where alignment input hashes are generated\n        - at this point, the alignment inputs are fixed\n        - all calls to get_alignment_inputs_hash() need to return the same value\n    \"\"\"\n    if self.alignment_inputs_hash is None:\n        self.set_alignment_inputs_hash()\n    if self.alignment_inputs_hash is None:\n        raise TypeError('Alignment_inputs_hash was not populated by the setter method')\n    return self.alignment_inputs_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.set_alignment_inputs_hash","title":"set_alignment_inputs_hash","text":"<pre><code>set_alignment_inputs_hash()\n</code></pre> <p>Unique hash string of sample alignment inputs. Useful to decide whether the analysis on the target needs to be rerun.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_alignment_inputs_hash(self):\n    \"\"\"\n    Unique hash string of sample alignment inputs. Useful to decide\n    whether the analysis on the target needs to be rerun.\n    \"\"\"\n    s = ' '.join(\n        sorted(' '.join(str(s.alignment_input)) for s in self.get_sequencing_groups() if s.alignment_input),\n    )\n    h = hashlib.sha256(s.encode()).hexdigest()[:38]\n    self.alignment_inputs_hash = f'{h}_{len(self.get_sequencing_group_ids())}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_sg_hash","title":"get_sg_hash","text":"<pre><code>get_sg_hash()\n</code></pre> <p>If the SG hash was generated, return it, otherwise generate and return it.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sg_hash(self) -&gt; str:\n    \"\"\"If the SG hash was generated, return it, otherwise generate and return it.\"\"\"\n    if self.sg_hash is None:\n        self.set_sg_hash()\n    if self.sg_hash is None:\n        raise TypeError('SG_hash was not populated by the setter method')\n    return self.sg_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.set_sg_hash","title":"set_sg_hash","text":"<pre><code>set_sg_hash()\n</code></pre> <p>Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_sg_hash(self):\n    \"\"\"Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.\"\"\"\n    sg_ids = sorted(self.get_sequencing_group_ids())\n    h = hashlib.sha256(''.join(sg_ids).encode()).hexdigest()[:38]\n    self.sg_hash = f'{h}_{len(sg_ids)}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_job_prefix","title":"get_job_prefix","text":"<pre><code>get_job_prefix()\n</code></pre> <p>Prefix job names.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_job_prefix(self) -&gt; str:\n    \"\"\"\n    Prefix job names.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.rich_id_map","title":"rich_id_map","text":"<pre><code>rich_id_map()\n</code></pre> <p>Map if internal IDs to participant or external IDs, if the latter is provided.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def rich_id_map(self) -&gt; dict[str, str]:\n    \"\"\"\n    Map if internal IDs to participant or external IDs, if the latter is provided.\n    \"\"\"\n    return {s.id: s.rich_id for s in self.get_sequencing_groups() if s.participant_id != s.id}\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.create_dataset","title":"create_dataset","text":"<pre><code>create_dataset(name)\n</code></pre> <p>Create a dataset and add it to the cohort.</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def create_dataset(self, name: str) -&gt; 'Dataset':\n    \"\"\"\n    Create a dataset and add it to the cohort.\n    \"\"\"\n    if name in self._datasets_by_name:\n        return self._datasets_by_name[name]\n\n    if name == self.analysis_dataset.name:\n        ds = self.analysis_dataset\n    else:\n        ds = Dataset(name=name)\n\n    self._datasets_by_name[ds.name] = ds\n    return ds\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_cohorts","title":"get_cohorts","text":"<pre><code>get_cohorts(only_active=True)\n</code></pre> <p>Gets list of all cohorts. Include only \"active\" cohorts (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_cohorts(self, only_active: bool = True) -&gt; list['Cohort']:\n    \"\"\"\n    Gets list of all cohorts.\n    Include only \"active\" cohorts (unless only_active is False)\n    \"\"\"\n    cohorts = list(self._cohorts_by_id.values())\n    if only_active:\n        cohorts = [c for c in cohorts if c.active]\n    return cohorts\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_cohort_ids","title":"get_cohort_ids","text":"<pre><code>get_cohort_ids(only_active=True)\n</code></pre> <p>Get list of cohort IDs. Include only \"active\" cohorts (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_cohort_ids(self, only_active: bool = True) -&gt; list['str']:\n    \"\"\"\n    Get list of cohort IDs.\n    Include only \"active\" cohorts (unless only_active is False)\n    \"\"\"\n    return [c.get_cohort_id() for c in self.get_cohorts(only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_cohort_by_id","title":"get_cohort_by_id","text":"<pre><code>get_cohort_by_id(id, only_active=True)\n</code></pre> <p>Get cohort by id. Include only \"active\" cohorts (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_cohort_by_id(\n    self,\n    id: str,\n    only_active: bool = True,\n) -&gt; Optional['Cohort']:\n    \"\"\"\n    Get cohort by id.\n    Include only \"active\" cohorts (unless only_active is False)\n    \"\"\"\n    cohort = self._cohorts_by_id.get(id)\n    if not cohort:\n        logger.warning(f'Cohort {id} not found in the multi-cohort')\n\n    if not only_active:  # Return cohort even if it's inactive\n        return cohort\n    if isinstance(cohort, Cohort) and cohort.active:\n        return cohort\n    return None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_datasets","title":"get_datasets","text":"<pre><code>get_datasets(only_active=True)\n</code></pre> <p>Gets list of all datasets. Include only \"active\" datasets (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_datasets(self, only_active: bool = True) -&gt; list['Dataset']:\n    \"\"\"\n    Gets list of all datasets.\n    Include only \"active\" datasets (unless only_active is False)\n    \"\"\"\n    all_datasets = list(self._datasets_by_name.values())\n    if only_active:\n        all_datasets = [d for d in all_datasets if d.active and d.get_sequencing_groups()]\n    return all_datasets\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_sequencing_groups","title":"get_sequencing_groups","text":"<pre><code>get_sequencing_groups(only_active=True)\n</code></pre> <p>Gets a flat list of all sequencing groups from all datasets. uses a dictionary to avoid duplicates (we could have the same sequencing group in multiple cohorts) Include only \"active\" sequencing groups (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_sequencing_groups(\n    self,\n    only_active: bool = True,\n) -&gt; list['SequencingGroup']:\n    \"\"\"\n    Gets a flat list of all sequencing groups from all datasets.\n    uses a dictionary to avoid duplicates (we could have the same sequencing group in multiple cohorts)\n    Include only \"active\" sequencing groups (unless only_active is False)\n    \"\"\"\n    all_sequencing_groups: dict[str, SequencingGroup] = {}\n    for dataset in self.get_datasets(only_active):\n        for sg in dataset.get_sequencing_groups(only_active):\n            all_sequencing_groups[sg.id] = sg\n    return list(all_sequencing_groups.values())\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.create_cohort","title":"create_cohort","text":"<pre><code>create_cohort(id, name, dataset=None)\n</code></pre> <p>Create a cohort and add it to the multi-cohort.</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def create_cohort(self, id: str, name: str, dataset: str | None = None) -&gt; 'Cohort':\n    \"\"\"\n    Create a cohort and add it to the multi-cohort.\n    \"\"\"\n    if id in self._cohorts_by_id:\n        logger.debug(f'Cohort {id} already exists in the multi-cohort')\n        return self._cohorts_by_id[id]\n\n    c = Cohort(id=id, name=name, dataset=dataset)\n    self._cohorts_by_id[c.id] = c\n    return c\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.add_dataset","title":"add_dataset","text":"<pre><code>add_dataset(d)\n</code></pre> <p>Add a Dataset to the MultiCohort Args:     d: Dataset object</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def add_dataset(self, d: 'Dataset') -&gt; 'Dataset':\n    \"\"\"\n    Add a Dataset to the MultiCohort\n    Args:\n        d: Dataset object\n    \"\"\"\n    if d.name in self._datasets_by_name:\n        logger.debug(\n            f'Dataset {d.name} already exists in the MultiCohort {self.name}',\n        )\n    else:\n        # We need create a new dataset to avoid manipulating the cohort dataset at this point\n        self._datasets_by_name[d.name] = Dataset(d.name)\n    return self._datasets_by_name[d.name]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_dataset_by_name","title":"get_dataset_by_name","text":"<pre><code>get_dataset_by_name(name, only_active=True)\n</code></pre> <p>Get dataset by name. Include only \"active\" datasets (unless only_active is False)</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_dataset_by_name(\n    self,\n    name: str,\n    only_active: bool = True,\n) -&gt; Optional['Dataset']:\n    \"\"\"\n    Get dataset by name.\n    Include only \"active\" datasets (unless only_active is False)\n    \"\"\"\n    ds_by_name = {d.name: d for d in self.get_datasets(only_active)}\n    return ds_by_name.get(name)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs()\n</code></pre> <p>Attributes for Hail Batch job.</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def get_job_attrs(self) -&gt; dict:\n    \"\"\"\n    Attributes for Hail Batch job.\n    \"\"\"\n    return {\n        'sequencing_groups': self.get_sequencing_group_ids(),\n        'datasets': [d.name for d in self.get_datasets()],\n        'cohorts': [c.id for c in self.get_cohorts()],\n    }\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.MultiCohort.write_ped_file","title":"write_ped_file","text":"<pre><code>write_ped_file(out_path=None, use_participant_id=False)\n</code></pre> <p>Create a PED file for all samples in the whole MultiCohort Duplication of the Cohort method PED is written with no header line to be strict specification compliant</p> Source code in <code>src/cpg_flow/targets/multicohort.py</code> <pre><code>def write_ped_file(\n    self,\n    out_path: Path | None = None,\n    use_participant_id: bool = False,\n) -&gt; Path:\n    \"\"\"\n    Create a PED file for all samples in the whole MultiCohort\n    Duplication of the Cohort method\n    PED is written with no header line to be strict specification compliant\n    \"\"\"\n    datas = []\n    for sequencing_group in self.get_sequencing_groups():\n        datas.append(\n            sequencing_group.pedigree.get_ped_dict(\n                use_participant_id=use_participant_id,\n            ),\n        )\n    if not datas:\n        raise ValueError(f'No pedigree data found for {self.name}')\n    df = pd.DataFrame(datas)\n\n    if out_path is None:\n        out_path = self.analysis_dataset.tmp_prefix() / 'ped' / f'{self.get_alignment_inputs_hash()}.ped'\n\n    if not get_config()['workflow'].get('dry_run', False):\n        with out_path.open('w') as fp:\n            df.to_csv(fp, sep='\\t', index=False, header=False)\n    return out_path\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup","title":"cpg_flow.targets.SequencingGroup","text":"<pre><code>SequencingGroup(\n    id,\n    dataset,\n    *,\n    sequencing_type,\n    sequencing_technology,\n    sequencing_platform,\n    external_id=None,\n    participant_id=None,\n    meta=None,\n    sex=None,\n    pedigree=None,\n    alignment_input=None,\n    assays=None,\n    forced=False\n)\n</code></pre> <p>               Bases: <code>Target</code></p> <p>Represents a sequencing group.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def __init__(\n    self,\n    id: str,\n    dataset: 'Dataset',\n    *,\n    sequencing_type: str,\n    sequencing_technology: str,\n    sequencing_platform: str,\n    external_id: str | None = None,\n    participant_id: str | None = None,\n    meta: dict | None = None,\n    sex: Sex | None = None,\n    pedigree: Optional['PedigreeInfo'] = None,\n    alignment_input: AlignmentInput | None = None,\n    assays: tuple[Assay, ...] | None = None,\n    forced: bool = False,\n):\n    super().__init__()\n    self.id = id\n    self.name = id\n    self._external_id = external_id\n    self.sequencing_type = sequencing_type\n    self.sequencing_technology = sequencing_technology\n    self.sequencing_platform = sequencing_platform\n\n    self.dataset = dataset\n    self._participant_id = participant_id\n    self.meta: dict = meta or dict()\n    self.pedigree: PedigreeInfo = pedigree or PedigreeInfo(\n        sequencing_group=self,\n        fam_id=self.participant_id,\n        sex=sex or Sex.UNKNOWN,\n    )\n    if sex:\n        self.pedigree.sex = sex\n    self.alignment_input: AlignmentInput | None = alignment_input\n    self.assays: tuple[Assay, ...] | None = assays\n    self.forced = forced\n    self.active = True\n    # Only set if the file exists / found in Metamist:\n    self.gvcf: GvcfPath | None = None\n    self.cram: CramPath | None = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.alignment_inputs_hash","title":"alignment_inputs_hash  <code>instance-attribute</code>","text":"<pre><code>alignment_inputs_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.sg_hash","title":"sg_hash  <code>instance-attribute</code>","text":"<pre><code>sg_hash = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id = id\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = id\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.sequencing_type","title":"sequencing_type  <code>instance-attribute</code>","text":"<pre><code>sequencing_type = sequencing_type\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.sequencing_technology","title":"sequencing_technology  <code>instance-attribute</code>","text":"<pre><code>sequencing_technology = sequencing_technology\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.sequencing_platform","title":"sequencing_platform  <code>instance-attribute</code>","text":"<pre><code>sequencing_platform = sequencing_platform\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.dataset","title":"dataset  <code>instance-attribute</code>","text":"<pre><code>dataset = dataset\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.meta","title":"meta  <code>instance-attribute</code>","text":"<pre><code>meta = meta or dict()\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.pedigree","title":"pedigree  <code>instance-attribute</code>","text":"<pre><code>pedigree = pedigree or PedigreeInfo(\n    sequencing_group=self,\n    fam_id=participant_id,\n    sex=sex or UNKNOWN,\n)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.alignment_input","title":"alignment_input  <code>instance-attribute</code>","text":"<pre><code>alignment_input = alignment_input\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.assays","title":"assays  <code>instance-attribute</code>","text":"<pre><code>assays = assays\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.forced","title":"forced  <code>instance-attribute</code>","text":"<pre><code>forced = forced\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.active","title":"active  <code>instance-attribute</code>","text":"<pre><code>active = True\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.gvcf","title":"gvcf  <code>instance-attribute</code>","text":"<pre><code>gvcf = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.cram","title":"cram  <code>instance-attribute</code>","text":"<pre><code>cram = None\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.participant_id","title":"participant_id  <code>property</code> <code>writable</code>","text":"<pre><code>participant_id\n</code></pre> <p>Get ID of participant corresponding to this sequencing group, or substitute it with external ID.</p>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.external_id","title":"external_id  <code>property</code>","text":"<pre><code>external_id\n</code></pre> <p>Get external sample ID, or substitute it with the internal ID.</p>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.rich_id","title":"rich_id  <code>property</code>","text":"<pre><code>rich_id\n</code></pre> <p>ID for reporting purposes: composed of internal as well as external or participant IDs.</p>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.make_sv_evidence_path","title":"make_sv_evidence_path  <code>property</code>","text":"<pre><code>make_sv_evidence_path\n</code></pre> <p>Path to the evidence root for GATK-SV evidence files.</p>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.target_id","title":"target_id  <code>property</code>","text":"<pre><code>target_id\n</code></pre> <p>Unique target ID</p>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_sequencing_group_ids","title":"get_sequencing_group_ids","text":"<pre><code>get_sequencing_group_ids(only_active=True)\n</code></pre> <p>Get flat list of all sequencing group IDs corresponding to this target.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sequencing_group_ids(self, only_active: bool = True) -&gt; list[str]:\n    \"\"\"\n    Get flat list of all sequencing group IDs corresponding to this target.\n    \"\"\"\n    return [s.id for s in self.get_sequencing_groups(only_active=only_active)]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_alignment_inputs_hash","title":"get_alignment_inputs_hash","text":"<pre><code>get_alignment_inputs_hash()\n</code></pre> <p>If this hash has been set, return it, otherwise set it, then return it This should be safe as it matches the current usage: - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)     - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets) - we then set up the Stages, where alignment input hashes are generated     - at this point, the alignment inputs are fixed     - all calls to get_alignment_inputs_hash() need to return the same value</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_alignment_inputs_hash(self) -&gt; str:\n    \"\"\"\n    If this hash has been set, return it, otherwise set it, then return it\n    This should be safe as it matches the current usage:\n    - we set up the Targets in this workflow (populating SGs, Datasets, Cohorts)\n        - at this point the targets are malleable (e.g. addition of an additional Cohort may add SGs to Datasets)\n    - we then set up the Stages, where alignment input hashes are generated\n        - at this point, the alignment inputs are fixed\n        - all calls to get_alignment_inputs_hash() need to return the same value\n    \"\"\"\n    if self.alignment_inputs_hash is None:\n        self.set_alignment_inputs_hash()\n    if self.alignment_inputs_hash is None:\n        raise TypeError('Alignment_inputs_hash was not populated by the setter method')\n    return self.alignment_inputs_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.set_alignment_inputs_hash","title":"set_alignment_inputs_hash","text":"<pre><code>set_alignment_inputs_hash()\n</code></pre> <p>Unique hash string of sample alignment inputs. Useful to decide whether the analysis on the target needs to be rerun.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_alignment_inputs_hash(self):\n    \"\"\"\n    Unique hash string of sample alignment inputs. Useful to decide\n    whether the analysis on the target needs to be rerun.\n    \"\"\"\n    s = ' '.join(\n        sorted(' '.join(str(s.alignment_input)) for s in self.get_sequencing_groups() if s.alignment_input),\n    )\n    h = hashlib.sha256(s.encode()).hexdigest()[:38]\n    self.alignment_inputs_hash = f'{h}_{len(self.get_sequencing_group_ids())}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_sg_hash","title":"get_sg_hash","text":"<pre><code>get_sg_hash()\n</code></pre> <p>If the SG hash was generated, return it, otherwise generate and return it.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def get_sg_hash(self) -&gt; str:\n    \"\"\"If the SG hash was generated, return it, otherwise generate and return it.\"\"\"\n    if self.sg_hash is None:\n        self.set_sg_hash()\n    if self.sg_hash is None:\n        raise TypeError('SG_hash was not populated by the setter method')\n    return self.sg_hash\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.set_sg_hash","title":"set_sg_hash","text":"<pre><code>set_sg_hash()\n</code></pre> <p>Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def set_sg_hash(self):\n    \"\"\"Unique hash string from Sequencing Group IDs, used to create a unique string to use in output paths.\"\"\"\n    sg_ids = sorted(self.get_sequencing_group_ids())\n    h = hashlib.sha256(''.join(sg_ids).encode()).hexdigest()[:38]\n    self.sg_hash = f'{h}_{len(sg_ids)}'\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.rich_id_map","title":"rich_id_map","text":"<pre><code>rich_id_map()\n</code></pre> <p>Map if internal IDs to participant or external IDs, if the latter is provided.</p> Source code in <code>src/cpg_flow/targets/target.py</code> <pre><code>def rich_id_map(self) -&gt; dict[str, str]:\n    \"\"\"\n    Map if internal IDs to participant or external IDs, if the latter is provided.\n    \"\"\"\n    return {s.id: s.rich_id for s in self.get_sequencing_groups() if s.participant_id != s.id}\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_ped_dict","title":"get_ped_dict","text":"<pre><code>get_ped_dict(use_participant_id=False)\n</code></pre> <p>Returns a dictionary of pedigree fields for this sequencing group, corresponding a PED file entry.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def get_ped_dict(self, use_participant_id: bool = False) -&gt; dict[str, str]:\n    \"\"\"\n    Returns a dictionary of pedigree fields for this sequencing group, corresponding\n    a PED file entry.\n    \"\"\"\n    return self.pedigree.get_ped_dict(use_participant_id)\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.make_cram_path","title":"make_cram_path","text":"<pre><code>make_cram_path()\n</code></pre> <p>Path to a CRAM file. Not checking its existence here.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def make_cram_path(self) -&gt; CramPath:\n    \"\"\"\n    Path to a CRAM file. Not checking its existence here.\n    \"\"\"\n    path = self.dataset.prefix() / 'cram' / f'{self.id}.cram'\n    return CramPath(\n        path=path,\n        index_path=path.with_suffix('.cram.crai'),\n        reference_assembly=reference_path('broad/ref_fasta'),\n    )\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.make_gvcf_path","title":"make_gvcf_path","text":"<pre><code>make_gvcf_path()\n</code></pre> <p>Path to a GVCF file. Not checking its existence here.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def make_gvcf_path(self) -&gt; GvcfPath:\n    \"\"\"\n    Path to a GVCF file. Not checking its existence here.\n    \"\"\"\n    return GvcfPath(self.dataset.prefix() / 'gvcf' / f'{self.id}.g.vcf.gz')\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_sequencing_groups","title":"get_sequencing_groups","text":"<pre><code>get_sequencing_groups(only_active=True)\n</code></pre> <p>Implementing the abstract method.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def get_sequencing_groups(\n    self,\n    only_active: bool = True,\n) -&gt; list['SequencingGroup']:\n    \"\"\"\n    Implementing the abstract method.\n    \"\"\"\n    if only_active and not self.active:\n        return []\n    return [self]\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_job_attrs","title":"get_job_attrs","text":"<pre><code>get_job_attrs()\n</code></pre> <p>Attributes for Hail Batch job.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def get_job_attrs(self) -&gt; dict:\n    \"\"\"\n    Attributes for Hail Batch job.\n    \"\"\"\n    attrs = {\n        'dataset': self.dataset.name,\n        'sequencing_group': self.id,\n    }\n    participant_id: str | None = self._participant_id or self._external_id\n    if participant_id:\n        attrs['participant_id'] = participant_id\n    return attrs\n</code></pre>"},{"location":"reference/targets/#cpg_flow.targets.SequencingGroup.get_job_prefix","title":"get_job_prefix","text":"<pre><code>get_job_prefix()\n</code></pre> <p>Prefix job names.</p> Source code in <code>src/cpg_flow/targets/sequencing_group.py</code> <pre><code>def get_job_prefix(self) -&gt; str:\n    \"\"\"\n    Prefix job names.\n    \"\"\"\n    return f'{self.dataset.name}/{self.id}: '\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":"<p>Utility functions and constants.</p>"},{"location":"reference/utils/#cpg_flow.utils.format_logger","title":"cpg_flow.utils.format_logger","text":"<pre><code>format_logger(\n    log_level=no,\n    fmt_string=DEFAULT_LOG_FORMAT,\n    coloured=COLOURED_LOGS,\n)\n</code></pre> <p>loguru is a cleaner interface than the standard logging module, but it doesn't allow for multiple instances instead of calling a get_logger function which returns a logger, we assume that any module using logging has imported <code>from loguru import logger</code> to get access to the logger.</p> <p>loguru.logger is also resistant to deepcopy, so there really is only a single global instance, meaning that the display/formatting of the logger is global to the entire process, and should only be set once.</p> <p>This helper method formats the logger instance with the given parameters, stripping out any previous handlers Because the global logger instance is modified, there is no return value</p> <p>from loguru import logger from cpg_flow.utils import format_logger format_logger(log_level=10, fmt_string='{time} {level} {message}', coloured=True) logger.info('This is an info message')</p> PARAMETER DESCRIPTION <code>log_level</code> <p>logging level, defaults to INFO. Can be overridden by config</p> <p> TYPE: <code>int</code> DEFAULT: <code>no</code> </p> <code>fmt_string</code> <p>format string for this logger, defaults to DEFAULT_LOG_FORMAT</p> <p> TYPE: <code>str</code> DEFAULT: <code>DEFAULT_LOG_FORMAT</code> </p> <code>coloured</code> <p>whether to colour the logger output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>COLOURED_LOGS</code> </p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def format_logger(\n    log_level: int = logger.level('INFO').no,\n    fmt_string: str = DEFAULT_LOG_FORMAT,\n    coloured: bool = COLOURED_LOGS,\n) -&gt; None:\n    \"\"\"\n    loguru is a cleaner interface than the standard logging module, but it doesn't allow for multiple instances\n    instead of calling a get_logger function which returns a logger, we assume that any module using logging has\n    imported `from loguru import logger` to get access to the logger.\n\n    loguru.logger is also resistant to deepcopy, so there really is only a single global instance, meaning that the\n    display/formatting of the logger is global to the entire process, and should only be set once.\n\n    This helper method formats the logger instance with the given parameters, stripping out any previous handlers\n    Because the global logger instance is modified, there is no return value\n\n    &gt;&gt;&gt; from loguru import logger\n    &gt;&gt;&gt; from cpg_flow.utils import format_logger\n    &gt;&gt;&gt; format_logger(log_level=10, fmt_string='{time} {level} {message}', coloured=True)\n    &gt;&gt;&gt; logger.info('This is an info message')\n\n    Args:\n        log_level (int): logging level, defaults to INFO. Can be overridden by config\n        fmt_string (str): format string for this logger, defaults to DEFAULT_LOG_FORMAT\n        coloured (bool): whether to colour the logger output\n    \"\"\"\n\n    # Remove any previous loguru handlers\n    logger.remove()\n\n    # Add loguru handler with given format and level\n    logger.add(\n        sys.stdout,\n        level=log_level,\n        format=fmt_string,\n        colorize=coloured,\n        enqueue=True,\n    )\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.chunks","title":"cpg_flow.utils.chunks","text":"<pre><code>chunks(iterable, chunk_size)\n</code></pre> <p>Yield successive n-sized chunks from an iterable</p> PARAMETER DESCRIPTION <code>iterable</code> <p>any iterable - tuple, str, list, set</p> <p> </p> <code>chunk_size</code> <p>size of intervals to return</p> <p> </p> RETURNS DESCRIPTION <code>Iterator[Any]</code> <p>intervals of requested size across the collection</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def chunks(iterable, chunk_size) -&gt; Iterator[Any]:\n    \"\"\"\n    Yield successive n-sized chunks from an iterable\n\n    Args:\n        iterable (): any iterable - tuple, str, list, set\n        chunk_size (): size of intervals to return\n\n    Returns:\n        intervals of requested size across the collection\n    \"\"\"\n\n    if isinstance(iterable, set):\n        iterable = list(iterable)\n\n    for i in range(0, len(iterable), chunk_size):\n        yield iterable[i : (i + chunk_size)]\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.generator_chunks","title":"cpg_flow.utils.generator_chunks","text":"<pre><code>generator_chunks(generator, size)\n</code></pre> <p>Iterates across a generator, returning specifically sized chunks</p> PARAMETER DESCRIPTION <code>generator</code> <p>any generator or method implementing yield</p> <p> </p> <code>size</code> <p>size of iterator to return</p> <p> </p> RETURNS DESCRIPTION <code>Iterator[list[Any]]</code> <p>a subset of the generator results</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def generator_chunks(generator, size) -&gt; Iterator[list[Any]]:\n    \"\"\"\n    Iterates across a generator, returning specifically sized chunks\n\n    Args:\n        generator (): any generator or method implementing yield\n        size (): size of iterator to return\n\n    Returns:\n        a subset of the generator results\n    \"\"\"\n    iterator = iter(generator)\n    for first in iterator:\n        yield list(chain([first], islice(iterator, size - 1)))\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.read_hail","title":"cpg_flow.utils.read_hail","text":"<pre><code>read_hail(path)\n</code></pre> <p>read a hail object using the appropriate method Args:     path (str): path to the input object Returns:     hail object (hl.MatrixTable or hl.Table)</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def read_hail(path):\n    \"\"\"\n    read a hail object using the appropriate method\n    Args:\n        path (str): path to the input object\n    Returns:\n        hail object (hl.MatrixTable or hl.Table)\n    \"\"\"\n    if path.strip('/').endswith('.ht'):\n        t = hl.read_table(str(path))\n    else:\n        assert path.strip('/').endswith('.mt')\n        t = hl.read_matrix_table(str(path))\n    logger.info(f'Read data from {path}')\n    return t\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.checkpoint_hail","title":"cpg_flow.utils.checkpoint_hail","text":"<pre><code>checkpoint_hail(\n    t, file_name, checkpoint_prefix=None, allow_reuse=False\n)\n</code></pre> <p>checkpoint method provide with a path and a prefix (GCP directory, can be None) allow_reuse sets whether the checkpoint can be reused - we typically want to avoid reuse, as it means we're continuing a previous failure from an unknown state</p> PARAMETER DESCRIPTION <code>t</code> <p> TYPE: <code>Table | MatrixTable</code> </p> <code>file_name</code> <p>name for this checkpoint</p> <p> TYPE: <code>str</code> </p> <code>checkpoint_prefix</code> <p>path to the checkpoint directory</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <code>allow_reuse</code> <p>whether to permit reuse of an existing checkpoint</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def checkpoint_hail(\n    t: hl.Table | hl.MatrixTable,\n    file_name: str,\n    checkpoint_prefix: str | None = None,\n    allow_reuse=False,\n):\n    \"\"\"\n    checkpoint method\n    provide with a path and a prefix (GCP directory, can be None)\n    allow_reuse sets whether the checkpoint can be reused - we\n    typically want to avoid reuse, as it means we're continuing a previous\n    failure from an unknown state\n\n    Args:\n        t (hl.Table | hl.MatrixTable):\n        file_name (str): name for this checkpoint\n        checkpoint_prefix (str): path to the checkpoint directory\n        allow_reuse (bool): whether to permit reuse of an existing checkpoint\n    \"\"\"\n\n    # drop the schema here\n    t.describe()\n\n    # log the current number of partitions\n    logger.info(f'Checkpointing object as {t.n_partitions()} partitions')\n\n    if checkpoint_prefix is None:\n        return t\n\n    path = join(checkpoint_prefix, file_name)\n    if can_reuse(path) and allow_reuse:\n        logger.info(f'Re-using {path}')\n        return read_hail(path)\n\n    logger.info(f'Checkpointing {path}')\n    return t.checkpoint(path, overwrite=True)\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.exists","title":"cpg_flow.utils.exists  <code>cached</code>","text":"<pre><code>exists(path, verbose=True)\n</code></pre> <p><code>exists_not_cached</code> that caches the result.</p> <p>The python code runtime happens entirely during the workflow construction, without waiting for it to finish, so there is no expectation that the object existence status would change during the runtime. This, this function uses <code>@lru_cache</code> to make sure that object existence is checked only once.</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>@lru_cache\ndef exists(path: Path | str, verbose: bool = True) -&gt; bool:\n    \"\"\"\n    `exists_not_cached` that caches the result.\n\n    The python code runtime happens entirely during the workflow construction,\n    without waiting for it to finish, so there is no expectation that the object\n    existence status would change during the runtime. This, this function uses\n    `@lru_cache` to make sure that object existence is checked only once.\n    \"\"\"\n    return exists_not_cached(path, verbose)\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.exists_not_cached","title":"cpg_flow.utils.exists_not_cached","text":"<pre><code>exists_not_cached(path, verbose=True)\n</code></pre> <p>Check if the object by path exists, where the object can be:     * local file,     * local directory,     * cloud object,     * cloud or local *.mt, *.ht, or *.vds Hail data, in which case it will check       for the existence of a corresponding _SUCCESS object instead. @param path: path to the file/directory/object/mt/ht @param verbose: print on each check @return: True if the object exists</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def exists_not_cached(path: Path | str, verbose: bool = True) -&gt; bool:\n    \"\"\"\n    Check if the object by path exists, where the object can be:\n        * local file,\n        * local directory,\n        * cloud object,\n        * cloud or local *.mt, *.ht, or *.vds Hail data, in which case it will check\n          for the existence of a corresponding _SUCCESS object instead.\n    @param path: path to the file/directory/object/mt/ht\n    @param verbose: print on each check\n    @return: True if the object exists\n    \"\"\"\n    path = cast('Path', to_path(path))\n\n    if path.suffix in {'.mt', '.ht'}:\n        path /= '_SUCCESS'\n    if path.suffix == '.vds':\n        path /= 'variant_data/_SUCCESS'\n\n    if verbose:\n        # noinspection PyBroadException\n        try:\n            res = check_exists_path(path)\n\n        # a failure to detect the parent folder causes a crash\n        # instead stick to a core responsibility -\n        # existence = False\n        except FileNotFoundError as fnfe:\n            logger.error(f'Failed checking {path}')\n            logger.error(f'{fnfe}')\n            return False\n        except BaseException:\n            traceback.print_exc()\n            logger.error(f'Failed checking {path}')\n            sys.exit(1)\n        logger.debug(f'Checked {path} [' + ('exists' if res else 'missing') + ']')\n        return res\n\n    return check_exists_path(path)\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.check_exists_path","title":"cpg_flow.utils.check_exists_path","text":"<pre><code>check_exists_path(test_path)\n</code></pre> <p>Check whether a path exists using a cached per-directory listing. NB. reversion to Strings prevents a get call, which is typically forbidden to local users - this prevents this method being used in the metamist audit processes</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def check_exists_path(test_path: Path) -&gt; bool:\n    \"\"\"\n    Check whether a path exists using a cached per-directory listing.\n    NB. reversion to Strings prevents a get call, which is typically\n    forbidden to local users - this prevents this method being used in the\n    metamist audit processes\n    \"\"\"\n    return basename(str(test_path)) in get_contents_of_path(dirname(str(test_path)))\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.get_contents_of_path","title":"cpg_flow.utils.get_contents_of_path  <code>cached</code>","text":"<pre><code>get_contents_of_path(test_path)\n</code></pre> <p>Get the contents of a GCS path, returning non-complete paths, eg:</p> <pre><code>get_contents_of_path('gs://my-bucket/my-dir/')\n'my-file.txt'\n</code></pre> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>@lru_cache\ndef get_contents_of_path(test_path: str) -&gt; set[str]:\n    \"\"\"\n    Get the contents of a GCS path, returning non-complete paths, eg:\n\n        get_contents_of_path('gs://my-bucket/my-dir/')\n        'my-file.txt'\n\n    \"\"\"\n    return {f.name for f in to_path(test_path.rstrip('/')).iterdir()}\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.can_reuse","title":"cpg_flow.utils.can_reuse","text":"<pre><code>can_reuse(path, overwrite=False)\n</code></pre> <p>Checks if the object at <code>path</code> is good to reuse: * overwrite has the default value of False, * check_intermediates has the default value of True, * object exists.</p> <p>If <code>path</code> is a collection, it requires all paths to exist.</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def can_reuse(\n    path: list[Path] | Path | str | None,\n    overwrite: bool = False,\n) -&gt; bool:\n    \"\"\"\n    Checks if the object at `path` is good to reuse:\n    * overwrite has the default value of False,\n    * check_intermediates has the default value of True,\n    * object exists.\n\n    If `path` is a collection, it requires all paths to exist.\n    \"\"\"\n    if overwrite:\n        return False\n\n    if not get_config()['workflow'].get('check_intermediates', True):\n        return False\n\n    if not path:\n        return False\n\n    paths = path if isinstance(path, list) else [path]\n    if not all(exists(fp, overwrite) for fp in paths):\n        return False\n\n    logger.debug(f'Reusing existing {path}')\n    return True\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.timestamp","title":"cpg_flow.utils.timestamp","text":"<pre><code>timestamp(rand_suffix_len=5)\n</code></pre> <p>Generate a timestamp string. If <code>rand_suffix_len</code> is set, adds a short random string of this length for uniqueness.</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def timestamp(rand_suffix_len: int = 5) -&gt; str:\n    \"\"\"\n    Generate a timestamp string. If `rand_suffix_len` is set, adds a short random\n    string of this length for uniqueness.\n    \"\"\"\n    result = time.strftime('%Y_%m%d_%H%M')\n    if rand_suffix_len:\n        rand_bit = ''.join(\n            choices(string.ascii_uppercase + string.digits, k=rand_suffix_len),\n        )\n        result += f'_{rand_bit}'\n    return result\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.slugify","title":"cpg_flow.utils.slugify","text":"<pre><code>slugify(line)\n</code></pre> <p>Slugify a string.</p> <p>Example:</p> <p>slugify(u'H\u00e9ll\u00f8 W.1') 'hello-w-1'</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def slugify(line: str):\n    \"\"\"\n    Slugify a string.\n\n    Example:\n    &gt;&gt;&gt; slugify(u'H\u00e9ll\u00f8 W.1')\n    'hello-w-1'\n    \"\"\"\n\n    line = unicodedata.normalize('NFKD', line).encode('ascii', 'ignore').decode()\n    line = line.strip().lower()\n    line = re.sub(\n        r'[\\s.]+',\n        '-',\n        line,\n    )\n    return line\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.rich_sequencing_group_id_seds","title":"cpg_flow.utils.rich_sequencing_group_id_seds","text":"<pre><code>rich_sequencing_group_id_seds(rich_id_map, file_names)\n</code></pre> <p>Helper function to add seds into a command that would extend sequencing group IDs in each file in <code>file_names</code> with an external ID, only if external ID is different from the original.</p> <p>@param rich_id_map: map used to replace sequencing groups, e.g. {'CPGAA': 'CPGAA|EXTID'} @param file_names: file names and Hail Batch Resource files where to replace IDs @return: bash command that does replacement</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def rich_sequencing_group_id_seds(\n    rich_id_map: dict[str, str],\n    file_names: list[str | ResourceFile],\n) -&gt; str:\n    \"\"\"\n    Helper function to add seds into a command that would extend sequencing group IDs\n    in each file in `file_names` with an external ID, only if external ID is\n    different from the original.\n\n    @param rich_id_map: map used to replace sequencing groups, e.g. {'CPGAA': 'CPGAA|EXTID'}\n    @param file_names: file names and Hail Batch Resource files where to replace IDs\n    @return: bash command that does replacement\n    \"\"\"\n    cmd = ''\n    for sgid, rich_sgid in rich_id_map.items():\n        for fname in file_names:\n            cmd += f\"sed -iBAK 's/{sgid}/{rich_sgid}/g' {fname}\"\n            cmd += '\\n'\n    return cmd\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.tshirt_mt_sizing","title":"cpg_flow.utils.tshirt_mt_sizing","text":"<pre><code>tshirt_mt_sizing(sequencing_type, cohort_size)\n</code></pre> <p>Some way of taking the details we have (#SGs, sequencing type) and producing an estimate (with padding) of the MT size on disc used to determine VM provision during ES export and Talos</p> PARAMETER DESCRIPTION <code>sequencing_type</code> <p> </p> <code>cohort_size</code> <p> </p> RETURNS DESCRIPTION <code>int</code> <p>str, the value for job.storage(X)</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def tshirt_mt_sizing(sequencing_type: str, cohort_size: int) -&gt; int:\n    \"\"\"\n    Some way of taking the details we have (#SGs, sequencing type)\n    and producing an estimate (with padding) of the MT size on disc\n    used to determine VM provision during ES export and Talos\n\n    Args:\n        sequencing_type ():\n        cohort_size ():\n\n    Returns:\n        str, the value for job.storage(X)\n    \"\"\"\n\n    # allow for an override from config\n    if preset := config_retrieve(['workflow', 'es_storage'], False):\n        return preset\n\n    if (sequencing_type == 'genome' and cohort_size &lt; 100) or (sequencing_type == 'exome' and cohort_size &lt; 1000):\n        return 50\n    return 500\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.get_intervals_from_bed","title":"cpg_flow.utils.get_intervals_from_bed","text":"<pre><code>get_intervals_from_bed(intervals_path)\n</code></pre> <p>Read genomic intervals from a bed file. Increment the start position of each interval by 1 to match the 1-based coordinate system used by GATK.</p> <p>Returns a list of interval strings in the format 'chrN:start-end'.</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def get_intervals_from_bed(intervals_path: Path) -&gt; list[str]:\n    \"\"\"\n    Read genomic intervals from a bed file.\n    Increment the start position of each interval by 1 to match the 1-based\n    coordinate system used by GATK.\n\n    Returns a list of interval strings in the format 'chrN:start-end'.\n    \"\"\"\n    with intervals_path.open('r') as f:\n        intervals = []\n        for line in f:\n            chrom, start, end = line.strip().split('\\t')\n            intervals.append(f'{chrom}:{int(start) + 1}-{end}')\n    return intervals\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.make_job_name","title":"cpg_flow.utils.make_job_name","text":"<pre><code>make_job_name(\n    name,\n    sequencing_group=None,\n    participant_id=None,\n    dataset=None,\n    part=None,\n)\n</code></pre> <p>Extend the descriptive job name to reflect job attributes.</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def make_job_name(\n    name: str,\n    sequencing_group: str | None = None,\n    participant_id: str | None = None,\n    dataset: str | None = None,\n    part: str | None = None,\n) -&gt; str:\n    \"\"\"\n    Extend the descriptive job name to reflect job attributes.\n    \"\"\"\n    if sequencing_group and participant_id:\n        sequencing_group = f'{sequencing_group}/{participant_id}'\n    if sequencing_group and dataset:\n        name = f'{dataset}/{sequencing_group}: {name}'\n    elif dataset:\n        name = f'{dataset}: {name}'\n    if part:\n        name += f', {part}'\n    return name\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.hash_from_list_of_strings","title":"cpg_flow.utils.hash_from_list_of_strings","text":"<pre><code>hash_from_list_of_strings(\n    string_list, hash_length=10, suffix=None\n)\n</code></pre> <p>Create a hash from a list of strings Args:     string_list ():     hash_length (int): how many characters to use from the hash     suffix (str): optional, clarify the type of value which was hashed Returns:</p> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def hash_from_list_of_strings(string_list: list[str], hash_length: int = 10, suffix: str | None = None) -&gt; str:\n    \"\"\"\n    Create a hash from a list of strings\n    Args:\n        string_list ():\n        hash_length (int): how many characters to use from the hash\n        suffix (str): optional, clarify the type of value which was hashed\n    Returns:\n    \"\"\"\n    hash_portion = hashlib.sha256(' '.join(string_list).encode()).hexdigest()[:hash_length]\n    full_hash = f'{hash_portion}_{len(string_list)}'\n\n    if suffix:\n        full_hash += f'_{suffix}'\n    return full_hash\n</code></pre>"},{"location":"reference/utils/#cpg_flow.utils.write_to_gcs_bucket","title":"cpg_flow.utils.write_to_gcs_bucket","text":"<pre><code>write_to_gcs_bucket(contents, path)\n</code></pre> Source code in <code>src/cpg_flow/utils.py</code> <pre><code>def write_to_gcs_bucket(contents, path: Path):\n    client = storage.Client()\n\n    if not str(path).startswith('gs:/'):\n        raise ValueError(f'Path {path} must be a GCS path')\n\n    new_path = str(path).removeprefix('gs:/').removeprefix('/')\n    bucket_name, blob_name = new_path.split('/', 1)\n\n    bucket = client.bucket(bucket_name)\n    if not bucket.exists():\n        raise ValueError(f'Bucket {bucket_name} does not exist')\n\n    blob = bucket.blob(blob_name)\n    blob.upload_from_string(contents)\n\n    return bucket_name, blob_name\n</code></pre>"},{"location":"reference/workflows/","title":"Workflow class","text":"<p>Provides a <code>Workflow</code> class and a <code>@stage</code> decorator that allow to define workflows in a declarative fashion.</p> <p>A <code>Stage</code> object is responsible for creating Hail Batch jobs and declaring outputs (files or metamist analysis objects) that are expected to be produced. Each stage acts on a <code>Target</code>, which can be of the following:</p> <pre><code>* SequencingGroup - an individual Sequencing Group (e.g. the CRAM of a single sample)\n* Dataset - a stratification of SGs in this analysis by Metamist Project (e.g. all SGs in acute-care)\n* Cohort - a stratification of SGs in this analysis by Metamist CustomCohort\n* MultiCohort - a union of all SGs in this analysis by Metamist CustomCohort\n</code></pre> <p>A <code>Workflow</code> object plugs stages together by resolving dependencies between different levels accordingly. Stages are defined in this package, and chained into Workflows by their inter-Stages dependencies. Workflow names are defined in main.py, which provides a way to choose a workflow using a CLI argument.</p>"},{"location":"reference/workflows/#cpg_flow.workflow.get_workflow","title":"cpg_flow.workflow.get_workflow","text":"<pre><code>get_workflow()\n</code></pre> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def get_workflow() -&gt; 'Workflow':\n    if _workflow is None:\n        raise WorkflowError(\n            'No workflow has been created yet: ensure that run_workflow is called before any calls to get_workflow'\n        )\n    return _workflow\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.run_workflow","title":"cpg_flow.workflow.run_workflow","text":"<pre><code>run_workflow(name, stages=None, wait=False, dry_run=False)\n</code></pre> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def run_workflow(\n    name: str,\n    stages: list['StageDecorator'] | None = None,\n    wait: bool | None = False,\n    dry_run: bool = False,\n) -&gt; 'Workflow':\n    global _workflow\n    if _workflow is None:\n        format_logger()\n        _workflow = Workflow(name=name, dry_run=dry_run)\n    _workflow.run(stages=stages, wait=wait)\n    return _workflow\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow","title":"cpg_flow.workflow.Workflow","text":"<pre><code>Workflow(name, stages=None, dry_run=None)\n</code></pre> <p>Encapsulates a Hail Batch object, stages, and a cohort of datasets of sequencing groups. Responsible for orchestrating stages.</p> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    stages: list['StageDecorator'] | None = None,\n    dry_run: bool | None = None,\n):\n    if _workflow is not None:\n        raise ValueError(\n            'Workflow already initialised. Use get_workflow() to get the instance',\n        )\n\n    self.dry_run = dry_run or config_retrieve(['workflow', 'dry_run'], False)\n    self.show_workflow = config_retrieve(['workflow', 'show_workflow'], False)\n    self.access_level = config_retrieve(['workflow', 'access_level'], 'test')\n\n    # TODO: should the ['dataset'] be a get? should we rename it to analysis dataset?\n    analysis_dataset = config_retrieve(['workflow', 'dataset'])\n    description = config_retrieve(['workflow', 'description'], name)\n    self.name = slugify(name)\n\n    self._output_version: str | None = None\n    if output_version := config_retrieve(['workflow', 'output_version'], None):\n        self._output_version = slugify(output_version)\n\n    self.run_timestamp: str = config_retrieve(['workflow', 'run_timestamp'], timestamp())\n\n    # Description\n    if self._output_version:\n        description += f': output_version={self._output_version}'\n    description += f': run_timestamp={self.run_timestamp}'\n    if sequencing_type := config_retrieve(['workflow', 'sequencing_type'], None):\n        description += f' [{sequencing_type}]'\n    if not self.dry_run:\n        if ds_set := set(d.name for d in get_multicohort().get_datasets()):\n            description += ' ' + ', '.join(sorted(ds_set))\n        reset_batch()\n        get_batch().name = description\n\n    self.status_reporter = None\n    if config_retrieve(['workflow', 'status_reporter'], None) == 'metamist':\n        self.status_reporter = MetamistStatusReporter()\n    self._stages: list[StageDecorator] | None = stages\n    self.queued_stages: list[Stage] = []\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.dry_run","title":"dry_run  <code>instance-attribute</code>","text":"<pre><code>dry_run = dry_run or config_retrieve(\n    [\"workflow\", \"dry_run\"], False\n)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.show_workflow","title":"show_workflow  <code>instance-attribute</code>","text":"<pre><code>show_workflow = config_retrieve(\n    [\"workflow\", \"show_workflow\"], False\n)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.access_level","title":"access_level  <code>instance-attribute</code>","text":"<pre><code>access_level = config_retrieve(\n    [\"workflow\", \"access_level\"], \"test\"\n)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name = slugify(name)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.run_timestamp","title":"run_timestamp  <code>instance-attribute</code>","text":"<pre><code>run_timestamp = config_retrieve(\n    [\"workflow\", \"run_timestamp\"], timestamp()\n)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.status_reporter","title":"status_reporter  <code>instance-attribute</code>","text":"<pre><code>status_reporter = None\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.queued_stages","title":"queued_stages  <code>instance-attribute</code>","text":"<pre><code>queued_stages = []\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.output_version","title":"output_version  <code>property</code>","text":"<pre><code>output_version\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.analysis_prefix","title":"analysis_prefix  <code>property</code>","text":"<pre><code>analysis_prefix\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.tmp_prefix","title":"tmp_prefix  <code>property</code>","text":"<pre><code>tmp_prefix\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.web_prefix","title":"web_prefix  <code>property</code>","text":"<pre><code>web_prefix\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.prefix","title":"prefix  <code>property</code>","text":"<pre><code>prefix\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.cohort_prefix","title":"cohort_prefix","text":"<pre><code>cohort_prefix(cohort, category=None)\n</code></pre> <p>Takes a cohort and category as an argument, calls through to the Workflow cohort_prefix method Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID e.g. \"gs://cpg-project-main/seqr_loader/COH123\", or \"gs://cpg-project-main-analysis/seqr_loader/COH123\"</p> PARAMETER DESCRIPTION <code>cohort</code> <p>we pull the analysis dataset and id from this Cohort</p> <p> TYPE: <code>Cohort</code> </p> <code>category</code> <p>sub-bucket for this project</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Path</code> <p>Path</p> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def cohort_prefix(self, cohort: Cohort, category: str | None = None) -&gt; Path:\n    \"\"\"\n    Takes a cohort and category as an argument, calls through to the Workflow cohort_prefix method\n    Result in the form PROJECT_BUCKET / WORKFLOW_NAME / COHORT_ID\n    e.g. \"gs://cpg-project-main/seqr_loader/COH123\", or \"gs://cpg-project-main-analysis/seqr_loader/COH123\"\n\n    Args:\n        cohort (Cohort): we pull the analysis dataset and id from this Cohort\n        category (str | None): sub-bucket for this project\n\n    Returns:\n        Path\n    \"\"\"\n    return cohort.dataset.prefix(category=category) / self.name / cohort.id\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.run","title":"run","text":"<pre><code>run(stages=None, wait=False)\n</code></pre> <p>Resolve stages, add and submit Hail Batch jobs. When <code>run_all_implicit_stages</code> is set, all required stages that were not defined explicitly would still be executed.</p> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def run(\n    self,\n    stages: list['StageDecorator'] | None = None,\n    wait: bool | None = False,\n):\n    \"\"\"\n    Resolve stages, add and submit Hail Batch jobs.\n    When `run_all_implicit_stages` is set, all required stages that were not defined\n    explicitly would still be executed.\n    \"\"\"\n    stages_value = stages or self._stages\n    if not stages_value:\n        raise WorkflowError('No stages added')\n    self.set_stages(stages_value)\n\n    if not self.dry_run:\n        get_batch().run(wait=wait)\n    else:\n        logger.info('Dry run: no jobs submitted')\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Workflow.set_stages","title":"set_stages","text":"<pre><code>set_stages(requested_stages)\n</code></pre> <p>Iterate over stages and call their queue_for_cohort(cohort) methods; through that, creates all Hail Batch jobs through Stage.queue_jobs().</p> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def set_stages(\n    self,\n    requested_stages: list['StageDecorator'],\n):\n    \"\"\"\n    Iterate over stages and call their queue_for_cohort(cohort) methods;\n    through that, creates all Hail Batch jobs through Stage.queue_jobs().\n    \"\"\"\n    # TOML options to configure stages:\n    skip_stages = get_config()['workflow'].get('skip_stages', [])\n    only_stages = get_config()['workflow'].get('only_stages', [])\n    first_stages = get_config()['workflow'].get('first_stages', [])\n    last_stages = get_config()['workflow'].get('last_stages', [])\n\n    # Only allow one of only_stages or first_stages/last_stages as they seem\n    # to be mutually exclusive.\n    if only_stages and (first_stages or last_stages or skip_stages):\n        raise WorkflowError(\n            \"Workflow config parameter 'only_stages' is incompatible with \"\n            + \"'first_stages', 'last_stages' and/or 'skip_stages'\",\n        )\n\n    logger.info(\n        f'End stages for the workflow \"{self.name}\": {[cls.__name__ for cls in requested_stages]}',\n    )\n    logger.info('Stages additional configuration:')\n    logger.info(f'  workflow/skip_stages: {skip_stages}')\n    logger.info(f'  workflow/only_stages: {only_stages}')\n    logger.info(f'  workflow/first_stages: {first_stages}')\n    logger.info(f'  workflow/last_stages: {last_stages}')\n\n    # Round 1: initialising stage objects.\n    stages_dict: dict[str, Stage] = {}\n    for cls in requested_stages:\n        if cls.__name__ in stages_dict:\n            continue\n        stages_dict[cls.__name__] = cls()\n\n    # Round 2: depth search to find implicit stages.\n    stages_dict = self._resolve_implicit_stages(\n        stages_dict=stages_dict,\n        skip_stages=skip_stages,\n        only_stages=only_stages,\n    )\n\n    # Round 3: set \"stage.required_stages\" fields to each stage.\n    for stg in stages_dict.values():\n        stg.required_stages = [\n            stages_dict[cls.__name__] for cls in stg.required_stages_classes if cls.__name__ in stages_dict\n        ]\n\n    # Round 4: determining order of execution.\n    stages, dag = self._determine_order_of_execution(stages_dict)\n\n    # Round 5: applying workflow options first_stages and last_stages.\n    if first_stages or last_stages:\n        logger.info('Applying workflow/first_stages and workflow/last_stages')\n        self._process_first_last_stages(stages, dag, first_stages, last_stages)\n    elif only_stages:\n        logger.info('Applying workflow/only_stages')\n        self._process_only_stages(stages, dag, only_stages)\n\n    if all(s.skipped for s in stages):\n        raise WorkflowError('No stages to run')\n\n    logger.info('Final workflow graph:')\n    for line in _render_graph(\n        dag,\n        target_stages=[cls.__name__ for cls in requested_stages],\n        only_stages=only_stages,\n        first_stages=first_stages,\n        last_stages=last_stages,\n    ):\n        logger.info(line)\n    # Round 6: actually adding jobs from the stages.\n    if not self.dry_run:\n        inputs = get_multicohort()  # Would communicate with metamist.\n        for i, stg in enumerate(stages):\n            logger.info('*' * 60)\n            logger.info(f'Stage #{i + 1}: {stg}')\n            # pipeline setup is now done in MultiCohort only\n            # the legacy version (input_datasets) is still supported\n            # that will create a MultiCohort with a single Cohort\n            if isinstance(inputs, MultiCohort):\n                stg.output_by_target = stg.queue_for_multicohort(inputs)\n            else:\n                raise WorkflowError(f'Unsupported input type: {inputs}')\n            if errors := self._process_stage_errors(stg.output_by_target):\n                raise WorkflowError(\n                    f'Stage {stg} failed to queue jobs with errors: ' + '\\n'.join(errors),\n                )\n    else:\n        self.queued_stages = [stg for stg in stages_dict.values() if not stg.skipped]\n        logger.info(f'Queued stages: {self.queued_stages}')\n\n    # Round 7: show the workflow\n    self._show_workflow(dag, skip_stages, only_stages, first_stages, last_stages)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Action","title":"cpg_flow.workflow.Action","text":"<p>               Bases: <code>Enum</code></p> <p>Indicates what a stage should do with a specific target.</p>"},{"location":"reference/workflows/#cpg_flow.workflow.Action.QUEUE","title":"QUEUE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>QUEUE = 1\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Action.SKIP","title":"SKIP  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SKIP = 2\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.Action.REUSE","title":"REUSE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>REUSE = 3\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.skip","title":"cpg_flow.workflow.skip","text":"<pre><code>skip(_fun=None, *, reason=None, assume_outputs_exist=False)\n</code></pre> <p>Decorator on top of <code>@stage</code> that sets the <code>self.skipped</code> field to True. By default, expected outputs of a skipped stage will be checked, unless <code>assume_outputs_exist</code> is True.</p> <p>@skip @stage class MyStage1(SequencingGroupStage):     ...</p> <p>@skip @stage(assume_outputs_exist=True) class MyStage2(SequencingGroupStage):     ...</p> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def skip(\n    _fun: Optional['StageDecorator'] = None,\n    *,\n    reason: str | None = None,\n    assume_outputs_exist: bool = False,\n) -&gt; Union['StageDecorator', Callable[..., 'StageDecorator']]:\n    \"\"\"\n    Decorator on top of `@stage` that sets the `self.skipped` field to True.\n    By default, expected outputs of a skipped stage will be checked,\n    unless `assume_outputs_exist` is True.\n\n    @skip\n    @stage\n    class MyStage1(SequencingGroupStage):\n        ...\n\n    @skip\n    @stage(assume_outputs_exist=True)\n    class MyStage2(SequencingGroupStage):\n        ...\n    \"\"\"\n\n    def decorator_stage(fun) -&gt; 'StageDecorator':\n        \"\"\"Implements decorator.\"\"\"\n\n        @functools.wraps(fun)\n        def wrapper_stage(*args, **kwargs) -&gt; 'Stage':\n            \"\"\"Decorator helper function.\"\"\"\n            s = fun(*args, **kwargs)\n            s.skipped = True\n            s.assume_outputs_exist = assume_outputs_exist\n            return s\n\n        return wrapper_stage\n\n    if _fun is None:\n        return decorator_stage\n    return decorator_stage(_fun)\n</code></pre>"},{"location":"reference/workflows/#cpg_flow.workflow.path_walk","title":"cpg_flow.workflow.path_walk","text":"<pre><code>path_walk(expected, collected=None)\n</code></pre> <p>recursive walk of expected_out if the object is iterable, walk it this gets around the issue with nested lists and dicts mainly around the use of Array outputs from Cromwell</p> PARAMETER DESCRIPTION <code>expected</code> <p>any type of object containing Paths</p> <p> TYPE: <code>Any</code> </p> <code>collected</code> <p>all collected paths so far</p> <p> TYPE: <code>set</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>set[Path]</code> <p>a set of all collected Path nodes</p> <p>Examples:</p> <p>path_walk({'a': {'b': {'c': Path('d')}}}) {Path('d')} path_walk({'a': {'b': {'c': [Path('d'), Path('e')]}}}) {Path('d'), Path('e')} path_walk({'a': Path('b'),'c': {'d': 'e'}, {'f': Path('g')}})</p> Source code in <code>src/cpg_flow/workflow.py</code> <pre><code>def path_walk(expected, collected: set | None = None) -&gt; set[Path]:\n    \"\"\"\n    recursive walk of expected_out\n    if the object is iterable, walk it\n    this gets around the issue with nested lists and dicts\n    mainly around the use of Array outputs from Cromwell\n\n    Args:\n        expected (Any): any type of object containing Paths\n        collected (set): all collected paths so far\n\n    Returns:\n        a set of all collected Path nodes\n\n    Examples:\n\n    &gt;&gt;&gt; path_walk({'a': {'b': {'c': Path('d')}}})\n    {Path('d')}\n    &gt;&gt;&gt; path_walk({'a': {'b': {'c': [Path('d'), Path('e')]}}})\n    {Path('d'), Path('e')}\n    &gt;&gt;&gt; path_walk({'a': Path('b'),'c': {'d': 'e'}, {'f': Path('g')}})\n    {Path('b'), Path('g')}\n    \"\"\"\n    if collected is None:\n        collected = set()\n\n    if expected is None:\n        return collected\n    if isinstance(expected, dict):\n        for value in expected.values():\n            collected.update(path_walk(value, collected))\n    if isinstance(expected, list | set):\n        for value in expected:\n            collected.update(path_walk(value, collected))\n    if isinstance(expected, str):\n        return collected\n    if isinstance(expected, Path):\n        if expected in collected:\n            raise ValueError(f'Duplicate path {expected} in expected_out')\n        collected.add(expected)\n    return collected\n</code></pre>"}]}